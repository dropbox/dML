diff --git a/aten/src/ATen/detail/MPSHooksInterface.h b/aten/src/ATen/detail/MPSHooksInterface.h
index 1a16072f..e4f66828 100644
--- a/aten/src/ATen/detail/MPSHooksInterface.h
+++ b/aten/src/ATen/detail/MPSHooksInterface.h
@@ -45,6 +45,9 @@ struct TORCH_API MPSHooksInterface : AcceleratorHooksInterface {
   virtual void deviceSynchronize() const {
     FAIL_MPSHOOKS_FUNC(__func__);
   }
+  virtual void releaseCurrentThreadSlot() const {
+    // No-op by default - only MPS backend implements this
+  }
   virtual void commitStream() const {
     FAIL_MPSHOOKS_FUNC(__func__);
   }
diff --git a/aten/src/ATen/mps/MPSAllocator.h b/aten/src/ATen/mps/MPSAllocator.h
index 1132ca62..c3957e29 100644
--- a/aten/src/ATen/mps/MPSAllocator.h
+++ b/aten/src/ATen/mps/MPSAllocator.h
@@ -8,6 +8,7 @@
 
 #include <c10/util/flat_hash_map.h>
 #include <mach/vm_page_size.h>
+#include <atomic>
 #include <cstdio>
 #include <mutex>
 #include <set>
@@ -58,19 +59,32 @@ struct BufferBlock {
   size_t requested_size; // requested size (before alignment)
   // buffer shape is used for retrieving base of views in cached graphs
   std::vector<int64_t> shape;
-  bool in_use = false;
+  // 32.21 fix: Make in_use atomic to allow safe reads from get_allocated_buffer_block()
+  // without holding pool_mutex. This prevents operating on blocks that are in TLS cache.
+  std::atomic<bool> in_use{false};
   HeapBlock* heap;
   id_t buf_id;
   // counter to candidate least recently used buffers for garbage collection
   uint32_t gc_count = 0;
   uint32_t use_count = 0;
   // counter to assign unique ids to buffer blocks
-  static uint64_t buffer_counter;
+  static std::atomic<uint64_t> buffer_counter;
   // Metal events used to sync GPU/CPU operations on the shared-storage buffers
   MPSEventPtr event;
+  // 24.1/24.7: Stream-aware allocation fields (CUDA pattern)
+  // Track which stream allocated this buffer and which streams have used it
+  // Note: Use stream IDs instead of raw pointers to prevent dangling pointer risk
+  // when streams are recycled (same pattern as MPSEvent::m_recording_stream_id)
+  int64_t alloc_stream_id = -1;  // -1 = no stream
+  std::unordered_set<int64_t> stream_uses_ids;
+  std::vector<MPSEventPtr> pending_events;
 
   BufferBlock(size_t Size, size_t RequestedSize = 0, const id<MTLBuffer> Buffer = nullptr, HeapBlock* Heap = nullptr)
-      : buffer(Buffer), size(Size), requested_size(RequestedSize), heap(Heap), buf_id(Buffer ? ++buffer_counter : 0) {}
+      : buffer(Buffer),
+        size(Size),
+        requested_size(RequestedSize),
+        heap(Heap),
+        buf_id(Buffer ? (buffer_counter.fetch_add(1, std::memory_order_relaxed) + 1) : 0) {}
 
   static bool Comparator(const BufferBlock* a, const BufferBlock* b) {
     return (a->size != b->size) ? a->size < b->size : (uintptr_t)a->buffer < (uintptr_t)b->buffer;
@@ -115,13 +129,13 @@ struct HeapBlock {
   // indicates if we split this heap to sub-allocate 'several' buffers (otherwise single buffer)
   bool is_split;
   // counter to assign unique ids to heap blocks
-  static uint64_t heap_counter;
+  static std::atomic<uint64_t> heap_counter;
 
   HeapBlock(size_t Size, const id<MTLHeap> Heap = nullptr, BufferPool* Pool = nullptr)
       : heap(Heap),
         size({.total = Size, .available = Size}),
         pool(Pool),
-        heap_id(Heap ? ++heap_counter : 0),
+        heap_id(Heap ? (heap_counter.fetch_add(1, std::memory_order_relaxed) + 1) : 0),
         is_split(true) {}
 
   static MTLResourceOptions getOptions(uint32_t usage) {
@@ -237,6 +251,9 @@ struct BufferPool {
   BufferPool(const id<MTLDevice> Device, uint32_t Usage)
       : device(Device), usage(Usage), heaps(HeapBlock::Comparator), available_buffers(BufferBlock::Comparator) {}
 
+  // Per-pool mutex for concurrent allocations to different pools
+  // Cache-line aligned to prevent false sharing between pools (Phase 24.4)
+  alignas(64) mutable std::mutex pool_mutex;
   const id<MTLDevice> device;
   // usage flags to customize the pool for various purposes (see UsageFlags enum)
   const uint32_t usage;
@@ -271,8 +288,10 @@ class MPSHeapAllocatorImpl {
     init_allocator();
   }
   ~MPSHeapAllocatorImpl() {
-    emptyCache();
+    shutdown();
   }
+  // Called at destruction to safely cleanup
+  void shutdown();
   // interface exposed to at::Allocator
   id<MTLBuffer> malloc(size_t size, uint32_t usage);
   // frees a buffer and returns it into buffer pool
@@ -288,14 +307,17 @@ class MPSHeapAllocatorImpl {
   // set the shape of a base tensor from a view tensor
   void setBufferShape(const void* ptr, const IntArrayRef& shape);
   // retrieve the shape of a base tensor from a view tensor
-  IntArrayRef getBufferShape(const void* ptr);
+  // 32.96 fix: Return std::vector<int64_t> instead of IntArrayRef.
+  // IntArrayRef is a non-owning view that becomes dangling after pool_mutex is released.
+  // Returning a copy ensures the caller has a valid, owned buffer.
+  std::vector<int64_t> getBufferShape(const void* ptr);
   // get the unique ID of the buffer
   id_t getBufferId(const void* ptr);
   // allocate a buffer from a specialized pool to import CPU scalars into GPU
   id<MTLBuffer> allocScalarBufferWithValue(void* value, size_t size);
-  // returns a CPU-mapping of the input buffer and its retainCount,
-  // if only it has Shared storage-mode and allocated on MPSAllocator
-  std::pair<const void*, uint32_t> getSharedBufferPtr(const void* buffer);
+  // Returns a CPU-mapping of the input buffer (Shared storage-mode only) as a
+  // DataPtr that retains the underlying MTLBuffer until the DataPtr is destroyed.
+  c10::DataPtr getSharedBufferPtr(const void* buffer);
   // records events for a list of MTLBuffers (list is used to lock the mutex once)
   // returns true if records any event (given if passed buffers exist and are shared-storage)
   bool recordEvents(c10::ArrayRef<const void*> buffers);
@@ -303,6 +325,10 @@ class MPSHeapAllocatorImpl {
   // on the passed shared buffers (list is used to lock the mutex once)
   // returns true if actually waited on any event
   bool waitForEvents(c10::ArrayRef<const void*> buffers);
+  // 24.1/24.7: CUDA-style recordStream() for cross-stream synchronization
+  // Tracks that a buffer is being used by a stream other than its allocating stream.
+  // When the buffer is freed, it won't be recycled until all recorded streams complete.
+  void recordStream(const void* ptr, MPSStream* stream);
   // this indicates how far (in Megabytes) the current total allocations are from the
   // low watermark limit which is used to detect if we're under memory pressure
   // This returns zero if we've reached the low watermark limit
@@ -358,15 +384,19 @@ class MPSHeapAllocatorImpl {
   constexpr static double default_low_watermark_ratio_discrete = 1.0;
 
   const id<MTLDevice> m_device;
+  // Global mutex - used only for m_allocated_buffers map access (brief critical sections)
   std::recursive_mutex m_mutex;
   // allocated buffers by device pointer
   ska::flat_hash_map<const void*, BufferBlock*> m_allocated_buffers;
   // using a container for pools to simplify iterating them
   ska::flat_hash_map<BufferPool::Kind, std::unique_ptr<BufferPool>> m_pools;
   // total memory allocated by HeapAllocator (including blocks in pools)
-  size_t m_total_allocated_memory = 0;
+  // Using atomic for lock-free updates from different pool operations
+  // THREAD-SAFETY (23.20): Cache-line aligned to prevent false sharing with
+  // m_current_allocated_memory (both are hot-path atomics)
+  alignas(64) std::atomic<size_t> m_total_allocated_memory{0};
   // currently active memory allocations in use (i.e., blocks not in pools)
-  size_t m_current_allocated_memory = 0;
+  alignas(64) std::atomic<size_t> m_current_allocated_memory{0};
   // max buffer size allowed by Metal
   size_t m_max_buffer_size = 0;
   // maximum total size allowed to be allocated
@@ -388,6 +418,11 @@ class MPSHeapAllocatorImpl {
   size_t m_low_watermark_limit;
   // use "PYTORCH_DEBUG_MPS_ALLOCATOR" env-var to set debug verbosity
   uint32_t m_debug_verbosity;
+  // Optional allocation size rounding for the caching allocator. When enabled,
+  // allocation sizes are rounded up to the next power-of-2 division to reduce
+  // fragmentation and increase buffer reuse (CUDA-style).
+  // Env var: PYTORCH_MPS_ALLOC_CONF=roundup_power2_divisions:<N>
+  size_t m_roundup_power2_divisions = 0;
   // default MPS stream
   MPSStream* m_stream;
   // we hold a reference to MPSEventPool so it could get destroyed after MPSAllocator
@@ -396,18 +431,20 @@ class MPSHeapAllocatorImpl {
   void init_allocator();
   void init_buffer_pools();
   HeapBlock* get_free_heap(AllocParams& params);
-  bool get_free_buffer(AllocParams& params);
+  bool get_free_buffer(AllocParams& params, std::unique_lock<std::mutex>& pool_lock);
   BufferBlock* get_allocated_buffer_block(const void* ptr);
   BufferBlock* alloc_buffer_block(size_t size, uint32_t usage);
   bool alloc_buffer(AllocParams& params);
   void free_buffer(BufferBlock* buffer_block);
   // returns true if the container heap is also released
-  bool release_buffer(BufferBlock* buffer_block, bool remove_empty_heap = true);
-  void release_buffers(BufferPool& pool);
-  bool release_available_cached_buffers(AllocParams& params);
+  bool release_buffer(BufferBlock* buffer_block, std::unique_lock<std::mutex>& pool_lock, bool remove_empty_heap = true);
+  void release_buffers(BufferPool& pool, std::unique_lock<std::mutex>& pool_lock);
+  bool release_available_cached_buffers(AllocParams& params, std::unique_lock<std::mutex>& pool_lock);
   bool release_cached_buffers();
   // free unused cached blocks to reclaim GPU memory if memory pressure is high
-  void garbage_collect_cached_buffers(AllocParams& params);
+  void garbage_collect_cached_buffers(AllocParams& params, std::unique_lock<std::mutex>& pool_lock);
+  // Phase 24.2: Opportunistically process pending buffers for a pool (caller holds lock)
+  void process_pending_buffers_locked(BufferPool& pool);
   // returns the suitable buffer pool type for the usage or
   // requested/allocated sizes
   BufferPool& get_pool(size_t requested_size, size_t aligned_size, uint32_t usage);
diff --git a/aten/src/ATen/mps/MPSAllocator.mm b/aten/src/ATen/mps/MPSAllocator.mm
index c8b3453f..d533e28e 100644
--- a/aten/src/ATen/mps/MPSAllocator.mm
+++ b/aten/src/ATen/mps/MPSAllocator.mm
@@ -2,12 +2,27 @@
 
 #include <ATen/CPUFunctions.h>
 #include <ATen/EmptyTensor.h>
+#include <ATen/core/Tensor.h>
 #include <ATen/mps/MPSAllocator.h>
 #include <c10/core/Allocator.h>
 #include <c10/core/Storage.h>
 #include <c10/util/env.h>
-
+#include <c10/util/llvmMathExtras.h>
+
+#ifndef AT_PER_OPERATOR_HEADERS
+#include <ATen/NativeFunctions.h>
+#else
+#include <ATen/ops/record_stream_native.h>
+#endif
+
+#include <c10/util/ScopeExit.h>
+#include <cctype>
+#include <cerrno>
+#include <chrono>
+#include <cstdlib>
 #include <iostream>
+#include <string_view>
+#include <thread>
 
 namespace at::mps {
 
@@ -15,26 +30,264 @@ C10_DEFINE_REGISTRY(MPSAllocatorCallbacksRegistry, IMpsAllocatorCallback)
 
 namespace HeapAllocator {
 
-uint64_t BufferBlock::buffer_counter = 0;
-uint64_t HeapBlock::heap_counter = 0;
+std::atomic<uint64_t> BufferBlock::buffer_counter{0};
+std::atomic<uint64_t> HeapBlock::heap_counter{0};
+
+// Phase 23.9: Thread-local small block cache for lock-free allocation fast path
+// Reduces contention on pool mutex for frequent small allocations
+static constexpr size_t kTLSCacheMaxSize = kMaxSmallAlloc;  // Only cache small blocks (< 1MB)
+static constexpr size_t kTLSCacheMaxBlocks = 4;  // Max blocks per thread
+
+// Flag to disable TLS cache flush during allocator shutdown (destructor ordering safety)
+static std::atomic<bool> s_allocator_alive{false};
+
+// 32.68 fix: Counter to track threads currently executing TLSBlockCache::flush().
+// shutdown() waits for this to reach zero after setting s_allocator_alive=false.
+// This prevents a TOCTOU race where a thread passes the s_allocator_alive check
+// but then accesses pools that are destroyed while it's still in the flush loop.
+static std::atomic<uint32_t> s_flush_in_progress_count{0};
+
+// 32.108 fix: Counter for pending completion handlers that access pool members.
+// Metal completion handlers run asynchronously on a separate thread AFTER
+// waitUntilCompleted returns. Without tracking, pools could be destroyed while
+// handlers are still pending, causing UAF on pool.pool_mutex and other members.
+// shutdown() waits for this counter to reach zero before destroying pools.
+static std::atomic<uint32_t> s_pending_completion_handlers{0};
+
+struct TLSBlockCache {
+  std::vector<BufferBlock*> blocks;
+  size_t total_size = 0;
+
+  // Try to get a cached block that fits the requested size
+  // Returns nullptr if no suitable block found
+  // NOTE (32.98): This function does NOT manage s_flush_in_progress_count.
+  // The caller (alloc_buffer_block) is responsible for incrementing the counter
+  // BEFORE calling try_get() and decrementing AFTER using the returned block.
+  // This ensures the entire TLS cache operation is protected from shutdown().
+  BufferBlock* try_get(size_t size, uint32_t usage) {
+    // Fast-fail if allocator is shutting down (caller should have already checked,
+    // but this is a fast-path early exit if the caller's check raced with shutdown)
+    if (!s_allocator_alive.load(std::memory_order_acquire)) {
+      return nullptr;
+    }
+    for (auto it = blocks.begin(); it != blocks.end(); ++it) {
+      BufferBlock* block = *it;
+      // Match on size (within 2x) and pool usage flags
+      // We only cache small private/shared blocks, not scalar
+      if (block->size >= size && block->size <= size * 2 &&
+          (block->heap->pool->usage & (UsageFlags::SHARED | UsageFlags::PRIVATE)) ==
+          (usage & (UsageFlags::SHARED | UsageFlags::PRIVATE))) {
+        total_size -= block->size;
+        blocks.erase(it);
+        return block;
+      }
+    }
+    return nullptr;
+  }
+
+  // Try to cache a block for later reuse
+  // Returns true if cached, false if cache is full or block is too large
+  bool try_put(BufferBlock* block) {
+    if (block->size > kTLSCacheMaxSize) return false;
+    if (blocks.size() >= kTLSCacheMaxBlocks) return false;
+    // Don't cache scalar or non-SMALL pool blocks
+    if (!(block->heap->pool->usage & UsageFlags::SMALL)) return false;
+    if (block->heap->pool->usage & UsageFlags::SCALAR) return false;
+    blocks.push_back(block);
+    total_size += block->size;
+    return true;
+  }
+
+  // Flush all cached blocks back to their pools
+  // Called on thread exit or when cache needs to be cleared
+  void flush();
+
+  ~TLSBlockCache() {
+    flush();
+  }
+};
+
+// Thread-local cache instance - automatically destroyed on thread exit
+static thread_local std::unique_ptr<TLSBlockCache> tls_block_cache;
+
+// Get or create the thread-local cache
+static TLSBlockCache& get_tls_cache() {
+  if (!tls_block_cache) {
+    tls_block_cache = std::make_unique<TLSBlockCache>();
+  }
+  return *tls_block_cache;
+}
+
+// Forward declaration for flush - needs access to pool mutex
+void TLSBlockCache::flush() {
+  // Safety check: if allocator is being destroyed, don't try to flush
+  // (destructor ordering: TLS may be destroyed after allocator on main thread)
+  if (!s_allocator_alive.load(std::memory_order_acquire)) {
+    blocks.clear();
+    total_size = 0;
+    return;
+  }
+  // 32.68 fix: Increment counter to signal we're in the critical section.
+  // This must happen AFTER the alive check to avoid counting threads that
+  // will skip the flush. shutdown() waits for this counter to reach zero.
+  s_flush_in_progress_count.fetch_add(1, std::memory_order_acq_rel);
+  // 32.70 fix: Use scope guard for exception-safe counter decrement.
+  // If any operation in the flush loop throws (e.g., set::insert can throw
+  // std::bad_alloc), we must still decrement the counter or shutdown() hangs.
+  auto decrement_guard = c10::make_scope_exit([&]() {
+    s_flush_in_progress_count.fetch_sub(1, std::memory_order_release);
+  });
+  // Double-check after incrementing: if shutdown started between our first check
+  // and increment, we must not proceed (pools may be getting destroyed).
+  if (!s_allocator_alive.load(std::memory_order_acquire)) {
+    blocks.clear();
+    total_size = 0;
+    return;  // decrement_guard runs on scope exit
+  }
+  for (BufferBlock* block : blocks) {
+    BufferPool& pool = *block->heap->pool;
+    std::lock_guard<std::mutex> lock(pool.pool_mutex);
+    // 32.75 fix (defense-in-depth): Verify the invariant that blocks in TLS cache
+    // have retainCount <= 1 and no pending events. With the 32.75 fix in free(),
+    // blocks should only enter TLS cache when retainCount <= 1. If this assertion
+    // fires, there's a bug elsewhere (e.g., GPU retained buffer after free, or
+    // pending_events added to a freed block). We assert rather than try to recover
+    // because:
+    // 1. in_use is already false (set in free() before caching)
+    // 2. m_current_allocated_memory was already decremented
+    // 3. The block cannot be properly deferred without complex state restoration
+    // 4. If this happens, we need to find and fix the root cause
+    TORCH_INTERNAL_ASSERT(
+        block->retainCount() <= 1 && block->pending_events.empty(),
+        "TLS cached block has retainCount=", block->retainCount(),
+        " pending_events=", block->pending_events.size(),
+        ". This indicates a bug: blocks should only enter TLS cache when idle.");
+    // Return to pool's available_buffers set
+    pool.available_buffers.insert(block);
+    pool.available_size += block->size;
+    block->shape.clear();
+    if (block->event) {
+      block->event.reset(nullptr);
+    }
+    block->in_use.store(false, std::memory_order_release);
+  }
+  blocks.clear();
+  total_size = 0;
+  // decrement_guard runs automatically on scope exit
+}
+
+static std::string_view trim(std::string_view s) {
+  while (!s.empty() && std::isspace(static_cast<unsigned char>(s.front()))) {
+    s.remove_prefix(1);
+  }
+  while (!s.empty() && std::isspace(static_cast<unsigned char>(s.back()))) {
+    s.remove_suffix(1);
+  }
+  return s;
+}
+
+static size_t parse_roundup_power2_divisions_conf(const std::string& env) {
+  size_t divisions = 0;
+  std::string_view remaining(env);
+  while (!remaining.empty()) {
+    const size_t comma = remaining.find(',');
+    const std::string_view token = trim(remaining.substr(0, comma));
+    if (!token.empty()) {
+      const size_t colon = token.find(':');
+      if (colon != std::string_view::npos) {
+        const std::string_view key = trim(token.substr(0, colon));
+        const std::string_view value = trim(token.substr(colon + 1));
+        if (key == "roundup_power2_divisions") {
+          const std::string value_str(value);
+          char* end = nullptr;
+          const unsigned long parsed = std::strtoul(value_str.c_str(), &end, 10);
+          TORCH_CHECK(
+              end != nullptr && *end == '\0',
+              "PYTORCH_MPS_ALLOC_CONF: invalid roundup_power2_divisions value '",
+              value,
+              "'");
+          divisions = static_cast<size_t>(parsed);
+          TORCH_CHECK(
+              divisions == 0 || c10::llvm::isPowerOf2_64(divisions),
+              "PYTORCH_MPS_ALLOC_CONF: roundup_power2_divisions must be 0 or a power of 2, got ",
+              divisions);
+        }
+      }
+    }
+    if (comma == std::string_view::npos) {
+      break;
+    }
+    remaining.remove_prefix(comma + 1);
+  }
+  return divisions;
+}
+
+static size_t roundup_power2_next_division(size_t size, size_t divisions) {
+  if (c10::llvm::isPowerOf2_64(size)) {
+    return size;
+  }
+
+  TORCH_CHECK(divisions >= 2, "Only 2 or more divisions are supported");
+
+  const size_t power2_floor = c10::llvm::PowerOf2Floor(size);
+  const size_t power2_division = power2_floor >> (63 - c10::llvm::countLeadingZeros(divisions));
+  if (power2_division == 0) {
+    return (power2_floor << 1);
+  }
+  const size_t round_size_floor = size & (~(power2_division - 1));
+  return (round_size_floor == size) ? size : round_size_floor + power2_division;
+}
 
 void MPSHeapAllocatorImpl::init_allocator() {
+  // Mark allocator as alive for TLS cache safety
+  s_allocator_alive.store(true, std::memory_order_release);
   init_buffer_pools();
 
   // debug verbosity flags (see DebugVerbosity enum)
+  // 32.28 fix: Add error checking for environment variable parsing
   static const auto verbosity_str = c10::utils::get_env("PYTORCH_DEBUG_MPS_ALLOCATOR");
-  m_debug_verbosity = verbosity_str ? strtol(verbosity_str->c_str(), nullptr, 0) : DebugVerbosity::SILENT;
+  if (verbosity_str) {
+    char* endptr = nullptr;
+    errno = 0;
+    long val = strtol(verbosity_str->c_str(), &endptr, 0);
+    TORCH_CHECK(errno == 0 && endptr != verbosity_str->c_str() && (*endptr == '\0' || std::isspace(*endptr)),
+                "PYTORCH_DEBUG_MPS_ALLOCATOR: invalid value '", *verbosity_str, "'");
+    m_debug_verbosity = static_cast<uint32_t>(val);
+  } else {
+    m_debug_verbosity = DebugVerbosity::SILENT;
+  }
+
+  static const auto alloc_conf_str = c10::utils::get_env("PYTORCH_MPS_ALLOC_CONF");
+  m_roundup_power2_divisions = alloc_conf_str ? parse_roundup_power2_divisions_conf(*alloc_conf_str) : 0;
+  if ((m_debug_verbosity & DebugVerbosity::PROFILING) && m_roundup_power2_divisions > 1) {
+    std::cerr << "MPS allocator size rounding: roundup_power2_divisions=" << m_roundup_power2_divisions << "\n";
+  }
 
+  // 32.28 fix: Add error checking for watermark ratio parsing
   static const auto high_watermark_ratio_str = c10::utils::get_env("PYTORCH_MPS_HIGH_WATERMARK_RATIO");
-  const double high_watermark_ratio =
-      high_watermark_ratio_str ? strtod(high_watermark_ratio_str->c_str(), nullptr) : default_high_watermark_ratio;
+  double high_watermark_ratio = default_high_watermark_ratio;
+  if (high_watermark_ratio_str) {
+    char* endptr = nullptr;
+    errno = 0;
+    double val = strtod(high_watermark_ratio_str->c_str(), &endptr);
+    TORCH_CHECK(errno == 0 && endptr != high_watermark_ratio_str->c_str() && (*endptr == '\0' || std::isspace(*endptr)),
+                "PYTORCH_MPS_HIGH_WATERMARK_RATIO: invalid value '", *high_watermark_ratio_str, "'");
+    high_watermark_ratio = val;
+  }
   setHighWatermarkRatio(high_watermark_ratio);
 
   const double default_low_watermark_ratio =
       m_device.hasUnifiedMemory ? default_low_watermark_ratio_unified : default_low_watermark_ratio_discrete;
   static const auto low_watermark_ratio_str = c10::utils::get_env("PYTORCH_MPS_LOW_WATERMARK_RATIO");
-  const double low_watermark_ratio =
-      low_watermark_ratio_str ? strtod(low_watermark_ratio_str->c_str(), nullptr) : default_low_watermark_ratio;
+  double low_watermark_ratio = default_low_watermark_ratio;
+  if (low_watermark_ratio_str) {
+    char* endptr = nullptr;
+    errno = 0;
+    double val = strtod(low_watermark_ratio_str->c_str(), &endptr);
+    TORCH_CHECK(errno == 0 && endptr != low_watermark_ratio_str->c_str() && (*endptr == '\0' || std::isspace(*endptr)),
+                "PYTORCH_MPS_LOW_WATERMARK_RATIO: invalid value '", *low_watermark_ratio_str, "'");
+    low_watermark_ratio = val;
+  }
   setLowWatermarkRatio(low_watermark_ratio);
 }
 
@@ -76,7 +329,12 @@ BufferPool& MPSHeapAllocatorImpl::get_pool(size_t requested_size, size_t aligned
 
 size_t MPSHeapAllocatorImpl::get_allocation_size(size_t size, uint32_t usage) const {
   MTLSizeAndAlign sizeAlign = [m_device heapBufferSizeAndAlignWithLength:size options:HeapBlock::getOptions(usage)];
-  return BufferBlock::alignUp(sizeAlign.size, sizeAlign.align);
+  size_t alloc_size = BufferBlock::alignUp(sizeAlign.size, sizeAlign.align);
+  if (m_roundup_power2_divisions > 1 && alloc_size > kMaxSmallAlloc) {
+    alloc_size = roundup_power2_next_division(alloc_size, m_roundup_power2_divisions);
+    alloc_size = BufferBlock::alignUp(alloc_size, sizeAlign.align);
+  }
+  return alloc_size;
 }
 
 void MPSHeapAllocatorImpl::setHighWatermarkRatio(double ratio) {
@@ -148,7 +406,10 @@ bool MPSHeapAllocatorImpl::alloc_buffer(AllocParams& params) {
   // insert heap after a buffer was created on it to update the order of heap's set
   pool.heaps.insert(heap);
   params.buffer_block = new BufferBlock(params.size(), params.requested_size, buffer, heap);
-  m_allocated_buffers[params.buffer_block->buffer] = params.buffer_block;
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    m_allocated_buffers[params.buffer_block->buffer] = params.buffer_block;
+  }
   pool.allocated_size += params.size();
   pool.n_buffers++;
 
@@ -165,7 +426,7 @@ bool MPSHeapAllocatorImpl::alloc_buffer(AllocParams& params) {
   return true;
 }
 
-bool MPSHeapAllocatorImpl::get_free_buffer(AllocParams& params) {
+bool MPSHeapAllocatorImpl::get_free_buffer(AllocParams& params, std::unique_lock<std::mutex>& pool_lock) {
   // this helps to monitor "implicit" allocations from MPS backend and to prevent OOM and system failure.
   if (m_high_watermark_ratio > 0.0 && current_allocated_size() + params.size() > m_max_total_allowed_size) {
     return false;
@@ -193,15 +454,22 @@ bool MPSHeapAllocatorImpl::get_free_buffer(AllocParams& params) {
       if (pool.heaps.lower_bound(&search_key) != pool.heaps.end()) {
         params.buffer_block = nullptr;
       } else if (buffer_block->retainCount() <= 1) {
+        // THREAD SAFETY NOTE (23.14): retainCount() check is safe here because:
+        // 1. We hold pool_lock, preventing concurrent allocator operations on this buffer
+        // 2. GPU command buffer completion can only decrement retainCount (from >1 to 1),
+        //    which is safe - we may think a buffer is busy when it just became available,
+        //    but we will never use a buffer that's actually in-use by the GPU
+        // 3. Only another allocator operation could increment retainCount, which requires pool_lock
+        //
         // otherwise if buffer is releasable immediately, we make room by releasing the
         // buffer and reuse the new space within its heap container for the new smaller buffer allocation
-        release_buffer(buffer_block, false);
+        release_buffer(buffer_block, pool_lock, false);
         // this will skip unnecessary garbage collection as we'll reuse the newly released space
         params.has_memory_pressure = false;
       } else if (params.has_memory_pressure) {
         // the oversized buffer is busy and not reusable at the moment. So release it (and potentially its heap
         // container) in allocator, and ARC will later free up its backing memory when the busy command buffer finishes.
-        release_buffer(buffer_block, true);
+        release_buffer(buffer_block, pool_lock, true);
       } else {
         // only if there's no memory pressure, we'll reuse the oversized buffer
         params.buffer_block = buffer_block;
@@ -233,7 +501,55 @@ BufferBlock* MPSHeapAllocatorImpl::alloc_buffer_block(size_t size, uint32_t usag
   TORCH_CHECK(size < m_max_buffer_size, "Invalid buffer size: ", format_size(size));
 
   size_t alloc_size = get_allocation_size(size, usage);
+
+  // Phase 23.9: Check TLS cache first for small allocations (lock-free fast path)
+  // Only check for non-scalar small allocations
+  if (alloc_size <= kTLSCacheMaxSize && !(usage & UsageFlags::SCALAR)) {
+    // 32.98 fix: Protect the ENTIRE TLS cache operation with s_flush_in_progress_count.
+    // The previous 32.93 fix only protected try_get() internals, but the caller code
+    // that accesses cached_block->heap->pool and pool_mutex AFTER try_get() returns
+    // was unprotected. This left a TOCTOU window where shutdown() could destroy pools
+    // between try_get() returning and this code accessing pool_mutex (UAF).
+    // Now we increment counter BEFORE try_get() and decrement AFTER all pool access.
+    if (!s_allocator_alive.load(std::memory_order_acquire)) {
+      // Fall through to normal allocation path if shutting down
+    } else {
+      s_flush_in_progress_count.fetch_add(1, std::memory_order_acq_rel);
+      auto decrement_guard = c10::make_scope_exit([&]() {
+        s_flush_in_progress_count.fetch_sub(1, std::memory_order_release);
+      });
+      // Double-check after incrementing: if shutdown started between our first check
+      // and increment, skip TLS cache (pools may be getting destroyed).
+      if (s_allocator_alive.load(std::memory_order_acquire)) {
+        TLSBlockCache& cache = get_tls_cache();
+        if (BufferBlock* cached_block = cache.try_get(alloc_size, usage)) {
+          // Found a cached block - reuse it
+          // 27.1 fix: Hold pool lock briefly when modifying shared fields to prevent
+          // data races with threads calling get_allocated_buffer_block()
+          BufferPool& cached_pool = *cached_block->heap->pool;
+          {
+            std::lock_guard<std::mutex> lock(cached_pool.pool_mutex);
+            cached_block->in_use.store(true, std::memory_order_release);
+            cached_block->use_count++;
+            cached_block->requested_size = size;
+            auto* stream = getCurrentMPSStream();
+            cached_block->alloc_stream_id = stream ? static_cast<int64_t>(stream->unwrap().id()) : -1;
+          }
+          m_current_allocated_memory += cached_block->size;
+          return cached_block;  // decrement_guard runs on return
+        }
+      }
+      // decrement_guard runs when falling through (no cached block found)
+    }
+  }
+
   auto& pool = get_pool(size, alloc_size, usage);
+  std::unique_lock<std::mutex> pool_lock(pool.pool_mutex);
+
+  // Phase 24.2: Opportunistically reclaim buffers from completed GPU operations
+  // This is a non-blocking check that can free memory without waiting
+  process_pending_buffers_locked(pool);
+
   AllocParams params(alloc_size, size, &pool);
   // we care about memory pressure if only we're allocating large buffers when the
   // low watermark limit has been reached
@@ -241,24 +557,37 @@ BufferBlock* MPSHeapAllocatorImpl::alloc_buffer_block(size_t size, uint32_t usag
   params.has_unified_memory = m_device.hasUnifiedMemory;
 
   // first, try to get a block from the existing pool.
-  bool block_found = get_free_buffer(params);
+  bool block_found = get_free_buffer(params, pool_lock);
   if (!block_found) {
     // do garbage collection if memory pressure is high and there's enough memory in pool
     if (params.has_memory_pressure && alloc_size < pool.available_size) {
-      garbage_collect_cached_buffers(params);
+      garbage_collect_cached_buffers(params, pool_lock);
+    }
+
+    // Attempt allocate
+    block_found = alloc_buffer(params);
+
+    // Callbacks might release more memory (eg. by forcing a GC in the host language) thus
+    // we can retry getting a free buffer in the pool, before trying to alloc again.
+    if (!block_found) {
+      pool_lock.unlock();
+      trigger_memory_callbacks(nullptr, IMpsAllocatorCallback::EventType::ALLOCATION_FAILED);
+      pool_lock.lock();
+      block_found = get_free_buffer(params, pool_lock);
     }
 
-    block_found =
-        // Attempt allocate
-        alloc_buffer(params) ||
-        // Callbacks might release more memory (eg. by forcing a GC in the host language) thus
-        // we can retry getting a free buffer in the pool, before trying to alloc again.
-        (trigger_memory_callbacks(nullptr, IMpsAllocatorCallback::EventType::ALLOCATION_FAILED) &&
-         get_free_buffer(params)) ||
-        // Free enough available cached blocks to satisfy alloc and retry alloc.
-        (release_available_cached_buffers(params) && alloc_buffer(params)) ||
-        // Free all cached buffers and retry alloc.
-        (release_cached_buffers() && alloc_buffer(params));
+    // Free enough available cached blocks to satisfy alloc and retry alloc.
+    if (!block_found) {
+      block_found = release_available_cached_buffers(params, pool_lock) && alloc_buffer(params);
+    }
+
+    // Free all cached buffers and retry alloc.
+    if (!block_found) {
+      pool_lock.unlock();
+      const bool released = release_cached_buffers();
+      pool_lock.lock();
+      block_found = released && alloc_buffer(params);
+    }
   }
 
   BufferBlock* buffer_block = params.buffer_block;
@@ -295,17 +624,55 @@ BufferBlock* MPSHeapAllocatorImpl::alloc_buffer_block(size_t size, uint32_t usag
                   " pool.");
     }
   }
-  buffer_block->in_use = true;
+  buffer_block->in_use.store(true, std::memory_order_release);
   buffer_block->use_count++;
   m_current_allocated_memory += buffer_block->size;
+  // 24.1: Track which stream allocated this buffer (using ID, not pointer)
+  auto* alloc_stream = getCurrentMPSStream();
+  buffer_block->alloc_stream_id = alloc_stream ? static_cast<int64_t>(alloc_stream->unwrap().id()) : -1;
 
   return buffer_block;
 }
 
 void MPSHeapAllocatorImpl::free_buffer(BufferBlock* buffer_block) {
-  TORCH_INTERNAL_ASSERT(buffer_block->in_use);
+  TORCH_INTERNAL_ASSERT(buffer_block->in_use.load(std::memory_order_acquire));
 
   BufferPool& pool = *buffer_block->heap->pool;
+
+  // 32.73 fix: Check if GPU is still using this buffer before recycling.
+  // If retainCount > 1, the MTLBuffer is retained by an active command buffer.
+  // Adding to available_buffers while GPU is using the buffer causes a data race:
+  // another allocation could reuse the buffer while GPU work is still accessing it.
+  // Defer to buffers_pending_free where process_pending_buffers_locked() will
+  // check again when retainCount drops to 1.
+  if (buffer_block->retainCount() > 1) {
+    pool.buffers_pending_free.insert(buffer_block);
+    return;
+  }
+
+  // 24.7: Check pending events from cross-stream usage before recycling
+  // Remove completed events and check if any are still pending
+  if (!buffer_block->pending_events.empty()) {
+    auto it = buffer_block->pending_events.begin();
+    while (it != buffer_block->pending_events.end()) {
+      if ((*it)->query()) {
+        // Event completed, remove it
+        it = buffer_block->pending_events.erase(it);
+      } else {
+        ++it;
+      }
+    }
+    // If any events still pending, defer recycling
+    if (!buffer_block->pending_events.empty()) {
+      pool.buffers_pending_free.insert(buffer_block);
+      return;
+    }
+  }
+
+  // Clear stream tracking fields now that we're recycling
+  buffer_block->alloc_stream_id = -1;
+  buffer_block->stream_uses_ids.clear();
+
   // Makes sure the BufferBlock* isn't already present in the pool we're freeing it back into.
   TORCH_INTERNAL_ASSERT(pool.available_buffers.insert(buffer_block).second);
   pool.available_size += buffer_block->size;
@@ -316,23 +683,38 @@ void MPSHeapAllocatorImpl::free_buffer(BufferBlock* buffer_block) {
     // returns the MPSEvent back to MPSEventPool
     buffer_block->event.reset(nullptr);
   }
-  buffer_block->in_use = false;
+  buffer_block->in_use.store(false, std::memory_order_release);
 }
 
 BufferBlock* MPSHeapAllocatorImpl::get_allocated_buffer_block(const void* ptr) {
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
   auto it = m_allocated_buffers.find(ptr);
   if (it == m_allocated_buffers.end()) {
     return nullptr;
   }
-  return it->second;
+  BufferBlock* block = it->second;
+  // 32.21 fix: Don't return blocks that have been freed but are in TLS cache.
+  // After free(), in_use is false but block remains in m_allocated_buffers.
+  // Without this check, recordStream()/recordEvents() could modify freed blocks.
+  // in_use is atomic, so this read is safe without holding pool_mutex.
+  if (!block->in_use.load(std::memory_order_acquire)) {
+    return nullptr;
+  }
+  return block;
 }
 
-bool MPSHeapAllocatorImpl::release_buffer(BufferBlock* buffer_block, bool remove_empty_heap) {
+bool MPSHeapAllocatorImpl::release_buffer(
+    BufferBlock* buffer_block,
+    std::unique_lock<std::mutex>& pool_lock,
+    bool remove_empty_heap) {
   HeapBlock* heap_block = buffer_block->heap;
   BufferPool& pool = *heap_block->pool;
   pool.allocated_size -= buffer_block->size;
   pool.available_size -= buffer_block->size;
-  m_allocated_buffers.erase(buffer_block->buffer);
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    m_allocated_buffers.erase(buffer_block->buffer);
+  }
   pool.available_buffers.erase(buffer_block);
   pool.n_buffers--;
   // will re-insert later to keep the heaps list sorted based on heap's new available size (if heap not empty)
@@ -365,24 +747,52 @@ bool MPSHeapAllocatorImpl::release_buffer(BufferBlock* buffer_block, bool remove
     // size of the heap cannot be updated and we should defer updating until command buffer finishes.
     if (retainCount > 1) {
       pool.heaps_pending_update.insert(heap_block);
-      m_mutex.unlock();
-      m_stream->addCompletedHandler(^(id<MTLCommandBuffer>) {
-        std::lock_guard<std::recursive_mutex> lock(m_mutex);
-        // check if the heap block still exists
-        if (pool.heaps_pending_update.find(heap_block) != pool.heaps_pending_update.end()) {
-          pool.heaps_pending_update.erase(heap_block);
+      pool_lock.unlock();
+      // 32.62 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+      // static destruction when g_pool_alive is false. In that case, skip the deferred
+      // update since we're shutting down anyway.
+      MPSStream* stream = getCurrentMPSStream();
+      if (!stream) {
+        pool_lock.lock();
+        // Shutdown path: skip deferred update, clean up pending entry
+        pool.heaps_pending_update.erase(heap_block);
+        return false;
+      }
+      // 32.67 ABA fix: Capture heap_id to avoid operating on a different heap
+      // that was allocated at the same memory address after this heap was deleted.
+      // Without this check, if heap H1 is deleted and heap H2 is created at the
+      // same address, the find(heap_block) would find H2 and incorrectly operate on it.
+      const id_t captured_heap_id = heap_block->heap_id;
+      // 32.108 fix: Track pending completion handlers to prevent UAF during shutdown.
+      // Metal completion handlers run asynchronously AFTER waitUntilCompleted returns.
+      // Without this, shutdown() could destroy pools while this handler is still pending.
+      s_pending_completion_handlers.fetch_add(1, std::memory_order_acq_rel);
+      stream->addCompletedHandler(^(id<MTLCommandBuffer>) {
+        // Decrement counter when handler completes (always, even on early exit)
+        auto decrement_guard = c10::make_scope_exit([&]() {
+          s_pending_completion_handlers.fetch_sub(1, std::memory_order_release);
+        });
+        // 32.108 fix: Skip pool access if allocator is shutting down - pools may be destroyed
+        if (!s_allocator_alive.load(std::memory_order_acquire)) {
+          return;
+        }
+        std::lock_guard<std::mutex> lock(pool.pool_mutex);
+        // check if the heap block still exists AND has the same heap_id
+        auto it = pool.heaps_pending_update.find(heap_block);
+        if (it != pool.heaps_pending_update.end() && (*it)->heap_id == captured_heap_id) {
+          pool.heaps_pending_update.erase(it);
           pool.heaps.erase(heap_block);
           heap_block->updateAvailableSize();
           pool.heaps.insert(heap_block);
         }
       });
-      m_mutex.lock();
+      pool_lock.lock();
     }
   }
   return false;
 }
 
-void MPSHeapAllocatorImpl::release_buffers(BufferPool& pool) {
+void MPSHeapAllocatorImpl::release_buffers(BufferPool& pool, std::unique_lock<std::mutex>& pool_lock) {
   if (pool.available_buffers.empty()) {
     return;
   }
@@ -393,15 +803,13 @@ void MPSHeapAllocatorImpl::release_buffers(BufferPool& pool) {
               << ((pool.usage & UsageFlags::SCALAR) ? " scalar" : "")
               << " pool (total size: " << format_size(pool.allocated_size) << ", #buffers: " << pool.n_buffers << ")\n";
   }
-  auto it = pool.available_buffers.begin();
-  while (it != pool.available_buffers.end()) {
-    BufferBlock* buffer_block = *it;
-    ++it;
-    release_buffer(buffer_block);
+  while (!pool.available_buffers.empty()) {
+    BufferBlock* buffer_block = *pool.available_buffers.begin();
+    release_buffer(buffer_block, pool_lock);
   }
 }
 
-bool MPSHeapAllocatorImpl::release_available_cached_buffers(AllocParams& params) {
+bool MPSHeapAllocatorImpl::release_available_cached_buffers(AllocParams& params, std::unique_lock<std::mutex>& pool_lock) {
   BufferPool& pool = *params.pool;
 
   if (pool.available_buffers.empty()) {
@@ -416,9 +824,9 @@ bool MPSHeapAllocatorImpl::release_available_cached_buffers(AllocParams& params)
       totalReleased += (*it)->size;
       if (it != pool.available_buffers.begin()) {
         --it;
-        release_buffer(*cur);
+        release_buffer(*cur, pool_lock);
       } else {
-        release_buffer(*cur);
+        release_buffer(*cur, pool_lock);
         break;
       }
     }
@@ -426,7 +834,7 @@ bool MPSHeapAllocatorImpl::release_available_cached_buffers(AllocParams& params)
       return false;
     }
   } else {
-    release_buffer(*it);
+    release_buffer(*it, pool_lock);
   }
   return true;
 }
@@ -436,30 +844,34 @@ bool MPSHeapAllocatorImpl::release_cached_buffers() {
     std::cerr << "Attempting to release cached buffers (MPS allocated: " << format_size(m_total_allocated_memory)
               << ", other allocations: " << format_size(current_allocated_size() - m_total_allocated_memory) << ")\n";
   }
-  // before releasing the buffers make sure the command buffer has finished.
-  // we need to release the lock temporarily as synchronizing may cause deadlock with completion handlers.
-  m_mutex.unlock();
-  auto stream = getDefaultMPSStream();
-  dispatch_sync(stream->queue(), ^() {
-    stream->synchronize(SyncType::COMMIT_AND_WAIT);
-  });
-  m_mutex.lock();
+  // 32.30 fix: Synchronize ALL streams before releasing buffers.
+  // In multi-stream parallel code, buffers may be in use by non-default streams.
+  // Only synchronizing the default stream risks use-after-free on other streams.
+  // 32.82 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed (called via emptyCache() from destructor).
+  if (MPSStreamPool::isPoolAlive()) {
+    MPSStreamPool::instance().synchronizeAllStreams();
+  }
+  // 32.59 fix: Do NOT hold m_mutex while acquiring pool_mutex.
+  // See freeInactiveBuffers() for detailed explanation of lock order inversion.
+  // m_pools is never modified after init, so iteration is safe without m_mutex.
   // Free all cached blocks to system allocator
   for (const auto& poolIt : m_pools) {
     BufferPool& pool = *poolIt.second;
-    release_buffers(pool);
+    std::unique_lock<std::mutex> pool_lock(pool.pool_mutex);
+    release_buffers(pool, pool_lock);
   }
   return true;
 }
 
-void MPSHeapAllocatorImpl::garbage_collect_cached_buffers(AllocParams& params) {
+void MPSHeapAllocatorImpl::garbage_collect_cached_buffers(AllocParams& params, std::unique_lock<std::mutex>& pool_lock) {
   // skip garbage collection if memory pressure has already relieved
   if (current_allocated_size() < m_low_watermark_limit) {
     return;
   }
   // attempt to collect garbage until we reach below low watermark limit
   const auto target_size = current_allocated_size() - m_low_watermark_limit;
-  const BufferPool& pool = *params.pool;
+  BufferPool& pool = *params.pool;
   // calculate the total age of the free-able blocks. We'll use it later to get the average age threshold.
   double total_age = 0.0;
   unsigned int freeable_block_count = 0, freed_count = 0;
@@ -492,7 +904,7 @@ void MPSHeapAllocatorImpl::garbage_collect_cached_buffers(AllocParams& params) {
         total_age -= buffer_block->gc_count;
         freeable_block_count--;
         freed_count++;
-        release_buffer(buffer_block, !buffer_block->heap->is_split);
+        release_buffer(buffer_block, pool_lock, !buffer_block->heap->is_split);
       }
     }
   }
@@ -504,31 +916,61 @@ void MPSHeapAllocatorImpl::garbage_collect_cached_buffers(AllocParams& params) {
   }
 }
 
+// Phase 24.2: Opportunistically process pending buffers for a pool
+// This allows reclaiming memory from completed GPU operations without blocking
+// Caller must already hold the pool lock
+void MPSHeapAllocatorImpl::process_pending_buffers_locked(BufferPool& pool) {
+  if (pool.buffers_pending_free.empty()) {
+    return;
+  }
+  // THREAD-SAFETY (32.32): Collect blocks to free before modifying the set.
+  // free_buffer() can re-insert into buffers_pending_free (line 553) if pending_events
+  // are present. Since buffers_pending_free is std::unordered_set, insertion may cause
+  // rehashing which invalidates all iterators. Collect first, then process.
+  std::vector<BufferBlock*> blocks_to_free;
+  for (auto it = pool.buffers_pending_free.begin(); it != pool.buffers_pending_free.end();) {
+    BufferBlock* buffer_block = *it;
+    // retainCount <= 1 means GPU is done with this buffer
+    if (buffer_block->retainCount() <= 1) {
+      blocks_to_free.push_back(buffer_block);
+      it = pool.buffers_pending_free.erase(it);
+    } else {
+      ++it;
+    }
+  }
+  // Now safe to call free_buffer() - iteration is complete
+  for (BufferBlock* block : blocks_to_free) {
+    free_buffer(block);
+  }
+}
+
 // public interface to MPSAllocator
 id<MTLBuffer> MPSHeapAllocatorImpl::malloc(size_t size, uint32_t usage) {
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
   BufferBlock* buffer_block = alloc_buffer_block(size, usage);
   return buffer_block ? buffer_block->buffer : nullptr;
 }
 
 bool MPSHeapAllocatorImpl::isSharedBuffer(const void* ptr) {
+  // 32.19 TOCTOU fix: Hold m_mutex during entire operation to prevent
+  // release_buffer() from deleting the BufferBlock between lookup and access.
+  // release_buffer() requires m_mutex to erase from m_allocated_buffers before delete.
   std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
-  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
-  // it's OK for the buffer_block to not exist yet
-  return buffer_block && (buffer_block->heap->pool->usage & UsageFlags::SHARED);
+  auto it = m_allocated_buffers.find(ptr);
+  if (it == m_allocated_buffers.end()) {
+    return false;
+  }
+  BufferBlock* buffer_block = it->second;
+  // pool->usage is a constant, safe to read under m_mutex
+  return buffer_block->heap->pool->usage & UsageFlags::SHARED;
 }
 
 id<MTLBuffer> MPSHeapAllocatorImpl::allocScalarBufferWithValue(void* value, size_t size) {
-  BufferBlock* buffer_block = nullptr;
+  BufferBlock* buffer_block = alloc_buffer_block(size, UsageFlags::SCALAR);
+  if (!buffer_block) {
+    return nullptr;
+  }
   {
-    std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
-    buffer_block = alloc_buffer_block(size, UsageFlags::SCALAR);
-    if (!buffer_block) {
-      return nullptr;
-    }
+    std::lock_guard<std::mutex> lock(buffer_block->heap->pool->pool_mutex);
     if (!buffer_block->cpu_ptr) {
       buffer_block->cpu_ptr = [buffer_block->buffer contents];
     }
@@ -538,155 +980,497 @@ id<MTLBuffer> MPSHeapAllocatorImpl::allocScalarBufferWithValue(void* value, size
   return buffer_block->buffer;
 }
 
-std::pair<const void*, uint32_t> MPSHeapAllocatorImpl::getSharedBufferPtr(const void* ptr) {
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
+namespace {
+void ReleaseSharedBufferPtrMapping(void* ctx) {
+  if (!ctx) {
+    return;
+  }
+  id<MTLBuffer> buffer = (id<MTLBuffer>)ctx;
+  [buffer release];
+}
+} // namespace
 
-  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
-  // return if buffer was not allocated on MPSAllocator or isn't a Shared buffer
-  if (!buffer_block || !(buffer_block->heap->pool->usage & UsageFlags::SHARED)) {
-    return {nullptr, 0};
+c10::DataPtr MPSHeapAllocatorImpl::getSharedBufferPtr(const void* ptr) {
+  // 32.19 TOCTOU fix: Use double-check pattern.
+  BufferPool* pool = nullptr;
+  BufferBlock* buffer_block = nullptr;
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    if (it == m_allocated_buffers.end()) {
+      return {nullptr, c10::Device(c10::DeviceType::CPU)};
+    }
+    buffer_block = it->second;
+    if (!(buffer_block->heap->pool->usage & UsageFlags::SHARED)) {
+      return {nullptr, c10::Device(c10::DeviceType::CPU)}; // Not a shared buffer
+    }
+    pool = buffer_block->heap->pool;
+  }
+  std::lock_guard<std::mutex> pool_lock(pool->pool_mutex);
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    if (it == m_allocated_buffers.end() || it->second != buffer_block) {
+      return {nullptr, c10::Device(c10::DeviceType::CPU)}; // Buffer was released between locks
+    }
+  }
+  // 32.78 fix: Check if block is still in use (not freed to TLS cache).
+  // The 32.76 fix added this check to recordStream(), but getSharedBufferPtr()
+  // was missed. Without this check, a block freed to TLS cache (in_use=false)
+  // can have its cpu_ptr accessed and buffer retained, corrupting the cached
+  // block state when TLS cache reuses or flushes it.
+  if (!buffer_block->in_use.load(std::memory_order_acquire)) {
+    return {nullptr, c10::Device(c10::DeviceType::CPU)}; // Block was freed to TLS cache
   }
   if (!buffer_block->cpu_ptr) {
     buffer_block->cpu_ptr = [buffer_block->buffer contents];
   }
-  return {buffer_block->cpu_ptr, buffer_block->retainCount()};
+  if (!buffer_block->cpu_ptr) {
+    return {nullptr, c10::Device(c10::DeviceType::CPU)};
+  }
+  id<MTLBuffer> buffer = buffer_block->buffer;
+  [buffer retain];
+  return {buffer_block->cpu_ptr, (void*)buffer, &ReleaseSharedBufferPtrMapping, c10::Device(c10::DeviceType::CPU)};
 }
 
 bool MPSHeapAllocatorImpl::recordEvents(c10::ArrayRef<const void*> buffers) {
   bool recordedEvent = false;
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
+
+  // THREAD-SAFETY FIX: Get the current thread's stream instead of using nullptr
+  // which would default to stream 0 and cause cross-stream race conditions.
+  // Each thread should record events on its own stream.
+  MPSStream* currentStream = getCurrentMPSStream();
+  // 32.77 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+  // static destruction when g_pool_alive is false. Without this check, we would
+  // pass nullptr to record() which asserts TORCH_INTERNAL_ASSERT(stream).
+  if (!currentStream) {
+    return false;  // Pool is destroyed, cannot record events
+  }
 
   for (const auto& buffer : buffers) {
-    BufferBlock* buffer_block = get_allocated_buffer_block(buffer);
-    // return if buffer was not allocated on MPSAllocator or isn't a Shared buffer
-    if (buffer_block && (buffer_block->heap->pool->usage & UsageFlags::SHARED)) {
-      if (!buffer_block->event) {
-        buffer_block->event = m_event_pool->acquireEvent(false, nullptr);
-        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(buffer_block->event);
+    // 32.19 TOCTOU fix: Use double-check pattern for each buffer.
+    BufferPool* pool = nullptr;
+    BufferBlock* buffer_block = nullptr;
+    {
+      std::lock_guard<std::recursive_mutex> lock(m_mutex);
+      auto it = m_allocated_buffers.find(buffer);
+      if (it == m_allocated_buffers.end()) {
+        continue;
+      }
+      buffer_block = it->second;
+      if (!(buffer_block->heap->pool->usage & UsageFlags::SHARED)) {
+        continue;  // Not a shared buffer
       }
-      buffer_block->event->record(/*needsLock*/ false);
-      recordedEvent = true;
+      pool = buffer_block->heap->pool;
     }
+    std::lock_guard<std::mutex> pool_lock(pool->pool_mutex);
+    {
+      std::lock_guard<std::recursive_mutex> lock(m_mutex);
+      auto it = m_allocated_buffers.find(buffer);
+      if (it == m_allocated_buffers.end() || it->second != buffer_block) {
+        continue;  // Buffer was released between locks
+      }
+    }
+    // 32.78 fix: Check if block is still in use (not freed to TLS cache).
+    // Same pattern as 32.76 fix for recordStream().
+    if (!buffer_block->in_use.load(std::memory_order_acquire)) {
+      continue;  // Block was freed to TLS cache - caller has a stale reference
+    }
+    if (!buffer_block->event) {
+      buffer_block->event = m_event_pool->acquireEvent(false, currentStream);
+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(buffer_block->event);
+    }
+    buffer_block->event->record(currentStream, /*needsLock*/ false);
+    recordedEvent = true;
   }
   return recordedEvent;
 }
 
 bool MPSHeapAllocatorImpl::waitForEvents(c10::ArrayRef<const void*> buffers) {
-  std::vector<BufferBlock*> buffer_blocks;
-  {
-    std::lock_guard<std::recursive_mutex> lock(m_mutex);
-    for (const auto& buffer : buffers) {
-      BufferBlock* buffer_block = get_allocated_buffer_block(buffer);
-      // wait on event if "shared" buffer was allocated on MPSAllocator and
-      // or actually needs waiting (based on retainCount)
-      if (buffer_block && (buffer_block->heap->pool->usage & UsageFlags::SHARED) && buffer_block->retainCount() > 1 &&
-          buffer_block->event) {
-        buffer_blocks.push_back(buffer_block);
-      }
-    }
-  }
+  // 27.7 fix: Hold lock during synchronize() to prevent use-after-free.
+  // Previously collected raw event pointers outside lock, risking dangling
+  // pointers if another thread freed the buffer and returned event to pool.
   bool waitedForEvent = false;
 
-  for (const auto& buffer_block : buffer_blocks) {
-    // check for retain count again as the previous wait might have released the buffer
+  for (const auto& buffer : buffers) {
+    // 32.19 TOCTOU fix: Use double-check pattern for each buffer.
+    BufferPool* pool = nullptr;
+    BufferBlock* buffer_block = nullptr;
+    {
+      std::lock_guard<std::recursive_mutex> lock(m_mutex);
+      auto it = m_allocated_buffers.find(buffer);
+      if (it == m_allocated_buffers.end()) {
+        continue;
+      }
+      buffer_block = it->second;
+      if (!(buffer_block->heap->pool->usage & UsageFlags::SHARED)) {
+        continue;  // Not a shared buffer
+      }
+      pool = buffer_block->heap->pool;
+    }
+    std::lock_guard<std::mutex> pool_lock(pool->pool_mutex);
+    {
+      std::lock_guard<std::recursive_mutex> lock(m_mutex);
+      auto it = m_allocated_buffers.find(buffer);
+      if (it == m_allocated_buffers.end() || it->second != buffer_block) {
+        continue;  // Buffer was released between locks
+      }
+    }
+    // 32.78 fix: Check if block is still in use (not freed to TLS cache).
+    // Same pattern as 32.76 fix for recordStream().
+    if (!buffer_block->in_use.load(std::memory_order_acquire)) {
+      continue;  // Block was freed to TLS cache - caller has a stale reference
+    }
+    // wait on event if "shared" buffer was allocated on MPSAllocator and
+    // or actually needs waiting (based on retainCount)
     if (buffer_block->retainCount() > 1) {
+      if (!buffer_block->event) {
+        return false;
+      }
+      // Synchronize while holding lock - event cannot be freed by another thread
       bool waitedOnCPU = buffer_block->event->synchronize();
       if (waitedOnCPU) {
-        // after waiting, it's a good time to free some pending inactive buffers
-        freeInactiveBuffers();
-        waitedForEvent |= buffer_block->retainCount() <= 1;
+        waitedForEvent = true;
       } else {
-        // even if one of the buffers weren't recorded beforehand, we return
-        // without continuing with other buffers since retainCount > 1
-        waitedForEvent = false;
-        break;
+        // The event has not been recorded (or was already signaled); callers
+        // expect "did wait" semantics here, so stop early.
+        return waitedForEvent;
       }
     }
   }
+  // Free inactive buffers after all waits complete (outside per-buffer locks)
+  if (waitedForEvent) {
+    freeInactiveBuffers();
+  }
   return waitedForEvent;
 }
 
+// 24.1/24.7: CUDA-style recordStream() for cross-stream synchronization
+void MPSHeapAllocatorImpl::recordStream(const void* ptr, MPSStream* stream) {
+  if (!ptr || !stream) {
+    return;
+  }
+  // 32.19 TOCTOU fix: Use double-check pattern to avoid deadlock.
+  BufferPool* pool = nullptr;
+  BufferBlock* buffer_block = nullptr;
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    if (it == m_allocated_buffers.end()) {
+      return;
+    }
+    buffer_block = it->second;
+    pool = buffer_block->heap->pool;
+  }
+  std::lock_guard<std::mutex> pool_lock(pool->pool_mutex);
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    if (it == m_allocated_buffers.end() || it->second != buffer_block) {
+      return;  // Buffer was released between locks
+    }
+  }
+
+  // 32.76 fix: Check if block is still in use (not freed to TLS cache).
+  // The 32.21 fix added this check to get_allocated_buffer_block(), but
+  // recordStream() bypasses that function by accessing m_allocated_buffers
+  // directly. Without this check, a block freed to TLS cache (in_use=false)
+  // can have its stream_uses_ids modified, corrupting the cached block state.
+  if (!buffer_block->in_use.load(std::memory_order_acquire)) {
+    return;  // Block was freed to TLS cache - caller has a stale reference
+  }
+
+  // If same as allocating stream, no cross-stream sync needed
+  int64_t stream_id = stream ? static_cast<int64_t>(stream->unwrap().id()) : -1;
+  if (stream_id == buffer_block->alloc_stream_id) {
+    return;
+  }
+
+  // Track this stream if not already recorded
+  if (buffer_block->stream_uses_ids.insert(stream_id).second) {
+    // First time this stream uses this buffer - create sync event
+    // The event will be signaled when the stream completes current work
+    MPSEventPtr event = m_event_pool->acquireEvent(false, stream);
+    // 32.20 fix: Use needsLock=false to avoid deadlock.
+    // With needsLock=true, record() uses dispatch_sync to stream's queue.
+    // If another thread on that queue waits for pool.pool_mutex (which we hold),
+    // we deadlock. With needsLock=false, we take event's mutex directly.
+    // This is safe because MPSStream has its own mutex for internal state.
+    event->record(stream, false, false);  // needsLock=false avoids dispatch_sync
+    buffer_block->pending_events.push_back(std::move(event));
+  }
+}
+
 id_t MPSHeapAllocatorImpl::getBufferId(const void* ptr) {
+  // 32.19 TOCTOU fix: Hold m_mutex during entire operation.
+  // buf_id is constant, safe to read under m_mutex alone.
   std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
-  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
-  return buffer_block ? buffer_block->buf_id : 0;
+  auto it = m_allocated_buffers.find(ptr);
+  if (it == m_allocated_buffers.end()) {
+    return 0;
+  }
+  return it->second->buf_id;
 }
 
 ssize_t MPSHeapAllocatorImpl::getUnalignedBufferSize(const void* ptr) {
+  // 32.19 TOCTOU fix: Hold m_mutex during entire operation.
+  // requested_size is set at allocation and never modified, so m_mutex alone is sufficient.
   std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
-  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
-  if (buffer_block) {
-    return (ssize_t)buffer_block->requested_size;
+  auto it = m_allocated_buffers.find(ptr);
+  if (it == m_allocated_buffers.end()) {
+    return -1;  // -1 indicates the passed buffer pointer wasn't found
   }
-  // -1 indicates the passed buffer pointer wasn't found
-  return -1;
+  return (ssize_t)it->second->requested_size;
 }
 
 void MPSHeapAllocatorImpl::setBufferShape(const void* ptr, const IntArrayRef& shape) {
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
-  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
-  TORCH_INTERNAL_ASSERT(buffer_block, "failed to find the buffer ", ptr);
+  // 32.19 TOCTOU fix: Must verify buffer is still allocated after taking pool_mutex.
+  // Shape is mutable and requires pool_mutex protection. We use a double-check pattern
+  // to avoid deadlock from opposite lock ordering with release_buffer().
+  BufferPool* pool = nullptr;
+  BufferBlock* buffer_block = nullptr;
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    TORCH_INTERNAL_ASSERT(it != m_allocated_buffers.end(), "failed to find the buffer ", ptr);
+    buffer_block = it->second;
+    pool = buffer_block->heap->pool;
+  }
+  // Now take pool_mutex. Since we released m_mutex, we must re-verify.
+  std::lock_guard<std::mutex> pool_lock(pool->pool_mutex);
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    // Re-verify buffer is still in map (could have been released between locks)
+    auto it = m_allocated_buffers.find(ptr);
+    TORCH_INTERNAL_ASSERT(it != m_allocated_buffers.end(), "buffer was released during setBufferShape");
+    TORCH_INTERNAL_ASSERT(it->second == buffer_block, "buffer_block changed unexpectedly");
+  }
+  // 32.86 fix: Check if block is still in use (not freed to TLS cache or released).
+  // The 32.76/32.78 fixes added this check to recordStream/recordEvents/waitForEvents/getSharedBufferPtr.
+  // Without this check, a freed block could be re-allocated to another thread, and we would
+  // corrupt that thread's buffer data by writing to shape.
+  if (!buffer_block->in_use.load(std::memory_order_acquire)) {
+    return;  // Block was freed - caller has stale reference
+  }
   // note that the IntArrayRef doesn't own the underlying data, and the backing
   // memory for shape data must persist as long as the buffer is in use.
   // So we need to copy to vector.
   buffer_block->shape = shape.vec();
 }
 
-IntArrayRef MPSHeapAllocatorImpl::getBufferShape(const void* ptr) {
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
-  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
-  if (buffer_block && !buffer_block->shape.empty()) {
-    return IntArrayRef{buffer_block->shape};
-  }
-  return IntArrayRef();
-}
-
-void MPSHeapAllocatorImpl::free(void* ptr) {
+// 32.96 fix: Return std::vector<int64_t> instead of IntArrayRef.
+// The old implementation returned IntArrayRef which is a non-owning view.
+// After the function returns, pool_mutex is released and other threads could
+// modify buffer_block->shape, making the IntArrayRef dangling (use-after-free).
+// Returning a copy (std::vector) ensures the caller has valid, owned data.
+std::vector<int64_t> MPSHeapAllocatorImpl::getBufferShape(const void* ptr) {
+  // 32.19 TOCTOU fix: Use double-check pattern to avoid deadlock.
+  BufferPool* pool = nullptr;
   BufferBlock* buffer_block = nullptr;
   {
     std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    if (it == m_allocated_buffers.end()) {
+      return {};
+    }
+    buffer_block = it->second;
+    pool = buffer_block->heap->pool;
+  }
+  std::lock_guard<std::mutex> pool_lock(pool->pool_mutex);
+  {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    auto it = m_allocated_buffers.find(ptr);
+    if (it == m_allocated_buffers.end() || it->second != buffer_block) {
+      return {};  // Buffer was released between locks
+    }
+  }
+  // 32.86 fix: Check if block is still in use (not freed to TLS cache or released).
+  // Same pattern as 32.76/32.78 fixes for other buffer lookup functions.
+  if (!buffer_block->in_use.load(std::memory_order_acquire)) {
+    return {};  // Block was freed - caller has stale reference
+  }
+  // Return a copy of the shape vector - safe to use after lock is released
+  return buffer_block->shape;
+}
 
-    buffer_block = get_allocated_buffer_block(ptr);
-    TORCH_INTERNAL_ASSERT(buffer_block);
-    const BufferPool& pool = *buffer_block->heap->pool;
-    if (!(pool.usage & UsageFlags::SCALAR)) {
-      free_buffer(buffer_block);
-      return;
+void MPSHeapAllocatorImpl::free(void* ptr) {
+  BufferBlock* buffer_block = get_allocated_buffer_block(ptr);
+  TORCH_INTERNAL_ASSERT(buffer_block);
+  BufferPool& pool = *buffer_block->heap->pool;
+  if (!(pool.usage & UsageFlags::SCALAR)) {
+    // Phase 23.9: Try to cache small blocks in TLS for lock-free reuse
+    // 32.57 fix: Take pool lock BEFORE reading pending_events/stream_uses_ids.
+    // recordStream() modifies stream_uses_ids under pool_mutex, so reading
+    // without lock is a data race (concurrent read/write to std::unordered_set).
+    std::lock_guard<std::mutex> lock(pool.pool_mutex);
+    // Only cache if: small block, no pending cross-stream events, GPU not using buffer,
+    // and TLS cache has room
+    // 32.75 fix: Added retainCount check. Without this, buffers with active GPU references
+    // (retainCount > 1) could enter the TLS cache, and when the cache flushes, go directly
+    // to available_buffers bypassing the retainCount check in free_buffer(). This causes
+    // the same data race that 32.73 fixed: another thread could reuse the buffer while
+    // the GPU is still accessing it.
+    if ((pool.usage & UsageFlags::SMALL) &&
+        buffer_block->pending_events.empty() &&
+        buffer_block->stream_uses_ids.empty() &&
+        buffer_block->retainCount() <= 1) {
+      TLSBlockCache& cache = get_tls_cache();
+      if (cache.try_put(buffer_block)) {
+        // Successfully cached - modify shared fields (already have lock)
+        buffer_block->alloc_stream_id = -1;
+        buffer_block->shape.clear();
+        if (buffer_block->event) {
+          buffer_block->event.reset(nullptr);
+        }
+        buffer_block->in_use.store(false, std::memory_order_release);
+        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(m_current_allocated_memory >= buffer_block->size);
+        m_current_allocated_memory -= buffer_block->size;
+        return;
+      }
     }
+    // TLS cache not applicable or full - free to pool (already have lock)
+    free_buffer(buffer_block);
+    return;
   }
   // we sync the scalar pool manually with completion handler at the time buffer is
-  // freed when the MPSScalar instance goes our of scope
-  m_stream->addCompletedHandler(^(id<MTLCommandBuffer>) {
-    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+  // freed when the MPSScalar instance goes out of scope
+  // 32.61 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+  // static destruction when g_pool_alive is false. In that case, free synchronously.
+  MPSStream* stream = getCurrentMPSStream();
+  if (!stream) {
+    // Shutdown path: pool is destroyed, free buffer synchronously
+    std::lock_guard<std::mutex> lock(pool.pool_mutex);
+    free_buffer(buffer_block);
+    return;
+  }
+  // 32.108 fix: Track pending completion handlers to prevent UAF during shutdown.
+  // Metal completion handlers run asynchronously AFTER waitUntilCompleted returns.
+  // Without this, shutdown() could destroy pools while this handler is still pending.
+  s_pending_completion_handlers.fetch_add(1, std::memory_order_acq_rel);
+  stream->addCompletedHandler(^(id<MTLCommandBuffer>) {
+    // Decrement counter when handler completes (always, even on early exit)
+    auto decrement_guard = c10::make_scope_exit([&]() {
+      s_pending_completion_handlers.fetch_sub(1, std::memory_order_release);
+    });
+    // 32.108 fix: Skip pool access if allocator is shutting down - pools may be destroyed
+    if (!s_allocator_alive.load(std::memory_order_acquire)) {
+      return;
+    }
+    std::lock_guard<std::mutex> lock(pool.pool_mutex);
     free_buffer(buffer_block);
   });
 }
 
 void MPSHeapAllocatorImpl::freeInactiveBuffers() {
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
-
+  // 32.59 fix: Do NOT hold m_mutex while acquiring pool_mutex.
+  // Holding m_mutex -> pool_mutex here causes lock order inversion with
+  // recordStream/recordEvents/waitForEvents which acquire pool_mutex -> m_mutex.
+  // This can cause deadlock: Thread A holds m_mutex waiting for pool_mutex,
+  // Thread B holds pool_mutex waiting for m_mutex.
+  //
+  // m_pools is populated in init_buffer_pools() and never modified after,
+  // so iteration without m_mutex is safe. (32.23 added m_mutex for hypothetical
+  // future pool addition, but that creates the deadlock.)
   for (const auto& poolIt : m_pools) {
     BufferPool& pool = *poolIt.second;
+    std::lock_guard<std::mutex> lock(pool.pool_mutex);
     if (!pool.buffers_pending_free.empty()) {
-      for (auto it = pool.buffers_pending_free.begin(), last = pool.buffers_pending_free.end(); it != last;) {
+      // THREAD-SAFETY (32.32): Same fix as process_pending_buffers_locked().
+      // Collect blocks first to avoid iterator invalidation from rehashing.
+      std::vector<BufferBlock*> blocks_to_free;
+      for (auto it = pool.buffers_pending_free.begin(); it != pool.buffers_pending_free.end();) {
         BufferBlock* buffer_block = *it;
         if (buffer_block->retainCount() <= 1) {
+          blocks_to_free.push_back(buffer_block);
           it = pool.buffers_pending_free.erase(it);
-          free_buffer(buffer_block);
         } else {
           ++it;
         }
       }
+      for (BufferBlock* block : blocks_to_free) {
+        free_buffer(block);
+      }
+    }
+  }
+}
+
+void MPSHeapAllocatorImpl::shutdown() {
+  // Mark allocator as no longer alive FIRST (before any other cleanup)
+  // This prevents TLS caches on other threads from trying to flush during destruction
+  s_allocator_alive.store(false, std::memory_order_release);
+
+  // 32.68 fix: Wait for any threads currently in TLSBlockCache::flush() to complete.
+  // The double-check in flush() ensures threads that see s_allocator_alive=false after
+  // incrementing the counter will decrement and exit.
+  // 32.69 fix: Increased timeout from 10ms to 5 seconds. 5 seconds is long enough
+  // for any reasonable flush to complete under normal conditions.
+  // 32.72 fix: On timeout, we now TORCH_CHECK(false) instead of warn-and-continue.
+  // Previously, timeout would continue to destruction, causing UAF when flush()
+  // accessed pool.pool_mutex after pools were destroyed. Clean abort is safer.
+  // Using exponential backoff: spin fast initially, then sleep longer.
+  constexpr int kFastSpinCount = 1000;     // First 1000 iterations: 1us each (~1ms)
+  constexpr int kSlowSpinCount = 5000;     // Next 5000 iterations: 1ms each (~5s)
+  int spin_count = 0;
+  while (s_flush_in_progress_count.load(std::memory_order_acquire) > 0) {
+    if (spin_count < kFastSpinCount) {
+      std::this_thread::sleep_for(std::chrono::microseconds(1));
+    } else if (spin_count < kFastSpinCount + kSlowSpinCount) {
+      std::this_thread::sleep_for(std::chrono::milliseconds(1));
+    } else {
+      // 32.72 fix: FATAL error instead of warn-and-continue.
+      // If we continue past this point with flush() still in progress, the subsequent
+      // pool destruction causes use-after-free when flush() accesses pool.pool_mutex.
+      // A 5+ second timeout indicates something is seriously wrong (deadlock, extreme
+      // contention, or bug in flush logic). Clean abort is safer than UAF.
+      TORCH_CHECK(false,
+          "MPSAllocator::shutdown: FATAL - timeout after 5s waiting for TLS cache flush. "
+          "This indicates a deadlock or extreme contention. "
+          "s_flush_in_progress_count=", s_flush_in_progress_count.load(std::memory_order_acquire),
+          ". Cannot continue destruction safely due to use-after-free risk.");
+    }
+    ++spin_count;
+  }
+
+  // THREAD-SAFETY (23.18): Synchronize all streams before emptying cache to ensure
+  // completion handlers have run. This prevents dangling pool references in handlers
+  // that were added via addCompletedHandler in release_buffer/release_heap.
+  // THREAD-SAFETY (30.1): Check if pool is still alive - static destruction order
+  // is undefined and pool may already be destroyed (consistent with MPSProfiler fix 29.3).
+  try {
+    if (MPSStreamPool::isPoolAlive()) {
+      MPSStreamPool::instance().synchronizeAllStreams();
     }
+  } catch (...) {
+    // Ignore exceptions during destruction - streams may already be torn down
+  }
+
+  // 32.108 fix: Wait for pending completion handlers to finish before destroying pools.
+  // Metal completion handlers run asynchronously on a separate thread AFTER
+  // waitUntilCompleted returns. Without this wait, pools could be destroyed while
+  // handlers are still executing, causing UAF on pool.pool_mutex.
+  // Note: Handlers that see s_allocator_alive=false will skip pool access and exit early.
+  spin_count = 0;
+  while (s_pending_completion_handlers.load(std::memory_order_acquire) > 0) {
+    if (spin_count < kFastSpinCount) {
+      std::this_thread::sleep_for(std::chrono::microseconds(1));
+    } else if (spin_count < kFastSpinCount + kSlowSpinCount) {
+      std::this_thread::sleep_for(std::chrono::milliseconds(1));
+    } else {
+      // If handlers haven't completed after 5+ seconds, something is seriously wrong.
+      // Unlike TLS flush, completion handlers should complete quickly after synchronize.
+      TORCH_WARN(
+          "MPSAllocator::shutdown: timeout after 5s waiting for completion handlers. "
+          "s_pending_completion_handlers=", s_pending_completion_handlers.load(std::memory_order_acquire),
+          ". Proceeding with shutdown (handlers will skip pool access due to alive flag).");
+      break;  // Safe to continue because handlers check s_allocator_alive before pool access
+    }
+    ++spin_count;
   }
+
+  emptyCache();
 }
 
 void MPSHeapAllocatorImpl::emptyCache() {
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
   release_cached_buffers();
 }
 
@@ -745,7 +1529,7 @@ struct TORCH_API MPSAllocator final : public IMPSAllocator {
     id<MTLBuffer> buf = _getAllocImpl().allocScalarBufferWithValue(value, size);
     return {buf, buf, &Delete, at::Device(at::DeviceType::MPS, 0)};
   }
-  std::pair<const void*, uint32_t> getSharedBufferPtr(const void* ptr) const override {
+  DataPtr getSharedBufferPtr(const void* ptr) const override {
     return _getAllocImpl().getSharedBufferPtr(ptr);
   }
   bool isSharedBuffer(const void* ptr) const override {
@@ -766,7 +1550,7 @@ struct TORCH_API MPSAllocator final : public IMPSAllocator {
   id_t getBufferId(const void* ptr) const override {
     return _getAllocImpl().getBufferId(ptr);
   };
-  IntArrayRef getBufferShape(const void* ptr) const override {
+  std::vector<int64_t> getBufferShape(const void* ptr) const override {
     return _getAllocImpl().getBufferShape(ptr);
   }
   void setBufferShape(const void* ptr, const IntArrayRef& shape) const override {
@@ -856,3 +1640,41 @@ bool isMPSPinnedPtr(const void* data) {
 }
 
 } // namespace at::mps
+
+namespace at::mps {
+static void record_stream_mps_impl(const void* ptr, MPSStream* stream) {
+  if (!ptr || !stream) {
+    return;
+  }
+  _getAllocImpl().recordStream(ptr, stream);
+}
+} // namespace at::mps
+
+namespace at::native {
+void record_stream_mps(Tensor& self, c10::Stream stream) {
+  const void* ptr = self.storage().data_ptr().get();
+  if (!ptr) {
+    return;
+  }
+
+  const auto data = stream.pack3();
+  TORCH_CHECK(
+      data.device_type == c10::DeviceType::MPS,
+      "record_stream_mps expected an MPS stream, got ",
+      c10::DeviceTypeName(data.device_type));
+
+  // 32.81 fix: Check if stream pool is alive before accessing. During static
+  // destruction, the pool may be destroyed and accessing instance() would
+  // return a dangling reference or cause undefined behavior.
+  if (!at::mps::MPSStreamPool::isPoolAlive()) {
+    return;
+  }
+  at::mps::MPSStream* mps_stream = at::mps::MPSStreamPool::instance().getStream(static_cast<size_t>(data.stream_id));
+  // 32.110/32.112 fix: Re-check pool is alive after getting stream to close TOCTOU race,
+  // and null-check the result. Pattern from 32.105 fix in MPSEvent::getRecordingStream().
+  if (!at::mps::MPSStreamPool::isPoolAlive() || mps_stream == nullptr) {
+    return;
+  }
+  at::mps::record_stream_mps_impl(ptr, mps_stream);
+}
+} // namespace at::native
diff --git a/aten/src/ATen/mps/MPSAllocatorInterface.h b/aten/src/ATen/mps/MPSAllocatorInterface.h
index 996c84a8..42b5ff76 100644
--- a/aten/src/ATen/mps/MPSAllocatorInterface.h
+++ b/aten/src/ATen/mps/MPSAllocatorInterface.h
@@ -18,7 +18,8 @@ class IMPSAllocator : public c10::Allocator {
   virtual void emptyCache() const = 0;
   virtual void freeInactiveBuffers() const = 0;
   virtual ssize_t getUnalignedBufferSize(const void* ptr) const = 0;
-  virtual IntArrayRef getBufferShape(const void* ptr) const = 0;
+  // 32.96 fix: Changed from IntArrayRef to std::vector<int64_t> to prevent dangling reference
+  virtual std::vector<int64_t> getBufferShape(const void* ptr) const = 0;
   virtual id_t getBufferId(const void* ptr) const = 0;
   virtual void setBufferShape(const void* ptr, const IntArrayRef& shape)
       const = 0;
@@ -36,8 +37,7 @@ class IMPSAllocator : public c10::Allocator {
   virtual size_t getCurrentAllocatedMemory() const = 0;
   virtual size_t getDriverAllocatedMemory() const = 0;
   virtual size_t getRecommendedMaxMemory() const = 0;
-  virtual std::pair<const void*, uint32_t> getSharedBufferPtr(
-      const void* ptr) const = 0;
+  virtual c10::DataPtr getSharedBufferPtr(const void* ptr) const = 0;
   virtual bool recordEvents(c10::ArrayRef<const void*> buffers) const = 0;
   virtual bool waitForEvents(c10::ArrayRef<const void*> buffers) const = 0;
 };
diff --git a/aten/src/ATen/mps/MPSEvent.h b/aten/src/ATen/mps/MPSEvent.h
index 379f65a3..39b2ae7f 100644
--- a/aten/src/ATen/mps/MPSEvent.h
+++ b/aten/src/ATen/mps/MPSEvent.h
@@ -3,11 +3,35 @@
 #pragma once
 
 #include <ATen/mps/MPSStream.h>
+#include <atomic>
+#include <chrono>
+#include <condition_variable>
 #include <ctime>
+#include <functional>
+#include <memory>
+#include <mutex>
 #include <stack>
+#include <unordered_map>
 
 namespace at::mps {
 
+// 32.107 fix: Shared callback state that survives MPSEvent destruction.
+// Callbacks capture a shared_ptr to this structure, ensuring the synchronization
+// primitives remain valid even after the MPSEvent C++ object is destroyed.
+// This fixes a UAF race where:
+// 1. Callback checks alive flag (returns true)
+// 2. Destructor sets alive=false and times out waiting for callback
+// 3. Destructor returns, C++ object memory freed
+// 4. Callback resumes and accesses freed memory via m_cpu_sync_cv
+// With CallbackState being shared, step 4 accesses valid memory.
+struct MPSEventCallbackState {
+  std::atomic<bool> alive{true};
+  mutable std::mutex sync_mutex{};
+  std::condition_variable sync_cv{};
+  bool sync_completed{false};
+  uint64_t completion_time{0};
+};
+
 // NOTE: don't create instances of this class directly.
 // Use MPSEventPool to acquire instances of MPSEvent.
 class MPSEvent {
@@ -15,10 +39,10 @@ class MPSEvent {
   explicit MPSEvent(id_t ID, MPSStream* stream, bool enable_timing);
   ~MPSEvent();
 
-  // records an event on the stream
-  void record(bool needsLock, bool syncEvent = false);
-  // makes all future work submitted to the stream wait for this event.
-  bool wait(bool needsLock, bool syncEvent = false);
+  // records an event on the given stream.
+  void record(MPSStream* stream, bool needsLock, bool syncEvent = false);
+  // makes all future work submitted to the given stream wait for this event.
+  bool wait(MPSStream* stream, bool needsLock, bool syncEvent = false);
   // schedules a notifyListener callback for the event.
   bool notify(bool needsLock, MTLSharedEventNotificationBlock block);
   // checks if events are already signaled.
@@ -28,15 +52,23 @@ class MPSEvent {
   bool synchronize();
   // resets this event with new parameters in case it gets reused from the event
   // pool
-  void reset(MPSStream* stream, bool enable_timing);
+  void reset(bool enable_timing);
   // returns the unique ID of the event instance
   id_t getID() const {
     return m_id;
   }
   // returns the completion timestamp of the event
   uint64_t getCompletionTime() const {
-    return m_completion_time;
+    std::lock_guard<std::mutex> lock(m_callback_state->sync_mutex);
+    return m_callback_state->completion_time;
   }
+  bool isTimingEnabled() const {
+    std::lock_guard<std::mutex> lock(m_mutex);
+    return m_enable_timing;
+  }
+  // returns the stream that recorded this event (for stream-specific sync)
+  // 27.3 fix: Returns stream looked up by ID, not cached pointer
+  MPSStream* getRecordingStream() const;
   // if already recorded, waits for cpu_sync_cv to be signaled
   void waitForCpuSync();
 
@@ -45,21 +77,23 @@ class MPSEvent {
   // enables measuring the completion time of the notifyListener of this event
   bool m_enable_timing;
   uint64_t m_signalCounter = 0;
-  MPSStream* m_stream = nullptr;
   MTLSharedEvent_t m_event = nullptr;
   MTLSharedEventListener* m_listener = nullptr;
-  // used to sync the events created on this Stream with CPU
-  std::mutex m_cpu_sync_mutex{};
-  std::condition_variable m_cpu_sync_cv{};
-  // CondVar predicate to sync the events created on this Stream with CPU
-  bool m_cpu_sync_completed = false;
-  // used to compute elapsed time
-  uint64_t m_completion_time = 0;
-
-  void recordLocked(bool syncEvent);
-  bool waitLocked(bool syncEvent);
+  // Cache-line aligned to prevent false sharing (Phase 24.4)
+  alignas(64) mutable std::mutex m_mutex{};
+  // 27.3 fix: tracks which stream recorded this event by ID, not raw pointer
+  // -1 means no stream recorded this event. Stream is looked up from pool at use time.
+  int64_t m_recording_stream_id = -1;
+  // 32.107 fix: Shared callback state for safe callback access after destruction.
+  // Callbacks capture a shared_ptr copy of this state, ensuring the synchronization
+  // primitives (mutex, cv) remain valid even after MPSEvent C++ object is destroyed.
+  // This fixes a UAF race between callback execution and destructor timeout.
+  std::shared_ptr<MPSEventCallbackState> m_callback_state;
+
+  void recordLocked(MPSStream* stream, bool syncEvent);
+  bool waitLocked(MPSStream* stream, bool syncEvent);
   bool notifyLocked(MTLSharedEventNotificationBlock block);
-  void notifyCpuSync();
+  void notifyCpuSync(uint64_t completion_time);
   static uint64_t getTime() {
     return clock_gettime_nsec_np(CLOCK_MONOTONIC_RAW);
   }
@@ -79,7 +113,9 @@ class MPSEventPool {
   id_t acquireEvent(bool enable_timing);
   void releaseEvent(id_t event_id);
   void recordEvent(id_t event_id, bool syncEvent);
+  void recordEvent(id_t event_id, MPSStream* stream, bool syncEvent);
   void waitForEvent(id_t event_id, bool syncEvent);
+  void waitForEvent(id_t event_id, MPSStream* stream, bool syncEvent);
   void synchronizeEvent(id_t event_id);
   bool queryEvent(id_t event_id);
   // returns elapsed time between two recorded events in milliseconds
@@ -92,11 +128,18 @@ class MPSEventPool {
   // dictionary to associate event IDs with event objects
   // used to retain in-use events out of the pool
   // for torch.mps.Event() bindings.
-  std::unordered_map<id_t, MPSEventPtr> m_in_use_events{};
-  uint64_t m_event_counter = 0;
+  // Uses shared_ptr for thread-safe access: getInUseEventShared() returns
+  // a copy that keeps the event alive even if releaseEvent() is called.
+  std::unordered_map<id_t, std::shared_ptr<MPSEvent>> m_in_use_events{};
+  std::atomic<uint64_t> m_event_counter{0};
   std::function<void(MPSEvent*)> m_default_deleter;
 
-  MPSEvent* getInUseEvent(id_t event_id, bool locked = true);
+  // Returns raw pointer for internal use. Always takes m_mutex lock.
+  // Note: The returned pointer is valid only while m_mutex could be held.
+  // Use getInUseEventShared() for thread-safe access outside locks.
+  MPSEvent* getInUseEvent(id_t event_id);
+  // Returns shared_ptr copy for thread-safe use outside lock
+  std::shared_ptr<MPSEvent> getInUseEventShared(id_t event_id);
 };
 
 // shared_ptr is used to get MPSEventPool destroyed after dependent instances
diff --git a/aten/src/ATen/mps/MPSEvent.mm b/aten/src/ATen/mps/MPSEvent.mm
index ac464614..8b329f6a 100644
--- a/aten/src/ATen/mps/MPSEvent.mm
+++ b/aten/src/ATen/mps/MPSEvent.mm
@@ -4,10 +4,64 @@
 
 namespace at::mps {
 
+// Flag to track if event pool is alive (for safe deleter during static destruction)
+static std::atomic<bool> s_event_pool_alive{false};
+
 MPSEvent::MPSEvent(id_t ID, MPSStream* stream, bool enable_timing)
-    : m_id(ID), m_enable_timing(enable_timing), m_stream(stream), m_event([stream->device() newSharedEvent]) {}
+    : m_id(ID),
+      m_enable_timing(enable_timing),
+      m_event([stream->device() newSharedEvent]),
+      m_callback_state(std::make_shared<MPSEventCallbackState>()) {
+  TORCH_INTERNAL_ASSERT(stream);
+}
 
 MPSEvent::~MPSEvent() {
+  // 32.107 fix: Mark callback state as not alive BEFORE any wait/cleanup.
+  // Callbacks capture m_callback_state by value (shared_ptr copy) and check
+  // its alive flag before accessing callback state members. This prevents
+  // cross-talk if a callback runs after the event is returned to pool.
+  // Note: The callback state survives MPSEvent destruction, so callbacks can
+  // safely access the sync primitives even after this destructor returns.
+  m_callback_state->alive.store(false, std::memory_order_release);
+
+  // 32.60 fix: Wait for any pending notify callbacks before destruction.
+  // recordLocked() and synchronize() register callbacks via notifyLocked() that
+  // access m_callback_state. We detect pending callbacks via sync_completed being
+  // false (set to false when callback registered, set to true when callback fires).
+  // Wait with timeout to avoid infinite hang if GPU work never completes.
+  bool callback_timed_out = false;
+  if (m_listener) {
+    std::unique_lock<std::mutex> lock(m_callback_state->sync_mutex);
+    if (!m_callback_state->sync_completed) {
+      // Callback is pending - wait for it (with 5 second timeout to avoid hang)
+      // 32.107 fix: Use a local copy of callback_state for the predicate to avoid
+      // capturing 'this' which would be invalid if this lambda runs after destruction.
+      auto state = m_callback_state;
+      auto result = m_callback_state->sync_cv.wait_for(lock, std::chrono::seconds(5),
+                                            [state] { return state->sync_completed; });
+      if (!result) {
+        // Timeout - callback never fired. This can happen if GPU work was never
+        // committed.
+        callback_timed_out = true;
+        TORCH_WARN_ONCE("MPSEvent destructor: timeout waiting for notify callback. "
+                        "Possible GPU work was never committed.");
+      }
+    }
+  }
+
+  // 32.106/32.107 fix: Skip Metal object cleanup if callback timed out.
+  // Even though m_callback_state survives destruction (callbacks can safely
+  // access it), the callback may still try to read m_event.signaledValue in
+  // getTime() which is a member of MPSEvent (not CallbackState). Actually
+  // getTime() uses clock_gettime_nsec_np() which doesn't access this.
+  // However, to be safe and consistent with 32.106, skip cleanup on timeout.
+  if (callback_timed_out) {
+    // Intentional leak to prevent potential use-after-free.
+    // The callback state (mutex, cv) survives via shared_ptr, so callbacks
+    // can still safely notify completion even after this destructor returns.
+    return;
+  }
+
   if (m_event) {
     [m_event release];
     m_event = nil;
@@ -18,34 +72,56 @@ MPSEvent::~MPSEvent() {
   }
 }
 
-void MPSEvent::recordLocked(bool syncEvent) {
-  // active encoders must end before encoding or waiting
-  m_stream->endKernelCoalescing();
+void MPSEvent::recordLocked(MPSStream* stream, bool syncEvent) {
   ++m_signalCounter;
+  // 27.3 fix: Track stream by ID (not raw pointer) for stream-specific sync in elapsedTime()
+  m_recording_stream_id = stream ? static_cast<int64_t>(stream->unwrap().id()) : -1;
   if (m_enable_timing) {
+    {
+      std::lock_guard<std::mutex> cpu_lock(m_callback_state->sync_mutex);
+      m_callback_state->completion_time = 0;
+      m_callback_state->sync_completed = false;
+    }
+    // 32.107 fix: Capture callback_state by value (shared_ptr copy) instead of 'this'.
+    // The callback state survives MPSEvent destruction, so callbacks can safely
+    // access sync primitives even after the destructor returns. This fixes the
+    // UAF race where callback passes alive check, gets delayed, destructor times
+    // out and frees C++ memory, then callback resumes and accesses freed memory.
+    auto state = m_callback_state;
     notifyLocked(^(id<MTLSharedEvent>, uint64_t) {
-      m_completion_time = getTime();
-      notifyCpuSync();
+      if (!state->alive.load(std::memory_order_acquire)) {
+        // Event destroyed or reset - still notify completion so destructor doesn't hang.
+        // The sync primitives are in the shared state, so this is safe.
+        std::lock_guard<std::mutex> lock(state->sync_mutex);
+        state->sync_completed = true;
+        state->sync_cv.notify_all();
+        return;
+      }
+      // Safe to access state members - they're in shared_ptr-managed memory
+      std::lock_guard<std::mutex> lock(state->sync_mutex);
+      state->completion_time = getTime();
+      state->sync_completed = true;
+      state->sync_cv.notify_all();
     });
   }
-  id<MTLCommandBuffer> commandBuffer = m_stream->commandBuffer();
-  [commandBuffer encodeSignalEvent:m_event value:m_signalCounter];
+  // Encode the signal under the stream mutex to ensure thread-safe ordering with
+  // other stream work when callers avoid dispatch_sync (e.g., allocator recordStream()).
+  stream->encodeSignalEvent(m_event, m_signalCounter);
   if (syncEvent) {
-    m_stream->synchronize(SyncType::COMMIT);
+    stream->synchronize(SyncType::COMMIT);
   }
 }
 
-bool MPSEvent::waitLocked(bool syncEvent) {
+bool MPSEvent::waitLocked(MPSStream* stream, bool syncEvent) {
   // check if event is not recorded yet
   if (m_event.signaledValue >= m_signalCounter) {
     return false;
   }
-  // active encoders must end before encoding or waiting
-  m_stream->endKernelCoalescing();
-  id<MTLCommandBuffer> commandBuffer = m_stream->commandBuffer();
-  [commandBuffer encodeWaitForEvent:m_event value:m_signalCounter];
+  // Encode the wait under the stream mutex to ensure thread-safe ordering with
+  // other stream work when callers avoid dispatch_sync.
+  stream->encodeWaitForEvent(m_event, m_signalCounter);
   if (syncEvent) {
-    m_stream->synchronize(SyncType::COMMIT);
+    stream->synchronize(SyncType::COMMIT);
   }
   return true;
 }
@@ -56,67 +132,110 @@ bool MPSEvent::notifyLocked(MTLSharedEventNotificationBlock block) {
     return false;
   }
   if (!m_listener) {
-    m_listener = [[MTLSharedEventListener alloc] init];
+    // THREAD-SAFETY (23.21): Use explicit dispatch queue for deterministic callback delivery.
+    // Without an explicit queue, notifications would be delivered on arbitrary threads,
+    // which could race with thread exit. Using a global queue ensures the callback
+    // context outlives any specific thread.
+    m_listener = [[MTLSharedEventListener alloc]
+        initWithDispatchQueue:dispatch_get_global_queue(QOS_CLASS_USER_INITIATED, 0)];
   }
   [m_event notifyListener:m_listener atValue:m_signalCounter block:block];
   return true;
 }
 
-void MPSEvent::record(bool needsLock, bool syncEvent) {
+void MPSEvent::record(MPSStream* stream, bool needsLock, bool syncEvent) {
+  TORCH_INTERNAL_ASSERT(stream);
   if (!needsLock) {
-    recordLocked(syncEvent);
+    std::lock_guard<std::mutex> lock(m_mutex);
+    recordLocked(stream, syncEvent);
     return;
   }
-  dispatch_sync(m_stream->queue(), ^() {
+  dispatch_block_t dispatch_block = ^() {
     @autoreleasepool {
-      recordLocked(syncEvent);
+      std::lock_guard<std::mutex> lock(m_mutex);
+      recordLocked(stream, syncEvent);
     }
-  });
+  };
+  if (dispatch_get_specific(getMPSStreamQueueSpecificKey()) == static_cast<void*>(stream)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(stream->queue(), dispatch_block);
+  }
 }
 
-bool MPSEvent::wait(bool needsLock, bool syncEvent) {
+bool MPSEvent::wait(MPSStream* stream, bool needsLock, bool syncEvent) {
+  TORCH_INTERNAL_ASSERT(stream);
   __block bool waited = false;
   if (!needsLock) {
-    return waitLocked(syncEvent);
+    std::lock_guard<std::mutex> lock(m_mutex);
+    return waitLocked(stream, syncEvent);
   }
-  dispatch_sync(m_stream->queue(), ^() {
+  dispatch_block_t dispatch_block = ^() {
     @autoreleasepool {
-      waited = waitLocked(syncEvent);
+      std::lock_guard<std::mutex> lock(m_mutex);
+      waited = waitLocked(stream, syncEvent);
     }
-  });
+  };
+  if (dispatch_get_specific(getMPSStreamQueueSpecificKey()) == static_cast<void*>(stream)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(stream->queue(), dispatch_block);
+  }
   return waited;
 }
 
 bool MPSEvent::notify(bool needsLock, MTLSharedEventNotificationBlock block) {
-  if (!needsLock) {
-    return notifyLocked(block);
-  }
-  __block bool scheduledNotify = false;
-  dispatch_sync(m_stream->queue(), ^() {
-    @autoreleasepool {
-      scheduledNotify = notifyLocked(block);
-    }
-  });
-  return scheduledNotify;
+  (void)needsLock;
+  std::lock_guard<std::mutex> lock(m_mutex);
+  return notifyLocked(block);
 }
 
-void MPSEvent::notifyCpuSync() {
-  std::lock_guard<std::mutex> lock(m_cpu_sync_mutex);
-  m_cpu_sync_completed = true;
-  m_cpu_sync_cv.notify_one();
+void MPSEvent::notifyCpuSync(uint64_t completion_time) {
+  // 32.107: This function is no longer called - callback directly accesses
+  // m_callback_state which survives MPSEvent destruction.
+  std::lock_guard<std::mutex> lock(m_callback_state->sync_mutex);
+  m_callback_state->completion_time = completion_time;
+  m_callback_state->sync_completed = true;
+  m_callback_state->sync_cv.notify_one();
 }
 
 void MPSEvent::waitForCpuSync() {
-  std::unique_lock<std::mutex> lock(m_cpu_sync_mutex);
-  m_cpu_sync_cv.wait(lock, [&] { return m_cpu_sync_completed; });
-  m_cpu_sync_completed = false;
+  // 32.107: Capture callback_state so predicate doesn't capture 'this'
+  auto state = m_callback_state;
+  std::unique_lock<std::mutex> lock(state->sync_mutex);
+  state->sync_cv.wait(lock, [state] { return state->sync_completed; });
+  state->sync_completed = false;
 }
 
 bool MPSEvent::synchronize() {
-  bool scheduledNotify = notifyLocked(^(id<MTLSharedEvent>, uint64_t) {
-    m_completion_time = getTime();
-    notifyCpuSync();
-  });
+  bool scheduledNotify = false;
+  {
+    std::lock_guard<std::mutex> lock(m_mutex);
+    {
+      std::lock_guard<std::mutex> cpu_lock(m_callback_state->sync_mutex);
+      m_callback_state->sync_completed = false;
+    }
+    // 32.107 fix: Capture callback_state by value (shared_ptr copy) instead of 'this'.
+    // The callback state survives MPSEvent destruction, so callbacks can safely
+    // access sync primitives even after the destructor returns. This also prevents
+    // cross-talk with new owners after reset() (32.94), since reset() creates a
+    // new callback_state and the old callbacks still have the old state.
+    auto state = m_callback_state;
+    scheduledNotify = notifyLocked(^(id<MTLSharedEvent>, uint64_t) {
+      if (!state->alive.load(std::memory_order_acquire)) {
+        // Event destroyed or reset - still notify completion so destructor doesn't hang.
+        std::lock_guard<std::mutex> lock(state->sync_mutex);
+        state->sync_completed = true;
+        state->sync_cv.notify_all();
+        return;
+      }
+      // Safe to access state members - they're in shared_ptr-managed memory
+      std::lock_guard<std::mutex> lock(state->sync_mutex);
+      state->completion_time = getTime();
+      state->sync_completed = true;
+      state->sync_cv.notify_all();
+    });
+  }
 
   if (scheduledNotify) {
     waitForCpuSync();
@@ -126,21 +245,77 @@ bool MPSEvent::synchronize() {
 }
 
 bool MPSEvent::query() const {
+  std::lock_guard<std::mutex> lock(m_mutex);
   // return false if not recorded or signaled yet
   return m_signalCounter && (m_event.signaledValue >= m_signalCounter);
 }
 
-void MPSEvent::reset(MPSStream* stream, bool enable_timing) {
-  if (stream != m_stream) {
-    m_signalCounter = 0;
-    m_event.signaledValue = 0;
-    m_stream = stream;
-  }
-  // reset record time
-  m_completion_time = 0;
+void MPSEvent::reset(bool enable_timing) {
+  std::lock_guard<std::mutex> lock(m_mutex);
+  // THREAD-SAFETY: Use monotonically increasing counter to prevent cross-talk.
+  // Don't reset signaledValue to 0 - pending GPU signals from previous owner
+  // could overwrite it with a higher value, causing query() to return true
+  // prematurely.
+  // 32.97 fix: Set signalCounter = current + 1 so query() returns false until record() is called.
+  // Previously, signalCounter was set to current, causing query() to return true for unrecorded
+  // events (signalCounter == signaledValue). This caused elapsedTime() to hang forever if called
+  // before recording because:
+  //   1. getCompletionTime() returns 0 (reset cleared it)
+  //   2. query() returns true (incorrectly, since signalCounter == signaledValue)
+  //   3. waitForCpuSync() blocks forever (no callback was scheduled since record() wasn't called)
+  // With signalCounter = current + 1, query() returns false until record() increments it again
+  // and encodes a signal at the new value.
+  uint64_t current_signaled = m_event.signaledValue;
+  m_signalCounter = current_signaled + 1;
+  // reset record time and recording stream
   m_enable_timing = enable_timing;
-  m_cpu_sync_completed = false;
-};
+  m_recording_stream_id = -1;
+  // 27.8 fix: Release listener to cancel any pending notifications from previous owner.
+  // Pending callbacks on the global queue could fire after event is reused, causing
+  // cross-talk with the new owner. Releasing the listener cancels the notification.
+  // NOTE: Releasing the listener may NOT actually cancel pending notifications -
+  // Metal doesn't guarantee cancellation. See 32.89/32.107 fix below for proper handling.
+  if (m_listener) {
+    [m_listener release];
+    m_listener = nil;
+  }
+  // 32.89/32.107 fix: Invalidate old callback state and create new one for the new owner.
+  // Even though we release the listener above, pending callbacks may still fire
+  // (Metal doesn't guarantee cancellation). Old callbacks captured the old
+  // m_callback_state shared_ptr - setting alive=false ensures they see "event
+  // destroyed/reset" and handle appropriately (notify completion but skip work).
+  // Creating a new m_callback_state ensures new callbacks see fresh state.
+  m_callback_state->alive.store(false, std::memory_order_release);
+  m_callback_state = std::make_shared<MPSEventCallbackState>();
+}
+
+// 27.3 fix: Look up stream by ID from pool instead of returning cached raw pointer
+MPSStream* MPSEvent::getRecordingStream() const {
+  std::lock_guard<std::mutex> lock(m_mutex);
+  if (m_recording_stream_id < 0) {
+    return nullptr;
+  }
+  // 32.80 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed and accessing instance() would return a dangling
+  // reference or cause undefined behavior. This can happen if elapsedTime() is
+  // called from a static destructor that runs after the pool destructor.
+  if (!MPSStreamPool::isPoolAlive()) {
+    return nullptr;
+  }
+  // Stream pool streams are never destroyed during normal operation,
+  // so this lookup is safe. If stream doesn't exist, getStream() returns
+  // the stream at that index (creating it if needed via call_once).
+  MPSStream* result = MPSStreamPool::instance().getStream(static_cast<size_t>(m_recording_stream_id));
+  // 32.105 fix: Re-check pool is alive after getting stream. This closes a TOCTOU race:
+  // 1. Thread A: initial isPoolAlive() check passes (pool is alive)
+  // 2. Thread B: ~MPSStreamPool() runs, sets g_pool_alive=false, destroys streams_[]
+  // 3. Thread A: getStream() returns pointer to freed memory (UAF!)
+  // Same pattern as 32.99/32.101/32.104 fixes for other pool accessor functions.
+  if (!MPSStreamPool::isPoolAlive()) {
+    return nullptr;
+  }
+  return result;
+}
 
 //-----------------------------------------------------------------
 //  MPSEventPool
@@ -148,30 +323,52 @@ void MPSEvent::reset(MPSStream* stream, bool enable_timing) {
 
 MPSEventPool::MPSEventPool(MPSStream* default_stream) : m_default_stream(default_stream) {
   // default deleter to return the event back to pool after it gets released
-  m_default_deleter = [&](MPSEvent* event) {
+  // 32.58 fix: Capture 'this' explicitly and check s_event_pool_alive before
+  // accessing pool members. During static destruction, MPSEventPtr objects in
+  // BufferBlock::pending_events may outlive the pool - the deleter must not
+  // access destroyed pool members.
+  m_default_deleter = [this](MPSEvent* event) {
+    // Safety check: if pool is destroyed, just delete the event
+    // (can't return to pool that no longer exists)
+    if (!s_event_pool_alive.load(std::memory_order_acquire)) {
+      delete event;
+      return;
+    }
     std::lock_guard<std::recursive_mutex> lock(m_mutex);
     m_pool.push(std::unique_ptr<MPSEvent>(event));
   };
+  // Mark pool as alive AFTER constructor completes
+  s_event_pool_alive.store(true, std::memory_order_release);
 }
 
 MPSEventPool::~MPSEventPool() {
+  // 32.58 fix: Mark pool as no longer alive FIRST, before any cleanup.
+  // This prevents in-flight deleters from trying to access pool members.
+  s_event_pool_alive.store(false, std::memory_order_release);
   emptyCache();
 }
 
 MPSEventPtr MPSEventPool::acquireEvent(bool enable_timing, MPSStream* stream) {
   if (!stream) {
-    stream = m_default_stream;
+    // Use thread's current stream, not default stream, for proper multi-thread support
+    stream = getCurrentMPSStream();
+    // 32.83 fix: During static destruction, getCurrentMPSStream() returns nullptr.
+    // Return nullptr early to avoid crash in MPSEvent constructor.
+    if (!stream) {
+      return nullptr;
+    }
   }
   {
     std::lock_guard<std::recursive_mutex> lock(m_mutex);
     if (!m_pool.empty()) {
       auto event = m_pool.top().release();
       m_pool.pop();
-      event->reset(stream, enable_timing);
+      event->reset(enable_timing);
       return MPSEventPtr(event, m_default_deleter);
     }
   }
-  auto new_event = std::make_unique<MPSEvent>(++m_event_counter, stream, enable_timing);
+  const auto new_id = m_event_counter.fetch_add(1, std::memory_order_relaxed) + 1;
+  auto new_event = std::make_unique<MPSEvent>(new_id, stream, enable_timing);
   return MPSEventPtr(new_event.release(), m_default_deleter);
 }
 
@@ -185,9 +382,16 @@ void MPSEventPool::emptyCache() {
 id_t MPSEventPool::acquireEvent(bool enable_timing) {
   std::lock_guard<std::recursive_mutex> lock(m_mutex);
   MPSEventPtr event = acquireEvent(enable_timing, nullptr);
-  TORCH_INTERNAL_ASSERT(event);
+  // 32.92 fix: During static destruction, getCurrentMPSStream() returns nullptr
+  // causing acquireEvent(enable_timing, nullptr) to return nullptr. Return 0
+  // (invalid event ID) instead of crashing. Event IDs start at 1, so 0 is safe
+  // to use as "no event" indicator.
+  if (!event) {
+    return 0;
+  }
   id_t event_id = event->getID();
-  m_in_use_events.emplace(event_id, std::move(event));
+  // Convert unique_ptr to shared_ptr for thread-safe access
+  m_in_use_events.emplace(event_id, std::shared_ptr<MPSEvent>(event.release(), event.get_deleter()));
   return event_id;
 }
 
@@ -199,35 +403,90 @@ void MPSEventPool::releaseEvent(id_t event_id) {
 }
 
 void MPSEventPool::recordEvent(id_t event_id, bool syncEvent) {
-  MPSEvent* event = getInUseEvent(event_id);
-  event->record(/*needsLock*/ true, syncEvent);
+  recordEvent(event_id, getCurrentMPSStream(), syncEvent);
+}
+
+void MPSEventPool::recordEvent(id_t event_id, MPSStream* stream, bool syncEvent) {
+  // 32.84 fix: During static destruction, streamFromGuardStream()/getCurrentMPSStream()
+  // can return nullptr. MPSEvent::record() asserts non-null.
+  if (!stream) {
+    return;
+  }
+  // 33.3 UAF fix: Use shared_ptr to keep event alive during operation.
+  // Raw pointer from getInUseEvent() becomes dangling if another thread
+  // calls releaseEvent() between the lookup and the record() call.
+  std::shared_ptr<MPSEvent> event = getInUseEventShared(event_id);
+  event->record(stream, /*needsLock*/ true, syncEvent);
 }
 
 void MPSEventPool::waitForEvent(id_t event_id, bool syncEvent) {
-  MPSEvent* event = getInUseEvent(event_id);
-  event->wait(/*needsLock*/ true, syncEvent);
+  waitForEvent(event_id, getCurrentMPSStream(), syncEvent);
+}
+
+void MPSEventPool::waitForEvent(id_t event_id, MPSStream* stream, bool syncEvent) {
+  // 32.84 fix: During static destruction, streamFromGuardStream()/getCurrentMPSStream()
+  // can return nullptr. MPSEvent::wait() asserts non-null.
+  if (!stream) {
+    return;
+  }
+  // 33.3 UAF fix: Use shared_ptr to keep event alive during operation.
+  std::shared_ptr<MPSEvent> event = getInUseEventShared(event_id);
+  event->wait(stream, /*needsLock*/ true, syncEvent);
 }
 
 void MPSEventPool::synchronizeEvent(id_t event_id) {
-  MPSEvent* event = getInUseEvent(event_id);
+  // 33.3 UAF fix: Use shared_ptr to keep event alive during operation.
+  std::shared_ptr<MPSEvent> event = getInUseEventShared(event_id);
   event->synchronize();
 }
 
 bool MPSEventPool::queryEvent(id_t event_id) {
-  MPSEvent* event = getInUseEvent(event_id);
+  // 33.3 UAF fix: Use shared_ptr to keep event alive during operation.
+  std::shared_ptr<MPSEvent> event = getInUseEventShared(event_id);
   return event->query();
 }
 
 double MPSEventPool::elapsedTime(id_t start_event_id, id_t end_event_id) {
-  // first make sure notifyListeners are called to capture events' completion times
-  dispatch_sync(m_default_stream->queue(), ^() {
-    m_default_stream->synchronize(SyncType::COMMIT_AND_WAIT);
-  });
-  std::lock_guard<std::recursive_mutex> lock(m_mutex);
-  MPSEvent* start_event = getInUseEvent(start_event_id, false);
-  MPSEvent* end_event = getInUseEvent(end_event_id, false);
-  // the notify is called on a separate thread, so this waits for that
-  end_event->waitForCpuSync();
+  // Get shared_ptr copies to keep events alive even if releaseEvent() is called.
+  // This fixes the raw pointer race where releaseEvent could invalidate pointers.
+  std::shared_ptr<MPSEvent> start_event = getInUseEventShared(start_event_id);
+  std::shared_ptr<MPSEvent> end_event = getInUseEventShared(end_event_id);
+
+  // Sync only the streams that recorded these events (not all streams).
+  // This is a scalability improvement - previously we called synchronizeAllStreams()
+  // which blocked all threads even if their streams weren't involved in timing.
+  MPSStream* start_stream = start_event->getRecordingStream();
+  MPSStream* end_stream = end_event->getRecordingStream();
+  // 32.95 fix: Check isPoolAlive() before using stream pointers.
+  // getRecordingStream() internally checks isPoolAlive(), but the returned pointer
+  // could become dangling if the pool is destroyed between that check and our use.
+  // This narrows the TOCTOU race window (though doesn't eliminate it entirely).
+  // During static destruction, the pool may be destroyed while elapsedTime() runs.
+  if (start_stream && MPSStreamPool::isPoolAlive()) {
+    start_stream->synchronize(SyncType::COMMIT_AND_WAIT);
+  }
+  if (end_stream && end_stream != start_stream && MPSStreamPool::isPoolAlive()) {
+    end_stream->synchronize(SyncType::COMMIT_AND_WAIT);
+  }
+
+  TORCH_CHECK(
+      start_event->isTimingEnabled() && end_event->isTimingEnabled(),
+      "Events were not created with argument 'enable_timing=True'");
+
+  // Wait for CPU sync - these calls block until GPU completion notification.
+  // The shared_ptr keeps the events alive during this blocking operation.
+  if (end_event->getCompletionTime() == 0) {
+    TORCH_CHECK(end_event->query(), "End event ", end_event_id, " must be recorded before calculating elapsed time.");
+    end_event->waitForCpuSync();
+  }
+  if (start_event->getCompletionTime() == 0) {
+    TORCH_CHECK(start_event->query(),
+                "Start event ",
+                start_event_id,
+                " must be recorded before calculating elapsed time.");
+    start_event->waitForCpuSync();
+  }
+
   const uint64_t start_time = start_event->getCompletionTime();
   const uint64_t end_time = end_event->getCompletionTime();
 
@@ -237,16 +496,22 @@ double MPSEventPool::elapsedTime(id_t start_event_id, id_t end_event_id) {
   return double(end_time - start_time) * 1e-6;
 }
 
-MPSEvent* MPSEventPool::getInUseEvent(id_t event_id, bool locked) {
-  if (locked) {
-    m_mutex.lock();
-  }
-  TORCH_CHECK(m_in_use_events.count(event_id) > 0, "Invalid Event ID: ", event_id);
-  MPSEvent* event = m_in_use_events[event_id].get();
-  if (locked) {
-    m_mutex.unlock();
-  }
-  return event;
+MPSEvent* MPSEventPool::getInUseEvent(id_t event_id) {
+  // Phase 32.48 fix: Removed unused 'locked' parameter that had a data race.
+  // The locked=false path accessed m_in_use_events without protection.
+  // Since no callers used locked=false, we now always take the lock.
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
+  auto it = m_in_use_events.find(event_id);
+  TORCH_CHECK(it != m_in_use_events.end(), "Invalid Event ID: ", event_id);
+  return it->second.get();
+}
+
+std::shared_ptr<MPSEvent> MPSEventPool::getInUseEventShared(id_t event_id) {
+  std::lock_guard<std::recursive_mutex> lock(m_mutex);
+  auto it = m_in_use_events.find(event_id);
+  TORCH_CHECK(it != m_in_use_events.end(), "Invalid Event ID: ", event_id);
+  // Return a copy of the shared_ptr to keep the event alive outside the lock
+  return it->second;
 }
 
 std::shared_ptr<MPSEventPool> getMPSEventPool() {
diff --git a/aten/src/ATen/mps/MPSGuardImpl.h b/aten/src/ATen/mps/MPSGuardImpl.h
index 008a8d57..ff38ce9a 100644
--- a/aten/src/ATen/mps/MPSGuardImpl.h
+++ b/aten/src/ATen/mps/MPSGuardImpl.h
@@ -33,6 +33,11 @@ struct TORCH_API MPSGuardImpl final
     : public c10::impl::DeviceGuardImplInterface {
   static constexpr c10::DeviceType static_type = c10::DeviceType::MPS;
 
+  // NOTE: This guard integrates MPSStreamPool by returning the per-thread
+  // current stream from getStream()/exchangeStream(). The MPS backend does not
+  // currently expose a public per-stream API; prefer device-wide sync via
+  // MPSHooks::deviceSynchronize() / torch.mps.synchronize() in user code.
+
   // constructor
   MPSGuardImpl() {}
   explicit MPSGuardImpl(c10::DeviceType t) {
@@ -68,21 +73,62 @@ struct TORCH_API MPSGuardImpl final
   }
 
   Stream getStream(Device d) const override {
-    return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    // Return the thread-local current stream (or default if not set)
+    // 32.64 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+    // static destruction when g_pool_alive is false.
+    MPSStream* current = getCurrentMPSStream();
+    if (!current) {
+      return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    }
+    return current->unwrap();
   }
 
   Stream getNewStream(Device, int priority = 0) const override {
     (void)priority;
-    return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    // Return a non-default stream from the pool (CUDA-style round-robin).
+    // Streams may be reused across threads; prefer getStream()/exchangeStream()
+    // for stable per-thread stream selection.
+    // 32.64 fix: Check for nullptr in case pool is destroyed.
+    MPSStream* stream = getStreamFromPool();
+    if (!stream) {
+      return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    }
+    return stream->unwrap();
   }
 
   Stream getDefaultStream(Device d) const override {
-    return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    // Return stream 0 (the default stream)
+    // 32.64 fix: Check for nullptr in case pool is destroyed.
+    MPSStream* defaultStream = getDefaultMPSStream();
+    if (!defaultStream) {
+      return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    }
+    return defaultStream->unwrap();
   }
 
   // NB: These do NOT set the current device
   Stream exchangeStream(Stream s) const override {
-    return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    // Get the current stream before setting new one
+    // 32.64 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+    // static destruction when g_pool_alive is false. In this case, return a
+    // default stream and skip modifying TLS state since pool is gone.
+    MPSStream* prev = getCurrentMPSStream();
+    if (!prev) {
+      return Stream(Stream::DEFAULT, Device(c10::DeviceType::MPS, 0));
+    }
+    Stream prevStream = prev->unwrap();
+
+    // Set the new stream as current for this thread
+    // Note: We need to map from Stream to MPSStream*
+    // For now, if the stream ID matches, use it from pool
+    // 32.64 fix: Check isPoolAlive() before accessing pool instance.
+    if (MPSStreamPool::isPoolAlive()) {
+      MPSStream* newStream = MPSStreamPool::instance().getStream(
+          static_cast<size_t>(s.id()));
+      setCurrentMPSStream(newStream);
+    }
+
+    return prevStream;
   }
   DeviceIndex deviceCount() const noexcept override {
     if (at::hasMPS()) {
diff --git a/aten/src/ATen/mps/MPSGuardImpl.mm b/aten/src/ATen/mps/MPSGuardImpl.mm
index a267b40f..09aed291 100644
--- a/aten/src/ATen/mps/MPSGuardImpl.mm
+++ b/aten/src/ATen/mps/MPSGuardImpl.mm
@@ -5,6 +5,23 @@
 
 namespace at::mps {
 
+namespace {
+MPSStream* streamFromGuardStream(const Stream& stream) {
+  TORCH_CHECK(stream.device_type() == DeviceType::MPS,
+              "Expected an MPS stream but got device type ",
+              stream.device_type(),
+              ".");
+  const auto stream_id = static_cast<size_t>(stream.id());
+  TORCH_CHECK(stream_id < MPSStreamPool::poolSize(), "Invalid MPS stream id ", stream_id, ".");
+  // 32.81 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed and accessing instance() would cause undefined behavior.
+  if (!MPSStreamPool::isPoolAlive()) {
+    return nullptr;
+  }
+  return MPSStreamPool::instance().getStream(stream_id);
+}
+} // namespace
+
 void MPSGuardImpl::createEvent(mpsEvent_t* event, const EventFlag flag) const {}
 
 void MPSGuardImpl::destroyEvent(void* event, const DeviceIndex device_index) const noexcept {
@@ -34,15 +51,14 @@ void MPSGuardImpl::record(void** event,
     mps_event_id = at::mps::getMPSEventPool()->acquireEvent(flag == EventFlag::BACKEND_DEFAULT);
     *event = (__bridge void*)(intptr_t)(mps_event_id);
   }
-  MPSStream mps_stream{stream};
-  at::mps::getMPSEventPool()->recordEvent(mps_event_id, true);
+  MPSStream* target_stream = streamFromGuardStream(stream);
+  at::mps::getMPSEventPool()->recordEvent(mps_event_id, target_stream, /*syncEvent*/ true);
 }
 
 void MPSGuardImpl::block(void* event, const Stream& stream) const {
   auto mps_event_id = (__bridge id_t)(intptr_t)(event);
-  MPSStream mps_stream{stream};
-
-  at::mps::getMPSEventPool()->waitForEvent(mps_event_id, false);
+  MPSStream* target_stream = streamFromGuardStream(stream);
+  at::mps::getMPSEventPool()->waitForEvent(mps_event_id, target_stream, /*syncEvent*/ false);
 }
 
 bool MPSGuardImpl::queryEvent(void* event) const {
@@ -63,7 +79,14 @@ double MPSGuardImpl::elapsedTime(void* event1, void* event2, const DeviceIndex d
 }
 
 void MPSGuardImpl::synchronizeDevice(const DeviceIndex device_index) const {
-  at::mps::getDefaultMPSStream()->synchronize(SyncType::COMMIT_AND_WAIT);
+  // THREAD-SAFETY FIX (21.15): Sync ALL streams for true device-wide synchronization
+  // This matches MPSHooks::deviceSynchronize() behavior
+  // 32.81 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed and accessing instance() would cause undefined behavior.
+  if (!at::mps::MPSStreamPool::isPoolAlive()) {
+    return;
+  }
+  at::mps::MPSStreamPool::instance().synchronizeAllStreams();
 }
 
 } // namespace at::mps
diff --git a/aten/src/ATen/mps/MPSHooks.h b/aten/src/ATen/mps/MPSHooks.h
index da5760c9..c81251a6 100644
--- a/aten/src/ATen/mps/MPSHooks.h
+++ b/aten/src/ATen/mps/MPSHooks.h
@@ -27,6 +27,7 @@ struct MPSHooks : public at::MPSHooksInterface {
 
   // MPSStream interface
   void deviceSynchronize() const override;
+  void releaseCurrentThreadSlot() const override;
   void commitStream() const override;
   void* getCommandBuffer() const override;
   void* getDispatchQueue() const override;
diff --git a/aten/src/ATen/mps/MPSHooks.mm b/aten/src/ATen/mps/MPSHooks.mm
index 34fbd31a..c94f1629 100644
--- a/aten/src/ATen/mps/MPSHooks.mm
+++ b/aten/src/ATen/mps/MPSHooks.mm
@@ -62,22 +62,57 @@ Generator MPSHooks::getNewGenerator([[maybe_unused]] DeviceIndex device_index) c
 }
 
 void MPSHooks::deviceSynchronize() const {
-  at::mps::getDefaultMPSStream()->synchronize(SyncType::COMMIT_AND_WAIT);
+  // DEVICE-WIDE SYNC: Synchronize ALL streams in the pool, not just current thread's stream.
+  // This matches CUDA semantics where cudaDeviceSynchronize() waits for all streams.
+  // Python docs promise: "Waits for all kernels in all streams on a MPS device to complete."
+  // 32.81 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed and accessing instance() would cause undefined behavior.
+  if (!at::mps::MPSStreamPool::isPoolAlive()) {
+    return;
+  }
+  at::mps::MPSStreamPool::instance().synchronizeAllStreams();
+}
+
+void MPSHooks::releaseCurrentThreadSlot() const {
+  // Clear the current thread's cached stream pointer. With round-robin pooling
+  // there is no exclusive slot ownership; this simply causes the next MPS use
+  // on this thread to reselect a stream.
+  at::mps::MPSStreamPool::releaseCurrentThreadSlot();
 }
 
 void MPSHooks::commitStream() const {
-  at::mps::getDefaultMPSStream()->synchronize(SyncType::COMMIT);
+  // THREAD-SAFETY FIX: Use current thread's stream for proper parallel execution
+  // 32.83 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+  // static destruction when pool is destroyed before this code runs.
+  auto stream = at::mps::getCurrentMPSStream();
+  if (!stream) {
+    return;
+  }
+  stream->synchronize(SyncType::COMMIT);
 }
 
 void* MPSHooks::getCommandBuffer() const {
-  auto stream = at::mps::getDefaultMPSStream();
+  // THREAD-SAFETY FIX: Use current thread's stream for proper parallel execution
+  // 32.83 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+  // static destruction when pool is destroyed before this code runs.
+  auto stream = at::mps::getCurrentMPSStream();
+  if (!stream) {
+    return nullptr;
+  }
   // Release pending computeCommandEncoder, as extensions is likely to allocate new one
   stream->endKernelCoalescing();
   return stream->commandBuffer();
 }
 
 void* MPSHooks::getDispatchQueue() const {
-  return at::mps::getDefaultMPSStream()->queue();
+  // THREAD-SAFETY FIX: Use current thread's stream for proper parallel execution
+  // 32.83 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+  // static destruction when pool is destroyed before this code runs.
+  auto stream = at::mps::getCurrentMPSStream();
+  if (!stream) {
+    return nullptr;
+  }
+  return stream->queue();
 }
 
 void MPSHooks::emptyCache() const {
diff --git a/aten/src/ATen/mps/MPSProfiler.h b/aten/src/ATen/mps/MPSProfiler.h
index c1cb9090..97fceb04 100644
--- a/aten/src/ATen/mps/MPSProfiler.h
+++ b/aten/src/ATen/mps/MPSProfiler.h
@@ -24,6 +24,20 @@ namespace at::mps {
 
 namespace Profiler {
 
+// THREAD SAFETY WARNING:
+// The MPS profiler uses shared unordered_maps (m_op_info_list, m_cpufallback_info_list,
+// m_copy_info_list, m_copystat_info_list) and non-atomic counters (runCount) without
+// mutex protection. When using multi-threaded MPS inference:
+//
+// 1. DISABLE profiling during parallel inference:
+//    - Do NOT enable signpost tracing (PYTORCH_MPS_LOG_LEVEL)
+//    - Do NOT enable operation profiling
+//    - Do NOT use Metal capture during parallel execution
+//
+// 2. Profiling is safe for single-threaded code or when all threads use the same stream.
+//
+// Future work: Add per-thread profiler structures or mutex protection for thread-safe profiling.
+
 struct BaseInfo {
   // profiling info types
   enum class Type {
@@ -379,9 +393,10 @@ class MPSProfiler {
   // stats logging could run either from destructor or signal handler
   // so this is used to check if logging has already started.
   std::atomic_bool hasLoggedStats{false};
-  // indicates there are pending completionHandler callbacks that haven't been
-  // called yet.
-  std::atomic_bool hasPendingCompletionHandlers{false};
+  // Counter of pending completionHandler callbacks that haven't been called yet.
+  // THREAD-SAFETY (27.9): Use atomic counter instead of bool to correctly track
+  // multiple pending handlers across different streams.
+  std::atomic<uint32_t> pendingCompletionHandlers{0};
   // used to capture sigint signal to log profiling stats
   static struct sigaction currentSigint, previousSigint;
 
diff --git a/aten/src/ATen/mps/MPSProfiler.mm b/aten/src/ATen/mps/MPSProfiler.mm
index a91574c5..b9158d93 100644
--- a/aten/src/ATen/mps/MPSProfiler.mm
+++ b/aten/src/ATen/mps/MPSProfiler.mm
@@ -2,8 +2,11 @@
 
 #include <ATen/mps/MPSProfiler.h>
 #include <c10/util/Exception.h>
+#include <c10/util/ScopeExit.h>
 #include <c10/util/env.h>
 #include <fmt/format.h>
+#include <chrono>
+#include <thread>
 
 // these need to be literal strings when passed to os_signpost*()
 // function macros; so no LUTs could be used
@@ -22,6 +25,16 @@
 namespace at::mps {
 namespace Profiler {
 
+// 32.109 fix: Flag to track if profiler is alive for safe handler access during static destruction.
+// Metal completion handlers run asynchronously AFTER waitUntilCompleted returns (Apple docs).
+// Without this check, handlers could access freed info objects after ~MPSProfiler destroys them.
+static std::atomic<bool> s_profiler_alive{false};
+
+// 32.109 fix: Counter for pending profiler handlers (completion + scheduled).
+// Unlike pendingCompletionHandlers (member variable), this is static so it survives MPSProfiler
+// destruction. Handlers decrement this counter after completing, allowing the destructor to wait.
+static std::atomic<uint32_t> s_profiler_pending_handlers{0};
+
 const std::string BaseInfo::toString(double gpuTime, double schedulingTime) const {
   // the gpuTime will be non-zero mainly for event-based signposts.
   // The interval-based signposts will have "duration" as well as accumulated
@@ -121,16 +134,56 @@ MPSProfiler::MPSProfiler() : m_os_log_events(nullptr), m_os_log_intervals(nullpt
   previousSigint.sa_handler = nullptr;
 
   initialize();
+  // 32.109 fix: Mark profiler as alive AFTER initialization completes
+  s_profiler_alive.store(true, std::memory_order_release);
 }
 
 MPSProfiler::~MPSProfiler() {
-  // first make sure completion handlers are completed
-  auto stream = getDefaultMPSStream();
-  dispatch_sync(stream->queue(), ^() {
-    if (hasPendingCompletionHandlers) {
-      stream->synchronize(SyncType::COMMIT_AND_WAIT);
+  // 32.109 fix: Mark profiler as no longer alive FIRST (before any cleanup).
+  // This prevents completion handlers from accessing info objects after we start destruction.
+  // Handlers that see s_profiler_alive=false will skip the info access.
+  s_profiler_alive.store(false, std::memory_order_release);
+
+  // THREAD-SAFETY (27.9): Sync ALL streams since completion handlers can be added
+  // to any stream via getCurrentMPSStream(), not just the default stream.
+  // Wait for all pending handlers to complete before proceeding.
+  // THREAD-SAFETY (29.3): Check if pool is still alive - static destruction
+  // order is undefined and pool may already be destroyed.
+  // 32.110 fix: Also sync if there are pending scheduled handlers, since those
+  // can race with destruction just like completion handlers.
+  if ((pendingCompletionHandlers.load() > 0 ||
+       s_profiler_pending_handlers.load(std::memory_order_acquire) > 0) &&
+      MPSStreamPool::isPoolAlive()) {
+    MPSStreamPool::instance().synchronizeAllStreams();
+  }
+
+  // 32.109 fix: Wait for profiler handlers to finish before destroying info objects.
+  // Metal completion handlers run asynchronously on a separate thread AFTER
+  // waitUntilCompleted returns (Apple documentation). synchronizeAllStreams() uses
+  // waitUntilCompleted, so handlers may still be running/pending after it returns.
+  // Without this wait, info objects could be freed while handlers access them (UAF).
+  // Note: Handlers that see s_profiler_alive=false will skip info access and exit early,
+  // but they still decrement s_profiler_pending_handlers, so we must wait.
+  constexpr int kFastSpinCount = 1000;   // ~1ms with 1us sleep
+  constexpr int kSlowSpinCount = 5000;   // ~5s with 1ms sleep
+  int spin_count = 0;
+  while (s_profiler_pending_handlers.load(std::memory_order_acquire) > 0) {
+    if (spin_count < kFastSpinCount) {
+      std::this_thread::sleep_for(std::chrono::microseconds(1));
+    } else if (spin_count < kFastSpinCount + kSlowSpinCount) {
+      std::this_thread::sleep_for(std::chrono::milliseconds(1));
+    } else {
+      // If handlers haven't completed after 5+ seconds, something is seriously wrong.
+      // Warn but continue since handlers check s_profiler_alive before info access.
+      TORCH_WARN(
+          "MPSProfiler::~MPSProfiler: timeout after 5s waiting for profiler handlers. "
+          "s_profiler_pending_handlers=", s_profiler_pending_handlers.load(std::memory_order_acquire),
+          ". Proceeding with shutdown (handlers will skip info access due to alive flag).");
+      break;  // Safe to continue because handlers check s_profiler_alive before info access
     }
-  });
+    ++spin_count;
+  }
+
   logProfilingStats();
 
   if (m_os_log_events) {
@@ -431,14 +484,34 @@ void MPSProfiler::endProfileCopy(uint64_t profileId, SyncType syncType) {
 void MPSProfiler::addProfilerScheduledHandler(BaseInfo& info) {
   const SignpostTypes signpostType = getSignpostType(info.type);
   const os_signpost_id_t intervalSignpostId = info.intervalSignpostId;
-
-  auto m_stream = getDefaultMPSStream();
-  // NOTE: the following block isn't thread-safe
-  [m_stream->commandBuffer() addScheduledHandler:^(id<MTLCommandBuffer> cb) {
+  // 32.110 fix: Capture message string at registration time so the scheduled handler
+  // doesn't touch `info` (which may be destroyed during shutdown).
+  const std::string infoStr = info.toString();
+
+  // Use getCurrentMPSStream() to add handler to the calling thread's stream
+  // This ensures thread-safe profiling when using the stream pool
+  auto current_stream = getCurrentMPSStream();
+  // 32.85 fix: During static destruction, getCurrentMPSStream() returns nullptr.
+  // Skip handler registration to avoid crash dereferencing nullptr.
+  if (!current_stream) {
+    return;
+  }
+  // 32.110 fix: Track pending scheduled handlers for safe destruction.
+  // Like completion handlers, scheduled handlers may run asynchronously AFTER
+  // waitUntilCompleted returns, and can race with ~MPSProfiler().
+  s_profiler_pending_handlers.fetch_add(1, std::memory_order_acq_rel);
+  current_stream->addScheduledHandler(^(id<MTLCommandBuffer>) {
+    // Decrement counter when handler completes (always, even on early exit)
+    auto decrement_guard = c10::make_scope_exit([&]() {
+      s_profiler_pending_handlers.fetch_sub(1, std::memory_order_release);
+    });
+    // Skip handler work if profiler is shutting down - logs/handles may be destroyed
+    if (!s_profiler_alive.load(std::memory_order_acquire)) {
+      return;
+    }
     // begin the interval once scheduling has completed (if INCLUDE_SCHEDULE_INTERVAL flag is disabled)
-    beginSignpostInterval(signpostType, intervalSignpostId, info.toString());
-    info.completed = false;
-  }];
+    beginSignpostInterval(signpostType, intervalSignpostId, infoStr);
+  });
 }
 
 void MPSProfiler::updateCopyStats(const CopyInfo& copyInfo, double gpuTime, double schedulingTime) {
@@ -469,21 +542,45 @@ void MPSProfiler::addProfilerCompletedHandler(BaseInfo& info, SyncType syncType)
   // reset signpostIds for sanity check on next call
   info.intervalSignpostId = 0;
   info.eventSignpostId = 0;
-  hasPendingCompletionHandlers = true;
 
-  auto m_stream = getDefaultMPSStream();
-  // NOTE: the following block isn't thread-safe
-  [m_stream->commandBuffer() addCompletedHandler:^(id<MTLCommandBuffer> cb) {
+  // Use getCurrentMPSStream() to add handler to the calling thread's stream
+  // This ensures thread-safe profiling when using the stream pool
+  auto current_stream = getCurrentMPSStream();
+  // 32.85 fix: During static destruction, getCurrentMPSStream() returns nullptr.
+  // Skip handler registration to avoid crash dereferencing nullptr.
+  // Important: We check BEFORE incrementing counters to avoid them getting stuck.
+  if (!current_stream) {
+    return;
+  }
+  // THREAD-SAFETY (27.9): Increment counter instead of setting bool
+  // Note: Must be incremented AFTER null check to ensure decrement always happens
+  ++pendingCompletionHandlers;
+  // 32.109 fix: Track pending handlers with static counter for safe destruction.
+  // Metal completion handlers run asynchronously AFTER waitUntilCompleted returns (Apple docs).
+  // The destructor waits for this counter to reach 0 before destroying info objects.
+  s_profiler_pending_handlers.fetch_add(1, std::memory_order_acq_rel);
+  current_stream->addCompletedHandler(^(id<MTLCommandBuffer> cb) {
+    // 32.109 fix: Decrement static counter when handler completes (always, even on early exit)
+    auto decrement_guard = c10::make_scope_exit([&]() {
+      s_profiler_pending_handlers.fetch_sub(1, std::memory_order_release);
+    });
+    // 32.109 fix: Skip info access if profiler is shutting down - info may be destroyed
+    if (!s_profiler_alive.load(std::memory_order_acquire)) {
+      // THREAD-SAFETY (27.9): Still decrement the member counter for consistency
+      --pendingCompletionHandlers;
+      return;
+    }
     CFTimeInterval gpuTime = cb.GPUEndTime > cb.GPUStartTime ? (cb.GPUEndTime - cb.GPUStartTime) * 1000.0 : 0.;
     CFTimeInterval schedulingTime =
         cb.kernelEndTime > cb.kernelStartTime ? (cb.kernelEndTime - cb.kernelStartTime) * 1000.0 : 0.;
 
     endProfileExecution(info, eventSignpostId, intervalSignpostId, gpuTime, schedulingTime);
-    hasPendingCompletionHandlers = false;
-  }];
+    // THREAD-SAFETY (27.9): Decrement counter instead of setting bool
+    --pendingCompletionHandlers;
+  });
 
-  m_stream->synchronize((m_profile_options & ProfileOptions::WAIT_UNTIL_COMPLETED) ? SyncType::COMMIT_AND_WAIT
-                                                                                   : syncType);
+  current_stream->synchronize((m_profile_options & ProfileOptions::WAIT_UNTIL_COMPLETED) ? SyncType::COMMIT_AND_WAIT
+                                                                                          : syncType);
 }
 
 void MPSProfiler::logOperationsProfilingStats(std::FILE* f) const {
@@ -821,11 +918,9 @@ void MPSProfiler::stopCapture(MPSStream* stream) {
 } // namespace Profiler
 
 Profiler::MPSProfiler& getMPSProfiler() {
-  static std::unique_ptr<Profiler::MPSProfiler> mps_profiler;
-  if (mps_profiler == nullptr) {
-    mps_profiler = std::make_unique<Profiler::MPSProfiler>();
-  }
-  return *mps_profiler;
+  // C++11 guarantees thread-safe initialization of function-local statics
+  static Profiler::MPSProfiler mps_profiler;
+  return mps_profiler;
 }
 
 } // namespace at::mps
diff --git a/aten/src/ATen/mps/MPSStream.h b/aten/src/ATen/mps/MPSStream.h
index 10627cfc..527b45a0 100644
--- a/aten/src/ATen/mps/MPSStream.h
+++ b/aten/src/ATen/mps/MPSStream.h
@@ -4,6 +4,10 @@
 
 #include <cstdint>
 #include <utility>
+#include <array>
+#include <atomic>
+#include <memory>
+#include <mutex>
 
 #include <ATen/mps/MPSDevice.h>
 #include <c10/core/DeviceGuard.h>
@@ -42,6 +46,16 @@ namespace at::mps {
 //-----------------------------------------------------------------
 //  MPSStream
 //-----------------------------------------------------------------
+//
+// THREAD SAFETY MODEL:
+// - Streams are cached per-thread via TLS (getCurrentMPSStream()).
+// - Pooled worker streams may be reused across threads (CUDA-style round-robin),
+//   so stream methods must tolerate concurrent callers.
+// - The serial dispatch queue is used to serialize GPU encoding work for this
+//   stream and to avoid re-entrant dispatch_sync on the same queue.
+// - _streamMutex protects per-stream mutable state (_commandBuffer/_commandEncoder)
+//   and is taken inside dispatched blocks; never hold it while calling
+//   dispatch_sync, because GCD may execute the block on a different thread.
 
 enum class SyncType {
   NONE, // no commit to command buffer
@@ -72,6 +86,11 @@ class TORCH_API MPSStream {
   MPSCommandBuffer_t commandBuffer();
   MTLComputeCommandEncoder_t commandEncoder();
   void endKernelCoalescing();
+  // Encode shared-event synchronization commands under the stream mutex.
+  // This is required because some callers (e.g., allocator recordStream()) must
+  // avoid dispatch_sync to the stream queue while holding allocator locks.
+  void encodeSignalEvent(MTLSharedEvent_t event, uint64_t value);
+  void encodeWaitForEvent(MTLSharedEvent_t event, uint64_t value);
   void synchronize(SyncType syncType);
   void fill(MTLBuffer_t buffer, uint8_t value, size_t length, size_t offset, SyncType syncType = SyncType::NONE);
   void copy(MTLBuffer_t srcBuffer,
@@ -92,6 +111,7 @@ class TORCH_API MPSStream {
                        NSDictionary* feeds,
                        NSDictionary* results,
                        SyncType syncType = SyncType::NONE);
+  void addScheduledHandler(MTLCommandBufferHandler block);
   void addCompletedHandler(MTLCommandBufferHandler block);
 
   /// Get the MPS device index that this stream is associated with.
@@ -119,8 +139,10 @@ class TORCH_API MPSStream {
   MPSGraphExecutionDescriptor* _executionDescriptor = nil;
   MPSGraphCompilationDescriptor* _compilationDescriptor = nil;
   dispatch_queue_t _serialQueue = nullptr;
-  // CommitAndContinue is enabled by default
-  bool _enableCommitAndContinue = true;
+  // CommitAndContinue is disabled for thread safety
+  bool _enableCommitAndContinue = false;
+  // Mutex to serialize all operations on this stream from multiple threads
+  mutable std::recursive_mutex _streamMutex;
 
   // use synchronize() to access any of these commit functions outside MPSStream
   void commit();
@@ -130,29 +152,150 @@ class TORCH_API MPSStream {
 };
 
 /**
- * Get the current MPS stream
+ * Get the current MPS stream for the current execution context.
+ *
+ * If called from within an `MPSStream` serial dispatch queue, returns the owning
+ * stream for that queue. Otherwise returns the thread-local current stream, or
+ * the default stream if not set.
  */
 TORCH_API MPSStream* getCurrentMPSStream();
 
 /**
- * Get the default MPS stream
+ * Get the default MPS stream (stream 0).
+ * This is the stream used by single-threaded code and is always available.
  */
 TORCH_API MPSStream* getDefaultMPSStream();
 
+/**
+ * Get a stream from the MPS stream pool.
+ *
+ * Returns a non-default stream (id 1..kMPSStreamsPerPool-1) in a CUDA-style
+ * round-robin fashion. Streams may be reused across threads; prefer
+ * getCurrentMPSStream() for a stable per-thread stream pointer.
+ */
+TORCH_API MPSStream* getStreamFromPool();
+
+/**
+ * Set the current stream for the calling thread.
+ * This affects subsequent getCurrentMPSStream() calls from this thread.
+ */
+TORCH_API void setCurrentMPSStream(MPSStream* stream);
+
+/**
+ * Internal: returns the dispatch queue-specific key used by MPSStream queues.
+ * The value stored for this key is the owning `MPSStream*`.
+ */
+TORCH_API void* getMPSStreamQueueSpecificKey();
+
 //-----------------------------------------------------------------
-//  MPSStreamImpl
+//  MPSStreamPool
 //-----------------------------------------------------------------
+// Stream pool for enabling parallel MPS inference.
+// Modeled after c10::cuda::CUDAStream pool design.
+//
+// Key design principles:
+// - 32 streams per pool (matching CUDA's kStreamsPerPool)
+// - CUDA-style round-robin selection for non-default streams
+// - Thread-local current stream caching for stable per-thread selection
+// - Lazy initialization on first use
+// - Default stream (index 0) always available for backward compatibility
 
-class TORCH_API MPSStreamImpl {
+static constexpr int kMPSStreamsPerPoolBits = 5;
+static constexpr int kMPSStreamsPerPool = 1 << kMPSStreamsPerPoolBits;  // 32 streams
+
+class TORCH_API MPSStreamPool {
  public:
   /**
-   * Gets single instance of the MPSStream.
+   * Get the singleton MPSStreamPool instance.
+   * Thread-safe via static initialization.
+   */
+  static MPSStreamPool& instance();
+
+  /**
+   * Get the default stream (stream 0).
+   * This is always the same stream, used for single-threaded code.
+   */
+  MPSStream* getDefaultStream();
+
+  /**
+   * Get stream by index (0 to kMPSStreamsPerPool-1).
+   * Used internally and for advanced use cases.
+   */
+  MPSStream* getStream(size_t index);
+
+  /**
+   * Get the thread-local current stream.
+   * Returns default stream if no stream has been set for this thread.
+   */
+  static MPSStream* getCurrentStream();
+
+  /**
+   * Set the thread-local current stream.
    */
-  static MPSStream* getInstance();
+  static void setCurrentStream(MPSStream* stream);
+
+  /**
+   * Get number of streams in the pool.
+   */
+  static constexpr size_t poolSize() { return kMPSStreamsPerPool; }
+
+  /**
+   * Heuristic for whether worker streams have ever been used.
+   *
+   * This intentionally does NOT attempt to count live threads: MPS uses a
+   * CUDA-style round-robin pool (streams can be reused across threads), so
+   * "active stream count" is not well-defined. Callers should treat
+   * `> 1` as "parallel streams may be in use".
+   */
+  size_t getActiveStreamCount() const;
+
+  /**
+   * Synchronize ALL active streams in the pool.
+   * This is used by torch.mps.synchronize() to implement true device-wide sync.
+   * Matches CUDA semantics where cudaDeviceSynchronize() waits for all streams.
+   */
+  void synchronizeAllStreams();
+
+  /**
+   * Clear the current thread's cached stream pointer.
+   *
+   * With round-robin pooling there is no exclusive slot ownership; this clears
+   * the thread-local cached stream pointer so the next MPS use by this thread
+   * reselects a stream. Safe to call multiple times (no-op if unset).
+   */
+  static void releaseCurrentThreadSlot();
+
+  /**
+   * Check if the stream pool singleton is still alive.
+   * Used by other singletons (e.g., MPSProfiler) in destructors to avoid
+   * use-after-free when static destruction order is undefined.
+   */
+  static bool isPoolAlive();
 
  private:
-  static MPSStream* _stream;
-  MPSStreamImpl();
+  MPSStreamPool();
+  ~MPSStreamPool();
+
+  // Non-copyable, non-movable
+  MPSStreamPool(const MPSStreamPool&) = delete;
+  MPSStreamPool& operator=(const MPSStreamPool&) = delete;
+  MPSStreamPool(MPSStreamPool&&) = delete;
+  MPSStreamPool& operator=(MPSStreamPool&&) = delete;
+
+  // Stream storage - lazily initialized
+  std::array<std::unique_ptr<MPSStream>, kMPSStreamsPerPool> streams_;
+
+  // NOTE: initialized_ removed in 32.37 fix - now uses stream_init_flags_[0] via call_once
+
+  // Mutex for thread-safe stream creation
+  // Cache-line aligned to prevent false sharing (Phase 24.4)
+  alignas(64) std::mutex stream_creation_mutex_;
+
+  // Per-stream once flags for lock-free fast-path (22.4 optimization)
+  std::array<std::once_flag, kMPSStreamsPerPool> stream_init_flags_;
+
+  void ensureInitialized();
+  MPSStream* createStream(size_t index);
 };
 
 } // namespace at::mps
diff --git a/aten/src/ATen/mps/MPSStream.mm b/aten/src/ATen/mps/MPSStream.mm
index 71325bd6..961fd845 100644
--- a/aten/src/ATen/mps/MPSStream.mm
+++ b/aten/src/ATen/mps/MPSStream.mm
@@ -3,6 +3,12 @@
 #include <ATen/mps/MPSAllocatorInterface.h>
 #include <ATen/mps/MPSProfiler.h>
 #include <ATen/mps/MPSStream.h>
+#include <c10/util/env.h>
+#include <mutex>
+#include <pthread.h>
+#include <algorithm>
+#include <string>
+#include <exception>
 
 @interface MPSGraphExecutionDescriptor ()
 @property(readwrite, atomic) BOOL enableCommitAndContinue;
@@ -10,20 +16,61 @@
 
 namespace at::mps {
 
+// NOTE: Global mutex g_mpsgraph_encode_mutex was REMOVED in N=109.
+// With thread-local MPSGraphCache, each thread has its own MPSGraph objects.
+// Testing confirmed that concurrent encoding to different graphs on different
+// streams is safe. This enables true parallel MPSGraph execution.
+// See MPS_PARALLEL_INFERENCE_PLAN.md Phase 21 for details.
+
+// Queue-specific key for detecting re-entrant dispatch_sync on the same stream queue.
+static char kMPSStreamQueueSpecificKey;
+
+void* getMPSStreamQueueSpecificKey() {
+  return &kMPSStreamQueueSpecificKey;
+}
+
 //-----------------------------------------------------------------
 //  MPSStream
 //-----------------------------------------------------------------
 
+// Helper to check env var for commitAndContinue override
+static int getCommitAndContinueEnvSetting() {
+  static int setting = []() {
+    auto env = c10::utils::get_env("MPS_ENABLE_COMMIT_AND_CONTINUE");
+    if (!env.has_value()) return -1;  // Not set - use default behavior
+    const char* str = env.value().c_str();
+    char* endptr = nullptr;
+    long val = std::strtol(str, &endptr, 10);
+    if (endptr == str || *endptr != '\0') {
+      TORCH_WARN("Invalid MPS_ENABLE_COMMIT_AND_CONTINUE value '", str,
+                 "', expected integer. Using default behavior.");
+      return -1;
+    }
+    return static_cast<int>(val);  // 0 = force disable, 1 = force enable
+  }();
+  return setting;
+}
+
 MPSStream::MPSStream(Stream stream) : _stream(stream) {
   _commandQueue = [MPSDevice::getInstance()->device() newCommandQueue];
   TORCH_CHECK(_stream.device_type() == DeviceType::MPS);
-  _serialQueue = dispatch_queue_create("metal gpu stream", nullptr);
+  const std::string queue_label =
+      "metal gpu stream " + std::to_string(static_cast<long long>(_stream.id()));
+  _serialQueue = dispatch_queue_create(queue_label.c_str(), nullptr);
+  dispatch_queue_set_specific(
+      _serialQueue, &kMPSStreamQueueSpecificKey, static_cast<void*>(this), nullptr);
   _executionDescriptor = [MPSGraphExecutionDescriptor new];
   _compilationDescriptor = [MPSGraphCompilationDescriptor new];
 
-  // disable commitAndContinue if Signpost tracing is enabled
-  if (getMPSProfiler().isSignpostTracingEnabled() || getMPSProfiler().isCaptureEnabled()) {
-    _enableCommitAndContinue = false;
+  // commitAndContinue allows GPU pipelining but can cause issues with concurrent streams.
+  // Default: Enable for stream 0 (single-thread), disable for worker streams (parallel).
+  // Override via MPS_ENABLE_COMMIT_AND_CONTINUE env var (0=disable, 1=enable all).
+  int env_setting = getCommitAndContinueEnvSetting();
+  if (env_setting >= 0) {
+    _enableCommitAndContinue = (env_setting != 0);
+  } else {
+    // Default: only default stream (id=0) uses commitAndContinue
+    _enableCommitAndContinue = (_stream.id() == 0);
   }
   _executionDescriptor.enableCommitAndContinue = _enableCommitAndContinue;
 
@@ -39,11 +86,23 @@ MPSStream::~MPSStream() {
   [_compilationDescriptor release];
   _executionDescriptor = nil;
   _compilationDescriptor = nil;
+  if (_serialQueue) {
+    dispatch_release(_serialQueue);
+    _serialQueue = nullptr;
+  }
+
+  // THREAD-SAFETY FIX (21.19): Release _prevCommandBuffer to avoid memory leak
+  // when commitAndContinue is disabled and flush() was called without commitAndWait()
+  if (_prevCommandBuffer) {
+    [_prevCommandBuffer release];
+    _prevCommandBuffer = nil;
+  }
 
   assert(_commandBuffer == nil);
 }
 
 MPSCommandBuffer* MPSStream::commandBuffer() {
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   if (!_commandBuffer) {
     _commandBuffer = [MPSCommandBuffer commandBufferFromCommandQueue:_commandQueue].retain;
   }
@@ -52,10 +111,15 @@ MPSCommandBuffer* MPSStream::commandBuffer() {
 }
 
 id<MTLDevice> MPSStream::device() const {
+  // 32.111 fix: Null check on _commandQueue before accessing
+  if (!_commandQueue) {
+    return nil;
+  }
   return [_commandQueue device];
 }
 
 id<MTLComputeCommandEncoder> MPSStream::commandEncoder() {
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   if (!_commandEncoder) {
     _commandEncoder = [commandBuffer() computeCommandEncoder].retain;
   }
@@ -63,7 +127,24 @@ id<MTLComputeCommandEncoder> MPSStream::commandEncoder() {
   return _commandEncoder;
 }
 
+void MPSStream::encodeSignalEvent(id<MTLSharedEvent> event, uint64_t value) {
+  TORCH_INTERNAL_ASSERT(event);
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
+  endKernelCoalescing();
+  id<MTLCommandBuffer> commandBuffer = this->commandBuffer();
+  [commandBuffer encodeSignalEvent:event value:value];
+}
+
+void MPSStream::encodeWaitForEvent(id<MTLSharedEvent> event, uint64_t value) {
+  TORCH_INTERNAL_ASSERT(event);
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
+  endKernelCoalescing();
+  id<MTLCommandBuffer> commandBuffer = this->commandBuffer();
+  [commandBuffer encodeWaitForEvent:event value:value];
+}
+
 void MPSStream::synchronize(SyncType syncType) {
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   endKernelCoalescing();
   switch (syncType) {
     case SyncType::NONE:
@@ -90,6 +171,11 @@ void MPSStream::synchronize(SyncType syncType) {
 }
 
 void MPSStream::commit() {
+  // 32.63: Take lock for thread-safety. With round-robin stream allocation (33.1),
+  // multiple threads may call commit() on the same stream. Although commit() is
+  // typically called from synchronize() which holds the lock, external callers
+  // may not hold the lock. Using recursive_mutex allows safe reentry.
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   if (_enableCommitAndContinue) {
     [commandBuffer() commitAndContinue];
   } else {
@@ -98,6 +184,10 @@ void MPSStream::commit() {
 }
 
 void MPSStream::commitAndWait() {
+  // 32.63: Take lock for thread-safety. With round-robin stream allocation (33.1),
+  // multiple threads may share the same stream. Protects _prevCommandBuffer and
+  // _commandBuffer access from concurrent modification.
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   if (_prevCommandBuffer) {
     // the previous command buffer (if exists) has already been committed,
     // so we just wait until it's completed and then dispose it.
@@ -115,11 +205,17 @@ void MPSStream::commitAndWait() {
 }
 
 void MPSStream::commitAndContinue() {
+  // 32.63: Take lock for thread-safety when accessing _commandBuffer.
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   assert(_commandBuffer);
   [_commandBuffer commitAndContinue];
 }
 
 void MPSStream::endKernelCoalescing() {
+  // 33.5: Take lock to prevent data race with commandEncoder() which also
+  // accesses _commandEncoder under _streamMutex. With round-robin stream
+  // reuse (33.1), another thread may call commandEncoder() on the same stream.
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   if (_commandEncoder) {
     [_commandEncoder endEncoding];
     [_commandEncoder release];
@@ -128,11 +224,19 @@ void MPSStream::endKernelCoalescing() {
 }
 
 void MPSStream::flush() {
+  // 32.63: Take lock for thread-safety when accessing _commandBuffer/_prevCommandBuffer.
+  // Called from commit() which now holds the lock, but recursive_mutex allows reentry.
+  std::lock_guard<std::recursive_mutex> lock(_streamMutex);
   if (_commandBuffer) {
     [_commandBuffer commit];
     // if commitAndContinue is disabled (e.g., for Profiler), we keep the command
     // buffer so we could wait on it later, if required.
     if (!_enableCommitAndContinue) {
+      // Release previous command buffer to avoid leak if flush() is called
+      // multiple times before commitAndWait() (which normally releases it).
+      if (_prevCommandBuffer) {
+        [_prevCommandBuffer release];
+      }
       _prevCommandBuffer = _commandBuffer;
     } else {
       [_commandBuffer release];
@@ -141,20 +245,51 @@ void MPSStream::flush() {
   }
 }
 
+void MPSStream::addScheduledHandler(MTLCommandBufferHandler block) {
+  dispatch_block_t dispatch_block = ^() {
+    @autoreleasepool {
+      std::lock_guard<std::recursive_mutex> lock(_streamMutex);
+      [commandBuffer() addScheduledHandler:block];
+    }
+  };
+  if (dispatch_get_specific(&kMPSStreamQueueSpecificKey) == static_cast<void*>(this)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(_serialQueue, dispatch_block);
+  }
+}
+
 void MPSStream::addCompletedHandler(MTLCommandBufferHandler block) {
-  dispatch_sync(_serialQueue, ^() {
+  dispatch_block_t dispatch_block = ^() {
     @autoreleasepool {
-      [commandBuffer() addCompletedHandler:block];
+      // Do not hold _streamMutex across dispatch_sync. GCD may run the block on a
+      // different thread, and stream helpers (e.g. commandBuffer()) take the same
+      // mutex. Holding it here would deadlock.
+      std::lock_guard<std::recursive_mutex> lock(_streamMutex);
+      MPSCommandBuffer* cb = _commandBuffer ? _commandBuffer : _prevCommandBuffer;
+      if (!cb) {
+        cb = commandBuffer();
+      }
+      [cb addCompletedHandler:block];
     }
-  });
+  };
+  if (dispatch_get_specific(&kMPSStreamQueueSpecificKey) == static_cast<void*>(this)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(_serialQueue, dispatch_block);
+  }
 }
 
 void MPSStream::fill(id<MTLBuffer> buffer, uint8_t value, size_t length, size_t offset, SyncType syncType) {
   if (length == 0) {
     return;
   }
-  dispatch_sync(_serialQueue, ^() {
+  dispatch_block_t dispatch_block = ^() {
     @autoreleasepool {
+      // Serialize operations on this stream. Do not take _streamMutex before
+      // dispatch_sync: GCD may execute the block on a different thread, and
+      // stream helpers take _streamMutex.
+      std::lock_guard<std::recursive_mutex> lock(_streamMutex);
       endKernelCoalescing();
       id<MTLBlitCommandEncoder> blitEncoder = [commandBuffer() blitCommandEncoder];
 
@@ -173,7 +308,12 @@ void MPSStream::fill(id<MTLBuffer> buffer, uint8_t value, size_t length, size_t
       [blitEncoder endEncoding];
       synchronize(syncType);
     }
-  });
+  };
+  if (dispatch_get_specific(&kMPSStreamQueueSpecificKey) == static_cast<void*>(this)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(_serialQueue, dispatch_block);
+  }
 }
 
 void MPSStream::copy(id<MTLBuffer> srcBuffer,
@@ -183,8 +323,12 @@ void MPSStream::copy(id<MTLBuffer> srcBuffer,
                      size_t dstOffset,
                      uint64_t profileId,
                      SyncType syncType) {
-  dispatch_sync(_serialQueue, ^() {
+  dispatch_block_t dispatch_block = ^() {
     @autoreleasepool {
+      // Serialize operations on this stream. Do not take _streamMutex before
+      // dispatch_sync: GCD may execute the block on a different thread, and
+      // stream helpers take _streamMutex.
+      std::lock_guard<std::recursive_mutex> lock(_streamMutex);
       endKernelCoalescing();
       id<MTLBlitCommandEncoder> blitEncoder = [commandBuffer() blitCommandEncoder];
 
@@ -213,7 +357,12 @@ void MPSStream::copy(id<MTLBuffer> srcBuffer,
         synchronize(syncType);
       }
     }
-  });
+  };
+  if (dispatch_get_specific(&kMPSStreamQueueSpecificKey) == static_cast<void*>(this)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(_serialQueue, dispatch_block);
+  }
 }
 
 void MPSStream::copy_and_sync(id<MTLBuffer> srcBuffer,
@@ -236,57 +385,316 @@ void MPSStream::executeMPSGraph(MPSGraph* mpsGraph, NSDictionary* feeds, NSDicti
   auto& profiler = getMPSProfiler();
   const bool isGraphProfilingEnabled = profiler.isOperationProfilingEnabled();
 
-  dispatch_sync(_serialQueue, ^() {
-    endKernelCoalescing();
-    if (isGraphProfilingEnabled) {
-      // this function call is only relevant for interval-based Signposts
-      // which exclude schedule time (only includes GPU run time)
-      profiler.beginProfileGPUInterval(mpsGraph);
-    }
-    // note: CommitAndContinue feature is enabled/disabled via "_executionDescriptor"
-    [mpsGraph encodeToCommandBuffer:commandBuffer()
-                              feeds:feeds
-                   targetOperations:nil
-                  resultsDictionary:results
-                executionDescriptor:_executionDescriptor];
-
-    SyncType _syncType = syncType;
-    // if commitAndContinue is disabled, we need to always commit manually after encoding
-    if (!_enableCommitAndContinue && syncType != SyncType::COMMIT_AND_WAIT) {
-      _syncType = SyncType::COMMIT;
-    }
+  // NOTE: No global mutex needed here. With thread-local MPSGraphCache,
+  // each thread encodes to its own graph objects on its own stream.
+  // See N=109 for testing that confirmed this is safe.
 
-    // check if graph execution profiling is enabled
-    if (isGraphProfilingEnabled) {
-      // with profiler enabled, we commit after adding the completedHandler in MPSProfiler
-      profiler.endProfileKernel(mpsGraph, _syncType);
-    } else {
-      synchronize(_syncType);
+  dispatch_block_t dispatch_block = ^() {
+    @autoreleasepool {
+      // Serialize operations on this stream. Do not take _streamMutex before
+      // dispatch_sync: GCD may execute the block on a different thread, and
+      // stream helpers take _streamMutex.
+      std::lock_guard<std::recursive_mutex> stream_lock(_streamMutex);
+      endKernelCoalescing();
+      if (isGraphProfilingEnabled) {
+        // this function call is only relevant for interval-based Signposts
+        // which exclude schedule time (only includes GPU run time)
+        profiler.beginProfileGPUInterval(mpsGraph);
+      }
+      // note: CommitAndContinue feature is enabled/disabled via "_executionDescriptor"
+      [mpsGraph encodeToCommandBuffer:commandBuffer()
+                                feeds:feeds
+                     targetOperations:nil
+                    resultsDictionary:results
+                  executionDescriptor:_executionDescriptor];
+
+      SyncType _syncType = syncType;
+      // if commitAndContinue is disabled, we need to always commit manually after encoding
+      if (!_enableCommitAndContinue && syncType != SyncType::COMMIT_AND_WAIT) {
+        _syncType = SyncType::COMMIT;
+      }
+
+      // check if graph execution profiling is enabled
+      if (isGraphProfilingEnabled) {
+        // with profiler enabled, we commit after adding the completedHandler in MPSProfiler
+        profiler.endProfileKernel(mpsGraph, _syncType);
+      } else {
+        synchronize(_syncType);
+      }
     }
-  });
+  };
+  if (dispatch_get_specific(&kMPSStreamQueueSpecificKey) == static_cast<void*>(this)) {
+    dispatch_block();
+  } else {
+    dispatch_sync(_serialQueue, dispatch_block);
+  }
 }
 
 //-----------------------------------------------------------------
-//  MPSStreamImpl
+//  MPSStreamPool
 //-----------------------------------------------------------------
 
-MPSStream* MPSStreamImpl::_stream = nullptr;
+// Global flag to track if pool is still alive (for safe TLS destruction)
+static std::atomic<bool> g_pool_alive{false};
+// Tracks whether pool was ever created (to distinguish "not yet init" from "destroyed")
+static std::atomic<bool> g_pool_ever_created{false};
+// CUDA-style round-robin stream index counter (streams 1..kMPSStreamsPerPool-1).
+static std::atomic<uint32_t> g_stream_counter{0};
+// Heuristic: becomes true once any worker stream is used (id != 0).
+static std::atomic<bool> g_worker_stream_used{false};
+
+static size_t getWorkerStreamIndex() {
+  constexpr uint32_t kWorkerStreams = static_cast<uint32_t>(kMPSStreamsPerPool - 1);
+  const uint32_t raw_idx = g_stream_counter.fetch_add(1, std::memory_order_relaxed);
+  return static_cast<size_t>((raw_idx % kWorkerStreams) + 1);
+}
+
+static thread_local MPSStream* tls_current_stream = nullptr;
+
+MPSStreamPool& MPSStreamPool::instance() {
+  static MPSStreamPool pool;
+  return pool;
+}
+
+MPSStreamPool::MPSStreamPool() {
+  // 32.71 fix: Set g_pool_alive BEFORE g_pool_ever_created to prevent TOCTOU race.
+  // The check pattern is: if (!g_pool_alive && g_pool_ever_created) return nullptr.
+  // If we set g_pool_ever_created first, a concurrent thread could see:
+  //   g_pool_alive=false (not yet set), g_pool_ever_created=true (just set)
+  // and incorrectly return nullptr during pool initialization.
+  // By setting g_pool_alive first, if g_pool_ever_created is true, g_pool_alive
+  // is guaranteed to have been set to true (though may be false if destructor ran).
+  g_pool_alive.store(true, std::memory_order_release);
+  g_pool_ever_created.store(true, std::memory_order_release);
+}
+
+MPSStreamPool::~MPSStreamPool() {
+  g_pool_alive.store(false, std::memory_order_release);
+}
+
+void MPSStreamPool::ensureInitialized() {
+  // TOCTOU fix (32.37): Use call_once to prevent race condition where
+  // two threads both see initialized_==false and both call createStream(0).
+  // 32.38 fix: Removed redundant lock and null check - call_once already
+  // guarantees single-thread execution and single invocation.
+  std::call_once(stream_init_flags_[0], [this]() {
+    streams_[0] = std::unique_ptr<MPSStream>(
+        new MPSStream(Stream(Stream::UNSAFE, c10::Device(DeviceType::MPS, 0),
+                            static_cast<StreamId>(0))));
+  });
+}
+
+MPSStream* MPSStreamPool::createStream(size_t index) {
+  TORCH_CHECK(index < kMPSStreamsPerPool,
+              "Stream index ", index, " out of range [0, ", kMPSStreamsPerPool, ")");
+
+  // Always lock to avoid data race on unique_ptr read/write
+  // The lock is lightweight and stream creation is infrequent
+  std::lock_guard<std::mutex> lock(stream_creation_mutex_);
+  if (streams_[index] == nullptr) {
+    streams_[index] = std::unique_ptr<MPSStream>(
+        new MPSStream(Stream(Stream::UNSAFE, c10::Device(DeviceType::MPS, 0),
+                            static_cast<StreamId>(index))));
+  }
+  return streams_[index].get();
+}
+
+MPSStream* MPSStreamPool::getDefaultStream() {
+  ensureInitialized();
+  return streams_[0].get();
+}
+
+MPSStream* MPSStreamPool::getStream(size_t index) {
+  ensureInitialized();
+  // Phase 22.4 optimization: use call_once for lock-free fast-path
+  // 32.38 fix: Removed redundant lock and null check - call_once already
+  // guarantees single-thread execution and single invocation.
+  // 32.79 fix: Also hold stream_creation_mutex_ when writing to streams_[index].
+  // call_once only synchronizes with other call_once calls on the same flag.
+  // synchronizeAllStreams() reads streams_[i].get() under stream_creation_mutex_
+  // but doesn't use call_once, so without this lock there's a data race:
+  //   Thread A: inside call_once writing to streams_[5] (no mutex)
+  //   Thread B: reading streams_[5].get() under stream_creation_mutex_
+  // unique_ptr is not atomic, so concurrent read/write is UB.
+  TORCH_CHECK(index < kMPSStreamsPerPool,
+              "Stream index ", index, " out of range [0, ", kMPSStreamsPerPool, ")");
+  std::call_once(stream_init_flags_[index], [this, index]() {
+    std::lock_guard<std::mutex> lock(stream_creation_mutex_);
+    streams_[index] = std::unique_ptr<MPSStream>(
+        new MPSStream(Stream(Stream::UNSAFE, c10::Device(DeviceType::MPS, 0),
+                            static_cast<StreamId>(index))));
+  });
+  return streams_[index].get();
+}
+
+void MPSStreamPool::synchronizeAllStreams() {
+  ensureInitialized();
+  // Collect streams under lock, then synchronize outside lock to avoid
+  // holding stream_creation_mutex_ during potentially long GPU waits.
+  std::array<MPSStream*, kMPSStreamsPerPool> streams_to_sync{};
+  {
+    std::lock_guard<std::mutex> lock(stream_creation_mutex_);
+    for (size_t i = 0; i < kMPSStreamsPerPool; ++i) {
+      streams_to_sync[i] = streams_[i].get();
+    }
+  }
+  // 32.39 fix: Catch exceptions to ensure ALL streams are synchronized.
+  // If one stream fails, continue synchronizing others to avoid partial sync
+  // which could cause use-after-free when buffers are released afterwards.
+  std::exception_ptr first_exception = nullptr;
+  for (size_t i = 0; i < kMPSStreamsPerPool; ++i) {
+    if (streams_to_sync[i] != nullptr) {
+      try {
+        streams_to_sync[i]->synchronize(SyncType::COMMIT_AND_WAIT);
+      } catch (...) {
+        // Capture first exception but continue synchronizing other streams
+        if (!first_exception) {
+          first_exception = std::current_exception();
+        }
+        TORCH_WARN("synchronizeAllStreams: exception synchronizing stream ", i);
+      }
+    }
+  }
+  // Re-throw first exception after all streams have been synchronized
+  if (first_exception) {
+    std::rethrow_exception(first_exception);
+  }
+}
+
+bool MPSStreamPool::isPoolAlive() {
+  return g_pool_alive.load(std::memory_order_acquire);
+}
+
+MPSStream* MPSStreamPool::getCurrentStream() {
+  // 33.6 (found N=317): Check pool is alive before returning cached pointer.
+  // After pool destruction, tls_current_stream holds dangling pointer.
+  // FIX N=333: Only return nullptr if pool was created then destroyed.
+  // If pool was never created, we should proceed to initialize it.
+  if (!g_pool_alive.load(std::memory_order_acquire)) {
+    if (g_pool_ever_created.load(std::memory_order_acquire)) {
+      // Pool was created and is now destroyed - unsafe to access
+      return nullptr;
+    }
+    // Pool never created - fall through to initialize
+  }
+  if (tls_current_stream != nullptr) {
+    // 32.99 fix: Re-check pool is alive after reading TLS. This closes a TOCTOU race:
+    // 1. Thread A: initial g_pool_alive check passes (pool is alive)
+    // 2. Thread B: ~MPSStreamPool() runs, sets g_pool_alive=false, destroys streams_[]
+    // 3. Thread A: returns tls_current_stream which now points to freed memory (UAF!)
+    // The re-check ensures we return nullptr if destruction happened in the race window.
+    // This pattern is similar to the 32.68 fix for TLSBlockCache::flush().
+    if (!g_pool_alive.load(std::memory_order_acquire)) {
+      return nullptr;
+    }
+    return tls_current_stream;
+  }
+
+  // Use pthread_main_np() to detect the actual main thread (macOS-specific).
+  // This is more reliable than std::call_once which would mark the first
+  // thread to call this function as "main", even if it's a worker thread.
+  if (pthread_main_np() == 1) {
+    // Main thread uses default stream (id 0)
+    tls_current_stream = MPSStreamPool::instance().getDefaultStream();
+  } else {
+    // Worker thread: assign a stream from the pool via round-robin.
+    tls_current_stream = MPSStreamPool::instance().getStream(getWorkerStreamIndex());
+    g_worker_stream_used.store(true, std::memory_order_release);
+  }
 
-MPSStream* MPSStreamImpl::getInstance() {
-  if (_stream == nullptr) {
-    _stream = new MPSStream(Stream(Stream::UNSAFE, c10::Device(DeviceType::MPS, 0), 0));
+  // 32.104 fix: Re-check pool is alive after assigning tls_current_stream.
+  // This closes a TOCTOU race analogous to 32.99/32.101:
+  // 1. Thread A: initial g_pool_alive check passes (pool is alive)
+  // 2. Thread A: tls_current_stream == nullptr, falls through to here
+  // 3. Thread B: ~MPSStreamPool() runs, sets g_pool_alive=false, destroys streams_[]
+  // 4. Thread A: returns tls_current_stream which now points to freed memory (UAF!)
+  // The 32.99 fix only covered the case where tls_current_stream was already set.
+  // This fix covers the first-time initialization path.
+  if (!g_pool_alive.load(std::memory_order_acquire)) {
+    return nullptr;
   }
-  return _stream;
+  return tls_current_stream;
 }
 
-MPSStreamImpl::MPSStreamImpl() {}
+void MPSStreamPool::setCurrentStream(MPSStream* stream) {
+  TORCH_CHECK(stream != nullptr, "setCurrentMPSStream called with nullptr");
+  // Phase 22.3: Extract pool index directly from stream ID (set at creation)
+  // This avoids O(n) scan and mutex contention in setCurrentStream()
+  StreamId stream_id = stream->unwrap().id();
+  TORCH_CHECK(stream_id >= 0 && stream_id < kMPSStreamsPerPool,
+              "setCurrentMPSStream called with invalid stream (stream ID: ", stream_id,
+              " not in range [0, ", kMPSStreamsPerPool, "))");
+  tls_current_stream = stream;
+  if (stream_id != 0) {
+    g_worker_stream_used.store(true, std::memory_order_release);
+  }
+}
+
+size_t MPSStreamPool::getActiveStreamCount() const {
+  return g_worker_stream_used.load(std::memory_order_acquire) ? size_t{2} : size_t{1};
+}
+
+void MPSStreamPool::releaseCurrentThreadSlot() {
+  tls_current_stream = nullptr;
+}
+
+//-----------------------------------------------------------------
+//  Public API Functions
+//-----------------------------------------------------------------
 
 MPSStream* getCurrentMPSStream() {
-  return getDefaultMPSStream();
+  // If called from within a stream's serial dispatch queue, prefer the queue's
+  // owning stream over thread-local state. GCD may execute blocks on worker
+  // threads that do not carry the originating thread's TLS.
+  if (void* stream_ptr = dispatch_get_specific(getMPSStreamQueueSpecificKey())) {
+    return static_cast<MPSStream*>(stream_ptr);
+  }
+  return MPSStreamPool::getCurrentStream();
 }
 
 MPSStream* getDefaultMPSStream() {
-  return MPSStreamImpl::getInstance();
+  // 32.64 fix: Check if pool is alive before accessing. During static
+  // destruction, the pool may be destroyed and accessing instance() would
+  // return a dangling pointer or cause undefined behavior.
+  // FIX N=333: Only return nullptr if pool was created then destroyed.
+  if (!g_pool_alive.load(std::memory_order_acquire)) {
+    if (g_pool_ever_created.load(std::memory_order_acquire)) {
+      return nullptr;
+    }
+    // Pool never created - fall through to initialize via instance()
+  }
+  MPSStream* result = MPSStreamPool::instance().getDefaultStream();
+  // 32.101 fix: Re-check pool is alive after getting stream. This closes a TOCTOU race:
+  // 1. Thread A: initial g_pool_alive check passes (pool is alive)
+  // 2. Thread B: ~MPSStreamPool() runs, sets g_pool_alive=false, destroys streams_[]
+  // 3. Thread A: returns stream pointer which now points to freed memory (UAF!)
+  // Same pattern as 32.99 fix for getCurrentStream().
+  if (!g_pool_alive.load(std::memory_order_acquire)) {
+    return nullptr;
+  }
+  return result;
+}
+
+MPSStream* getStreamFromPool() {
+  // 32.64 fix: Check if pool is alive before accessing.
+  // FIX N=333: Only return nullptr if pool was created then destroyed.
+  if (!g_pool_alive.load(std::memory_order_acquire)) {
+    if (g_pool_ever_created.load(std::memory_order_acquire)) {
+      return nullptr;
+    }
+    // Pool never created - fall through to initialize via instance()
+  }
+  g_worker_stream_used.store(true, std::memory_order_release);
+  MPSStream* result = MPSStreamPool::instance().getStream(getWorkerStreamIndex());
+  // 32.101 fix: Re-check pool is alive after getting stream (same pattern as getDefaultMPSStream).
+  if (!g_pool_alive.load(std::memory_order_acquire)) {
+    return nullptr;
+  }
+  return result;
+}
+
+void setCurrentMPSStream(MPSStream* stream) {
+  MPSStreamPool::setCurrentStream(stream);
 }
 
 } // namespace at::mps
diff --git a/aten/src/ATen/native/mps/MetalShaderLibrary.h b/aten/src/ATen/native/mps/MetalShaderLibrary.h
index 535edd29..b2d14747 100644
--- a/aten/src/ATen/native/mps/MetalShaderLibrary.h
+++ b/aten/src/ATen/native/mps/MetalShaderLibrary.h
@@ -15,19 +15,25 @@ typedef void* MTLComputeCommandEncoder_t;
 
 #include <c10/core/Scalar.h>
 #include <c10/util/OptionalArrayRef.h>
+#include <array>
 #include <functional>
+#include <mutex>
 #include <optional>
 #include <type_traits>
 #include <unordered_map>
 #include <utility>
 #include <vector>
 
-// Forward declaration of TensorBase and TensorIteratorBase
+// Forward declarations
 namespace at {
 class TensorBase;
 struct TensorIteratorBase;
 } // namespace at
 
+namespace at::mps {
+class MPSStream;
+} // namespace at::mps
+
 namespace at::native::mps {
 
 namespace detail {
@@ -50,6 +56,20 @@ constexpr bool has_size_type_v = has_size_type<T>::value;
 // Returns `gpuAddress` of respective `id<MTLBuffer>` plus storage offset
 void* get_tensor_gpu_address(const at::TensorBase&);
 
+// MetalKernelFunction wraps a Metal compute pipeline state and manages command encoding.
+//
+// THREAD SAFETY: MetalKernelFunction instances are NOT thread-safe.
+// Each thread should obtain its own instance via getKernelFunction().
+// Do NOT share instances across threads - the mutable state (current_stream_,
+// encoder) is not protected by locks.
+//
+// Usage pattern:
+//   auto fn = shaderLib.getKernelFunction(...);  // Thread-local instance
+//   fn.runCommandBlock([&] {
+//     fn.startEncoding();
+//     fn.setArg(...);
+//     fn.dispatch(...);
+//   });
 class MetalKernelFunction {
  public:
   MetalKernelFunction(MTLComputePipelineState_t cps_, MTLFunction_t f_);
@@ -94,6 +114,9 @@ class MetalKernelFunction {
   MTLComputePipelineState_t cps;
   MTLFunction_t func;
   MTLComputeCommandEncoder_t encoder = nullptr;
+  // Store stream captured before dispatch_sync to avoid TLS hazard
+  // (GCD may run the block on a different thread with different TLS)
+  at::mps::MPSStream* current_stream_ = nullptr;
 };
 
 class MetalShaderLibrary {
@@ -159,11 +182,23 @@ class MetalShaderLibrary {
   std::string shaderSource;
   unsigned nparams;
   MTLCompileOptions* compile_options;
-  std::unordered_map<std::string, MTLLibrary_t> libMap;
-  std::unordered_map<
+  // 33.2: Sharded maps to allow parallel access without UB.
+  // Each shard has its own map, protected by its own mutex.
+  // Previously, 16 mutexes guarded single shared maps, which caused UB
+  // when different threads locked different shards and modified the same map.
+  static constexpr size_t kCacheShards = 16;
+  mutable std::array<std::mutex, kCacheShards> cacheMutexes_;
+  std::array<std::unordered_map<std::string, MTLLibrary_t>, kCacheShards> libMaps_;
+  std::array<std::unordered_map<
       std::string,
-      std::pair<MTLComputePipelineState_t, MTLFunction_t>>
-      cplMap;
+      std::pair<MTLComputePipelineState_t, MTLFunction_t>>, kCacheShards> cplMaps_;
+
+  // Helper to get shard index from key
+  size_t getShardIndex(const std::string& key) const {
+    return std::hash<std::string>{}(key) % kCacheShards;
+  }
+  // Thread-safe one-time initialization flag for the no-params library
+  mutable std::once_flag libraryOnceFlag_;
 };
 
 class DynamicMetalShaderLibrary : public MetalShaderLibrary {
diff --git a/aten/src/ATen/native/mps/OperationUtils.h b/aten/src/ATen/native/mps/OperationUtils.h
index f9cd28ca..d5bceb5f 100644
--- a/aten/src/ATen/native/mps/OperationUtils.h
+++ b/aten/src/ATen/native/mps/OperationUtils.h
@@ -3,6 +3,7 @@
 #pragma once
 
 #include <initializer_list>
+#include <memory>
 #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
 #include <ATen/Tensor.h>
 #include <ATen/TensorIterator.h>
@@ -236,13 +237,14 @@ struct MPSKernelCache {
  public:
   static MPSKernelCache* getInstance() {
     if (_instance_cache == nullptr) {
-      _instance_cache = new MPSKernelCache();
+      // Use unique_ptr(new T()) instead of make_unique because constructor is private
+      _instance_cache = std::unique_ptr<MPSKernelCache>(new MPSKernelCache());
     }
-    return _instance_cache;
+    return _instance_cache.get();
   }
 
   ~MPSKernelCache() {
-    dispatch_release(serialQueue_);
+    // Thread-local cache - no GCD queue needed (21.8 optimization)
     for (const auto& i : cache_) {
       delete i.second.cachedKernel_;
     }
@@ -253,19 +255,16 @@ struct MPSKernelCache {
   void operator=(const MPSKernelCache&) = delete;
 
   MPSCachedKernel* CreateCachedKernel(const std::string& key, CreateCachedKernelBlock createCacheBlock) {
-    __block MPSCachedKernel* cachedKernel = nil;
     MPSCacheKey hash = std::hash<std::string>{}(key);
-    dispatch_sync_with_rethrow(serialQueue_, ^() {
-      if (cache_.count(hash) != 0) {
-        auto& entry = cache_.at(hash);
-        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == entry.key_, "Key collision in the MPS cached kernel!\n");
-        cachedKernel = entry.cachedKernel_;
-      } else {
-        cachedKernel = createCacheBlock();
-        CacheEntry entry(key, cachedKernel);
-        cache_.emplace(hash, entry);
-      }
-    });
+    // Thread-local cache - no synchronization needed (21.8 optimization)
+    if (cache_.count(hash) != 0) {
+      auto& entry = cache_.at(hash);
+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == entry.key_, "Key collision in the MPS cached kernel!\n");
+      return entry.cachedKernel_;
+    }
+    MPSCachedKernel* cachedKernel = createCacheBlock();
+    CacheEntry entry(key, cachedKernel);
+    cache_.emplace(hash, entry);
     return cachedKernel;
   }
   template <typename T>
@@ -274,17 +273,14 @@ struct MPSKernelCache {
   }
 
   MPSCachedKernel* LookUp(const std::string& key) const {
-    __block MPSCachedKernel* cachedKernel = nil;
-
     MPSCacheKey hash = std::hash<std::string>{}(key);
-    dispatch_sync_with_rethrow(serialQueue_, ^() {
-      if (cache_.count(hash) != 0) {
-        auto& entry = cache_.at(hash);
-        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == entry.key_, "Key collision in the MPS cached kernel!\n");
-        cachedKernel = entry.cachedKernel_;
-      }
-    });
-    return cachedKernel;
+    // Thread-local cache - no synchronization needed (21.8 optimization)
+    auto it = cache_.find(hash);
+    if (it != cache_.end()) {
+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == it->second.key_, "Key collision in the MPS cached kernel!\n");
+      return it->second.cachedKernel_;
+    }
+    return nullptr;
   }
 
   template <typename T>
@@ -293,13 +289,14 @@ struct MPSKernelCache {
   }
 
  private:
-  MPSKernelCache() {
-    serialQueue_ = dispatch_queue_create("kernel cache queue", DISPATCH_QUEUE_SERIAL);
-  }
+  MPSKernelCache() = default;
 
-  static MPSKernelCache* _instance_cache;
+  // THREAD-SAFETY FIX: Each thread gets its own kernel cache to prevent
+  // concurrent encoding of shared kernel objects which may not be thread-safe.
+  // Using unique_ptr for RAII to ensure proper cleanup when thread exits.
+  // 21.8 optimization: No GCD serialQueue needed for thread-local cache.
+  static thread_local std::unique_ptr<MPSKernelCache> _instance_cache;
   std::unordered_map<MPSCacheKey, CacheEntry> cache_;
-  dispatch_queue_t serialQueue_ = nullptr;
 };
 
 // Common template for creating cached kernel if missing
@@ -330,14 +327,14 @@ struct MPSGraphCache {
  public:
   static MPSGraphCache* getInstance() {
     if (_instance_cache == nullptr) {
-      _instance_cache = new MPSGraphCache();
+      // Use unique_ptr(new T()) instead of make_unique because constructor is private
+      _instance_cache = std::unique_ptr<MPSGraphCache>(new MPSGraphCache());
     }
-    return _instance_cache;
+    return _instance_cache.get();
   }
 
   ~MPSGraphCache() {
-    dispatch_release(serialQueue_);
-
+    // Thread-local cache - no GCD queue needed (21.8 optimization)
     for (const auto& i : cache_) {
       delete i.second.cachedGraph_;
     }
@@ -348,23 +345,18 @@ struct MPSGraphCache {
   void operator=(const MPSGraphCache&) = delete;
 
   MPSCachedGraph* CreateCachedGraph(const std::string& key, CreateCachedGraphBlock createCacheBlock) {
-    __block MPSCachedGraph* cachedGraph = nil;
-
     MPSCacheKey hash = std::hash<std::string>{}(key);
-
-    dispatch_sync_with_rethrow(serialQueue_, ^() {
-      // verify the cached entry doesn't already exist
-      if (cache_.count(hash) != 0) {
-        auto& entry = cache_.at(hash);
-        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == entry.key_, "Key collision in the MPS cached graph!\n");
-        cachedGraph = entry.cachedGraph_;
-      } else {
-        cachedGraph = createCacheBlock();
-        CacheEntry entry(key, cachedGraph);
-        cache_.emplace(hash, entry);
-        profileCachedGraph(entry);
-      }
-    });
+    // Thread-local cache - no synchronization needed (21.8 optimization)
+    // verify the cached entry doesn't already exist
+    if (cache_.count(hash) != 0) {
+      auto& entry = cache_.at(hash);
+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == entry.key_, "Key collision in the MPS cached graph!\n");
+      return entry.cachedGraph_;
+    }
+    MPSCachedGraph* cachedGraph = createCacheBlock();
+    CacheEntry entry(key, cachedGraph);
+    cache_.emplace(hash, entry);
+    profileCachedGraph(entry);
     return cachedGraph;
   }
 
@@ -374,19 +366,15 @@ struct MPSGraphCache {
   }
 
   MPSCachedGraph* LookUp(const std::string& key) const {
-    __block MPSCachedGraph* cachedGraph = nullptr;
-
     MPSCacheKey hash = std::hash<std::string>{}(key);
-
-    dispatch_sync(serialQueue_, ^() {
-      if (cache_.count(hash) != 0) {
-        auto& entry = cache_.at(hash);
-        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == entry.key_, "Key collision in the MPS cached graph!\n");
-        cachedGraph = entry.cachedGraph_;
-        profileCachedGraph(entry);
-      }
-    });
-    return cachedGraph;
+    // Thread-local cache - no synchronization needed (21.8 optimization)
+    auto it = cache_.find(hash);
+    if (it != cache_.end()) {
+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(key == it->second.key_, "Key collision in the MPS cached graph!\n");
+      profileCachedGraph(it->second);
+      return it->second.cachedGraph_;
+    }
+    return nullptr;
   }
 
   template <typename T>
@@ -395,16 +383,19 @@ struct MPSGraphCache {
   }
 
  private:
-  MPSGraphCache() {
-    serialQueue_ = dispatch_queue_create("cache queue", DISPATCH_QUEUE_SERIAL);
-  }
+  MPSGraphCache() = default;
+
   // this is defined in OperationUtils.mm to not include
   // MPSProfiler.h in header OperationUtils.h
   void profileCachedGraph(const CacheEntry& cacheEntry) const;
 
-  static MPSGraphCache* _instance_cache;
+  // THREAD-SAFETY FIX: Each thread gets its own graph cache to prevent
+  // concurrent encoding of shared MPSGraph objects which is not thread-safe.
+  // This enables true parallel nn.Module inference across multiple threads.
+  // Using unique_ptr for RAII to ensure proper cleanup when thread exits.
+  // 21.8 optimization: No GCD serialQueue needed for thread-local cache.
+  static thread_local std::unique_ptr<MPSGraphCache> _instance_cache;
   std::unordered_map<MPSCacheKey, CacheEntry> cache_;
-  dispatch_queue_t serialQueue_ = nullptr;
 };
 
 // Common template for creating graph with a specified cache if missing
diff --git a/aten/src/ATen/native/mps/OperationUtils.mm b/aten/src/ATen/native/mps/OperationUtils.mm
index bf3e9420..8bcb811b 100644
--- a/aten/src/ATen/native/mps/OperationUtils.mm
+++ b/aten/src/ATen/native/mps/OperationUtils.mm
@@ -3,6 +3,7 @@
 #include <ATen/native/mps/MetalShaderLibrary.h>
 #include <c10/metal/common.h>
 #include <functional>
+#include <mutex>
 #include <stdexcept>
 #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
 #include <ATen/TensorIterator.h>
@@ -23,6 +24,7 @@
 #endif
 
 #include <c10/util/env.h>
+#include <c10/util/ScopeExit.h>
 #include <mach-o/dyld.h>
 #include <mach-o/getsect.h>
 
@@ -56,8 +58,22 @@
 
 namespace at::native::mps {
 
+// dispatch_sync wrapper that propagates C++ exceptions across the dispatch boundary.
+//
+// NOTE: dispatch_sync() to a serial queue from within that same queue is undefined behavior
+// and can deadlock. We detect this for MPSStream queues via the queue-specific key and
+// execute the block inline in that case.
 void dispatch_sync_with_rethrow(dispatch_queue_t queue, void (^block)()) {
-  __block std::optional<std::exception_ptr> block_exception;
+  const void* key = getMPSStreamQueueSpecificKey();
+  void* queue_specific_value = dispatch_queue_get_specific(queue, key);
+  if (queue_specific_value != nullptr && queue_specific_value == dispatch_get_specific(key)) {
+    block();
+    return;
+  }
+  // 32.45 FIX: Use std::exception_ptr directly instead of std::optional<std::exception_ptr>.
+  // std::exception_ptr is already nullable and has implicit bool conversion, so wrapping
+  // it in std::optional adds unnecessary overhead.
+  __block std::exception_ptr block_exception;
   dispatch_sync(queue, ^() {
     try {
       block();
@@ -66,7 +82,7 @@ void dispatch_sync_with_rethrow(dispatch_queue_t queue, void (^block)()) {
     }
   });
   if (block_exception) {
-    std::rethrow_exception(*block_exception);
+    std::rethrow_exception(block_exception);
   }
 }
 
@@ -461,6 +477,12 @@ MPSNDArray* getMPSNDArray(const TensorBase& t, const IntArrayRef& sizes, const I
   return getMPSNDArray(t, getMPSShape(sizes.empty() ? t.sizes() : sizes), strides.empty() ? nil : getMPSShape(strides));
 }
 
+// THREAD-SAFETY: Global mutex for MPSNDArrayIdentity operations.
+// Apple's MPS framework may have internal shared state that makes concurrent
+// MPSNDArrayIdentity operations unsafe, similar to MPSNDArrayMatrixMultiplication.
+// This mutex serializes reshape operations to prevent crashes.
+static std::mutex s_ndarray_identity_mutex;
+
 MPSNDArray* getStridedMPSNDArray(const TensorBase& src, MPSNDArray* srcNDArray) {
   auto strides = src.strides();
   auto sizes = src.sizes();
@@ -492,6 +514,8 @@ MPSNDArray* getStridedMPSNDArray(const TensorBase& src, MPSNDArray* srcNDArray)
 
   srcNDArray = [srcNDArray arrayViewWithShape:sortedMPSShape strides:sortedStridesShape];
   if (hasNonZeroStrides) {
+    // THREAD-SAFETY: Serialize MPSNDArrayIdentity operations.
+    std::lock_guard<std::mutex> lock(s_ndarray_identity_mutex);
     MPSNDArrayIdentity* identity =
         [[[MPSNDArrayIdentity alloc] initWithDevice:MPSDevice::getInstance()->device()] autorelease];
     srcNDArray = [identity reshapeWithCommandBuffer:nil
@@ -599,6 +623,8 @@ Placeholder::Placeholder(MPSGraphTensor* mpsGraphTensor,
       TORCH_INTERNAL_ASSERT(srcNDArray);
 
       if (needsReshape) {
+        // THREAD-SAFETY: Serialize MPSNDArrayIdentity operations.
+        std::lock_guard<std::mutex> lock(s_ndarray_identity_mutex);
         MPSNDArrayIdentity* identity =
             [[[MPSNDArrayIdentity alloc] initWithDevice:MPSDevice::getInstance()->device()] autorelease];
         srcNDArray = [identity reshapeWithCommandBuffer:nil sourceArray:srcNDArray shape:mpsShape destinationArray:nil];
@@ -769,9 +795,13 @@ std::string get_mem_format_string(c10::MemoryFormat memory_format) {
   return mem_format_key;
 }
 
-MPSGraphCache* MPSGraphCache::_instance_cache = nullptr;
+// THREAD-SAFETY FIX: Per-thread graph cache for parallel nn.Module inference
+// Using unique_ptr for RAII to ensure proper cleanup when thread exits (fixes memory leak).
+thread_local std::unique_ptr<MPSGraphCache> MPSGraphCache::_instance_cache = nullptr;
 
-MPSKernelCache* MPSKernelCache::_instance_cache = nullptr;
+// THREAD-SAFETY FIX: Per-thread kernel cache for parallel inference
+// Using unique_ptr for RAII to ensure proper cleanup when thread exits (fixes memory leak).
+thread_local std::unique_ptr<MPSKernelCache> MPSKernelCache::_instance_cache = nullptr;
 
 void MPSGraphCache::profileCachedGraph(const CacheEntry& cacheEntry) const {
   auto& profiler = getMPSProfiler();
@@ -786,30 +816,33 @@ void MPSGraphCache::profileCachedGraph(const CacheEntry& cacheEntry) const {
 
 class MPSGraphCacheCallback : public IMpsAllocatorCallback {
  public:
-  MPSGraphCacheCallback() : graph_cache(MPSGraphCache::getInstance()) {}
-
   void executeMPSAllocatorCallback(void* ptr, EventType event) override {}
-
- private:
-  MPSGraphCache* graph_cache;
 };
 
 REGISTER_MPS_ALLOCATOR_CALLBACK("mps_graph_cache_callback", MPSGraphCacheCallback);
 
 // MetalShaderLibrary implementation
 MetalShaderLibrary::~MetalShaderLibrary() {
-  for (const auto& it : cplMap) {
-    auto [cpl, func] = it.second;
-    [cpl release];
-    [func release];
+  // 33.2: Iterate over all shards since maps are now per-shard
+  for (size_t shard = 0; shard < kCacheShards; ++shard) {
+    for (const auto& it : cplMaps_[shard]) {
+      auto [cpl, func] = it.second;
+      [cpl release];
+      [func release];
+    }
   }
 }
 
 id<MTLLibrary> MetalShaderLibrary::getLibrary() {
-  if (C10_UNLIKELY(!library)) {
+  // THREAD-SAFETY FIX: Use std::call_once for proper thread-safe initialization.
+  // The previous double-checked locking pattern had a race condition:
+  // fast path reads `library` without synchronization while slow path writes it.
+  // std::call_once guarantees that initialization happens exactly once and
+  // is visible to all threads with proper memory ordering.
+  std::call_once(libraryOnceFlag_, [this]() {
     TORCH_INTERNAL_ASSERT(nparams == 0);
     library = compileLibrary(shaderSource);
-  }
+  });
   return library;
 }
 
@@ -819,32 +852,48 @@ id<MTLLibrary> MetalShaderLibrary::getLibrary(const std::initializer_list<std::s
   for (auto p : params) {
     key += ":" + p;
   }
-  auto lib = libMap[key];
-  if (lib) {
-    return lib;
+
+  // 32.27 FIX: Hold shard lock during compilation to avoid double-compile race.
+  // Previously, two threads could both miss the cache, release the lock, compile
+  // the same shader in parallel, and then one would discard its duplicate.
+  // Now we hold the lock during compilation - the 16 shards still provide good
+  // parallelism across different keys (shader variants hash to different shards).
+  const size_t shard_idx = getShardIndex(key);
+  std::lock_guard<std::mutex> lock(cacheMutexes_[shard_idx]);
+
+  // Check cache under lock
+  auto it = libMaps_[shard_idx].find(key);
+  if (it != libMaps_[shard_idx].end()) {
+    return it->second;
   }
-  auto it = params.begin();
+
+  // Compile the library (under lock to prevent duplicate compilation)
+  id<MTLLibrary> lib = nil;
+  auto pit = params.begin();
   switch (nparams) {
     case 1:
-      lib = compileLibrary(fmt::format(fmt::runtime(shaderSource), *it));
+      lib = compileLibrary(fmt::format(fmt::runtime(shaderSource), *pit));
       break;
     case 2: {
-      auto& first = *it++;
-      auto& second = *it;
+      auto& first = *pit++;
+      auto& second = *pit;
       lib = compileLibrary(fmt::format(fmt::runtime(shaderSource), first, second));
       break;
     }
     case 3: {
-      auto& first = *it++;
-      auto& second = *it++;
-      auto& third = *it;
+      auto& first = *pit++;
+      auto& second = *pit++;
+      auto& third = *pit;
       lib = compileLibrary(fmt::format(fmt::runtime(shaderSource), first, second, third));
       break;
     }
     default:
-      TORCH_INTERNAL_ASSERT(false, "Unsupported number of paramaters ", nparams);
+      TORCH_INTERNAL_ASSERT(false, "Unsupported number of parameters ", nparams);
   }
-  return libMap[key] = lib;
+
+  // Store in cache (still under lock, no need to re-check)
+  libMaps_[shard_idx].emplace(key, lib);
+  return lib;
 }
 
 id<MTLLibrary> MetalShaderLibrary::compileLibrary(const std::string& src) {
@@ -870,23 +919,29 @@ id<MTLLibrary> MetalShaderLibrary::compileLibrary(const std::string& src) {
 
   const auto str = [NSString stringWithCString:src.c_str() encoding:NSASCIIStringEncoding];
   auto device = MPSDevice::getInstance()->device();
-  library = [device newLibraryWithSource:str options:options error:&error];
-  if (library == nil) {
+  // Use local variable to avoid race condition when called from getLibrary(params)
+  // The member 'library' is only used by parameterless getLibrary() which assigns the return value
+  id<MTLLibrary> lib = [device newLibraryWithSource:str options:options error:&error];
+  if (lib == nil) {
     if ([error domain] == MTLLibraryErrorDomain && [error code] == MTLLibraryErrorCompileFailure) {
       throw c10::SyntaxError([[error localizedDescription] UTF8String]);
     }
     TORCH_CHECK(false, "Failed to create metal library, error: ", [[error description] UTF8String]);
   }
-  return library;
+  return lib;
 }
 
 std::pair<id<MTLComputePipelineState>, id<MTLFunction>> MetalShaderLibrary::getLibraryPipelineState(
     id<MTLLibrary> lib,
     const std::string& fname) {
   const auto key = fmt::format("{}:{}", reinterpret_cast<void*>(lib), fname);
-  auto found_cpl = cplMap.find(key);
-  if (found_cpl != cplMap.end()) {
-    return found_cpl->second;
+  const size_t shard_idx = getShardIndex(key);  // 33.2: Sharded mutex + per-shard map
+  {
+    std::lock_guard<std::mutex> lock(cacheMutexes_[shard_idx]);
+    auto found_cpl = cplMaps_[shard_idx].find(key);
+    if (found_cpl != cplMaps_[shard_idx].end()) {
+      return found_cpl->second;
+    }
   }
 
   NSError* error = nil;
@@ -895,8 +950,15 @@ std::pair<id<MTLComputePipelineState>, id<MTLFunction>> MetalShaderLibrary::getL
   auto cpl = [[lib device] newComputePipelineStateWithFunction:func error:&error];
   TORCH_CHECK(cpl, "Failed to created pipeline state object, error: ", [[error description] UTF8String]);
 
-  cplMap[key] = std::make_pair(cpl, func);
-  return cplMap[key];
+  {
+    std::lock_guard<std::mutex> lock(cacheMutexes_[shard_idx]);
+    auto emplaced = cplMaps_[shard_idx].emplace(key, std::make_pair(cpl, func));
+    if (!emplaced.second) {
+      [cpl release];
+      [func release];
+    }
+    return emplaced.first->second;
+  }
 }
 
 std::vector<std::string> MetalShaderLibrary::getFunctionNames() {
@@ -925,12 +987,15 @@ class BundledShaderLibary : public MetalShaderLibrary {
 
  protected:
   id<MTLLibrary> getLibrary() override {
-    if (C10_UNLIKELY(!library)) {
+    // THREAD-SAFETY FIX: Use std::call_once for proper thread-safe initialization.
+    // The previous double-checked locking pattern had a race condition:
+    // multiple threads could see !library and race to initialize it.
+    std::call_once(bundledLibraryOnceFlag_, [this]() {
       auto device = MPSDevice::getInstance()->device();
       NSError* error = nil;
       library = [device newLibraryWithData:getSectionData("metal_basic") error:&error];
       TORCH_CHECK(library, "Failed to create metal library, error: ", [[error description] UTF8String]);
-    }
+    });
     return library;
   }
 
@@ -939,6 +1004,7 @@ class BundledShaderLibary : public MetalShaderLibrary {
   }
 
  private:
+  std::once_flag bundledLibraryOnceFlag_;
   static dispatch_data_t getSectionData(const std::string& name) {
     uint32_t idx = 0;
     for (const auto cnt : c10::irange(_dyld_image_count())) {
@@ -991,27 +1057,35 @@ void MetalShaderLibrary::exec_unary_kernel(TensorIteratorBase& iter,
     auto cplState = getPipelineStateForFunc(kernel_name);
 
     MPSStream* mpsStream = getCurrentMPSStream();
-    dispatch_sync(mpsStream->queue(), ^() {
-      auto computeEncoder = mpsStream->commandEncoder();
-
-      getMPSProfiler().beginProfileKernel(cplState, name, {inputTensor});
+    dispatch_block_t dispatch_block = ^() {
+      @autoreleasepool {
+        auto computeEncoder = mpsStream->commandEncoder();
+
+        getMPSProfiler().beginProfileKernel(cplState, name, {inputTensor});
+
+        [computeEncoder setComputePipelineState:cplState];
+        bind_iter_tensors(computeEncoder, iter);
+        if (!iter.is_contiguous()) {
+          mtl_setArgs<2>(computeEncoder,
+                         outputTensor.sizes(),
+                         inputTensor.strides(),
+                         outputTensor.strides(),
+                         inputTensor.ndimension());
+        }
+        if (alpha) {
+          mtl_setBytes(computeEncoder, getMPSScalar(*alpha, alpha_type), iter.is_contiguous() ? 2 : 6);
+        }
+        mtl_dispatch1DJob(computeEncoder, cplState, length);
 
-      [computeEncoder setComputePipelineState:cplState];
-      bind_iter_tensors(computeEncoder, iter);
-      if (!iter.is_contiguous()) {
-        mtl_setArgs<2>(computeEncoder,
-                       outputTensor.sizes(),
-                       inputTensor.strides(),
-                       outputTensor.strides(),
-                       inputTensor.ndimension());
-      }
-      if (alpha) {
-        mtl_setBytes(computeEncoder, getMPSScalar(*alpha, alpha_type), iter.is_contiguous() ? 2 : 6);
+        getMPSProfiler().endProfileKernel(cplState);
       }
-      mtl_dispatch1DJob(computeEncoder, cplState, length);
+    };
 
-      getMPSProfiler().endProfileKernel(cplState);
-    });
+    if (dispatch_get_specific(at::mps::getMPSStreamQueueSpecificKey()) == static_cast<void*>(mpsStream)) {
+      dispatch_block();
+    } else {
+      dispatch_sync_with_rethrow(mpsStream->queue(), dispatch_block);
+    }
   }
 }
 
@@ -1134,7 +1208,19 @@ MetalKernelFunction::~MetalKernelFunction() {
 }
 
 void MetalKernelFunction::runCommandBlock(std::function<void(void)> run) {
-  dispatch_sync_with_rethrow(getCurrentMPSStream()->queue(), ^() {
+  // Capture stream BEFORE dispatch_sync to avoid TLS hazard.
+  // GCD may run the block on a different thread with different TLS.
+  current_stream_ = getCurrentMPSStream();
+  // 32.102 fix: Check for nullptr - getCurrentMPSStream() returns nullptr during
+  // static destruction when g_pool_alive is false, or if pool not yet initialized.
+  // Without this check, current_stream_->queue() below would dereference nullptr.
+  TORCH_CHECK(current_stream_ != nullptr,
+              "MPS stream pool not available. Cannot execute Metal kernel.");
+  // Use RAII to ensure current_stream_ is cleared even if run() throws.
+  // Without this, dispatch_sync_with_rethrow would rethrow the exception
+  // and leave current_stream_ in a stale non-null state.
+  auto cleanup = c10::make_scope_exit([this] { current_stream_ = nullptr; });
+  dispatch_sync_with_rethrow(current_stream_->queue(), ^() {
     @autoreleasepool {
       run();
     }
@@ -1142,7 +1228,10 @@ void MetalKernelFunction::runCommandBlock(std::function<void(void)> run) {
 }
 
 void MetalKernelFunction::startEncoding() {
-  encoder = getCurrentMPSStream()->commandEncoder();
+  // Use captured stream, not TLS lookup (TLS hazard: GCD may run on different thread)
+  TORCH_CHECK(current_stream_ != nullptr,
+              "startEncoding() must be called from within runCommandBlock()");
+  encoder = current_stream_->commandEncoder();
   [encoder setComputePipelineState:cps];
 }
 
diff --git a/aten/src/ATen/native/mps/operations/BitwiseOps.mm b/aten/src/ATen/native/mps/operations/BitwiseOps.mm
index cc802bce..fdf9a6a0 100644
--- a/aten/src/ATen/native/mps/operations/BitwiseOps.mm
+++ b/aten/src/ATen/native/mps/operations/BitwiseOps.mm
@@ -127,7 +127,7 @@ static void handle_binary_op(const Tensor& self, const Tensor& other, Tensor& ou
     return;
   }
 
-  dispatch_sync(stream->queue(), ^() {
+  dispatch_sync_with_rethrow(stream->queue(), ^() {
     // this function call is a no-op if MPS Profiler is not enabled
     getMPSProfiler().beginProfileKernel(cplState, kernel_name, {self, other});
 
diff --git a/aten/src/ATen/native/mps/operations/Distributions.mm b/aten/src/ATen/native/mps/operations/Distributions.mm
index 4d3f99ea..69b661c8 100644
--- a/aten/src/ATen/native/mps/operations/Distributions.mm
+++ b/aten/src/ATen/native/mps/operations/Distributions.mm
@@ -128,6 +128,13 @@ Tensor& random_mps_impl(Tensor& self,
         newCachedGraph->resultTensor = castMPSTensor(mpsGraph, newCachedGraph->resultTensor, self.scalar_type());
     });
     // feed the updated state values to the graph
+    // THREAD SAFETY NOTE (23.19): Generator state handling is safe because:
+    // 1. Each execution creates its own MPSNDArray that receives a snapshot of state
+    // 2. The generator mutex serializes state updates across threads
+    // 3. writeBytes copies data to the NDArray, so concurrent graph executions
+    //    on different streams each have their own independent state copy
+    // 4. Even if thread B updates generator state while thread A's graph is running,
+    //    thread A's MPSNDArray already contains its snapshot and is unaffected
     MPSNDArrayDescriptor* stateDesc =
         [MPSNDArrayDescriptor descriptorWithDataType:MPSDataTypeInt32 shape:@[ @(at::mps::detail::PHILOX_STATE_N) ]];
     MPSNDArray* stateNDArray = [[[MPSNDArray alloc] initWithDevice:stream->device() descriptor:stateDesc] autorelease];
@@ -564,6 +571,8 @@ static Tensor& multinomial_with_replacement_mps_kernel(const Tensor& self,
                                                      name:@"resultTensor"];
     });
     // update the Philox state values on each run of the same graph
+    // THREAD SAFETY NOTE (23.19): See comment at first occurrence in random_mps_impl()
+    // for explanation of why generator state handling is thread-safe
     MPSNDArrayDescriptor* stateDesc =
         [MPSNDArrayDescriptor descriptorWithDataType:MPSDataTypeInt32 shape:@[ @(at::mps::detail::PHILOX_STATE_N) ]];
     MPSNDArray* stateNDArray = [[[MPSNDArray alloc] initWithDevice:stream->device() descriptor:stateDesc] autorelease];
diff --git a/aten/src/ATen/native/mps/operations/Gamma.mm b/aten/src/ATen/native/mps/operations/Gamma.mm
index 9feb5eba..565f2c6d 100644
--- a/aten/src/ATen/native/mps/operations/Gamma.mm
+++ b/aten/src/ATen/native/mps/operations/Gamma.mm
@@ -45,7 +45,7 @@ TORCH_IMPL_FUNC(lgamma_out_mps)(const Tensor& self, const Tensor& output_) {
     auto cplState = getCPLState(self, output, "lgamma");
 
     MPSStream* mpsStream = getCurrentMPSStream();
-    dispatch_sync(mpsStream->queue(), ^() {
+    dispatch_sync_with_rethrow(mpsStream->queue(), ^() {
       id<MTLComputeCommandEncoder> computeEncoder = mpsStream->commandEncoder();
 
       getMPSProfiler().beginProfileKernel(cplState, "lgamma_out", {self});
@@ -83,7 +83,7 @@ TORCH_IMPL_FUNC(digamma_out_mps)(const Tensor& self, const Tensor& output_) {
     id<MTLComputePipelineState> cplState = getCPLState(self, output, "digamma");
 
     MPSStream* mpsStream = getCurrentMPSStream();
-    dispatch_sync(mpsStream->queue(), ^() {
+    dispatch_sync_with_rethrow(mpsStream->queue(), ^() {
       id<MTLComputeCommandEncoder> computeEncoder = mpsStream->commandEncoder();
 
       getMPSProfiler().beginProfileKernel(cplState, "digamma_out", {self});
@@ -133,7 +133,7 @@ TORCH_IMPL_FUNC(polygamma_out_mps)(const int64_t order, const Tensor& self, cons
     id<MTLComputePipelineState> cplState = getCPLState(self, output, func_name);
 
     MPSStream* mpsStream = getCurrentMPSStream();
-    dispatch_sync(mpsStream->queue(), ^() {
+    dispatch_sync_with_rethrow(mpsStream->queue(), ^() {
       id<MTLComputeCommandEncoder> computeEncoder = mpsStream->commandEncoder();
 
       getMPSProfiler().beginProfileKernel(cplState, func_name, {self});
diff --git a/aten/src/ATen/native/mps/operations/Indexing.mm b/aten/src/ATen/native/mps/operations/Indexing.mm
index fa19d2f4..49db8bd7 100644
--- a/aten/src/ATen/native/mps/operations/Indexing.mm
+++ b/aten/src/ATen/native/mps/operations/Indexing.mm
@@ -300,9 +300,10 @@ static Tensor& nonzero_out_native_mps(const Tensor& self, Tensor& out_) {
   MPSStream* stream = getCurrentMPSStream();
   using CachedGraph = MPSUnaryCachedGraph;
 
-  dispatch_sync(stream->queue(), ^() {
-    stream->synchronize(SyncType::COMMIT_AND_WAIT);
-  });
+  // THREAD-SAFETY (23.17): Call synchronize() directly without dispatch_sync wrapper.
+  // dispatch_sync + synchronize(COMMIT_AND_WAIT) can cause deadlock by holding
+  // the queue blocked while waiting for GPU work to complete.
+  stream->synchronize(SyncType::COMMIT_AND_WAIT);
   int64_t total_nonzero = at::count_nonzero(self).item<int64_t>();
   at::native::resize_output(out_, {total_nonzero, nDim});
   if (out_.numel() == 0) {
@@ -385,9 +386,10 @@ Tensor& nonzero_out_mps(const Tensor& self, Tensor& out_) {
   MPSStream* stream = getCurrentMPSStream();
   using CachedGraph = MPSUnaryCachedGraph;
 
-  dispatch_sync(stream->queue(), ^() {
-    stream->synchronize(SyncType::COMMIT_AND_WAIT);
-  });
+  // THREAD-SAFETY (23.17): Call synchronize() directly without dispatch_sync wrapper.
+  // dispatch_sync + synchronize(COMMIT_AND_WAIT) can cause deadlock by holding
+  // the queue blocked while waiting for GPU work to complete.
+  stream->synchronize(SyncType::COMMIT_AND_WAIT);
   int64_t total_nonzero = at::count_nonzero(self).item<int64_t>();
   at::native::resize_output(out_, {total_nonzero, nDim});
   if (out_.numel() == 0) {
diff --git a/aten/src/ATen/native/mps/operations/Linear.mm b/aten/src/ATen/native/mps/operations/Linear.mm
index 219086ed..4b8c2688 100644
--- a/aten/src/ATen/native/mps/operations/Linear.mm
+++ b/aten/src/ATen/native/mps/operations/Linear.mm
@@ -6,18 +6,41 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/ops/linear_backward_native.h>
 #include <ATen/ops/linear_native.h>
+#include <mutex>
 
 namespace at::native {
 
 using namespace mps;
 
+// THREAD-SAFETY: Global mutex for MPSNDArrayMatrixMultiplication encoding.
+// Apple's MPS framework has internal shared state that makes concurrent encoding
+// of MPSNDArrayMatrixMultiplication kernels unsafe, even with per-thread instances.
+// This mutex serializes the no-graph linear path to prevent crashes.
+static std::mutex s_linear_nograph_mutex;
+
 static void _mps_linear_nograph(const Tensor& input, const Tensor& weight, const Tensor& bias, Tensor& output) {
   bool is_bias_defined = bias.defined();
 
   MPSStream* mpsStream = getCurrentMPSStream();
   id<MTLDevice> device = MPSDevice::getInstance()->device();
 
+  // Build cache key and look up kernel on calling thread (thread_local cache)
+  // This MUST happen before dispatch_sync since the cache is thread_local
   const std::string key = "mps_linear" + getTensorsStringKey({input, weight, bias}, true, true);
+
+  // Get or create kernel on calling thread
+  MPSCachedKernel* cachedKernel;
+  if (is_bias_defined) {
+    cachedKernel = LookUpOrCreateCachedKernel<MPSCachedKernel>(key, [&]() {
+      return [[[MPSNDArrayMatrixMultiplication alloc] initWithDevice:device sourceCount:3] autorelease];
+    });
+  } else {
+    cachedKernel = LookUpOrCreateCachedKernel<MPSCachedKernel>(key, [&]() {
+      return [[[MPSNDArrayMatrixMultiplication alloc] initWithDevice:device sourceCount:2] autorelease];
+    });
+  }
+  auto kernel = cachedKernel->kernel<MPSNDArrayMatrixMultiplication>();
+
   dispatch_sync_with_rethrow(mpsStream->queue(), ^() {
     @autoreleasepool {
       mpsStream->endKernelCoalescing();
@@ -39,13 +62,13 @@ static void _mps_linear_nograph(const Tensor& input, const Tensor& weight, const
                                                                offset:weight.storage_offset() * weight.element_size()
                                                            descriptor:weightDesc] autorelease];
 
+      // THREAD-SAFETY: Serialize only the kernel encoding.
+      // Apple's MPSNDArrayMatrixMultiplication has internal shared state that is not thread-safe.
+      // We minimize the critical section to just the encoding call.
+      std::lock_guard<std::mutex> lock(s_linear_nograph_mutex);
+
       if (is_bias_defined) {
         auto biasNDArray = getMPSNDArray(bias, bias.sizes(), bias.strides());
-        auto cachedKernel = LookUpOrCreateCachedKernel<MPSCachedKernel>(key, [&]() {
-          return [[[MPSNDArrayMatrixMultiplication alloc] initWithDevice:device sourceCount:3] autorelease];
-        });
-        auto kernel = cachedKernel->kernel<MPSNDArrayMatrixMultiplication>();
-
         getMPSProfiler().beginProfileKernel(kernel, "mps_linear", {input, weight, bias});
         [kernel encodeToCommandEncoder:computeEncoder
                          commandBuffer:commandBuffer
@@ -53,10 +76,6 @@ static void _mps_linear_nograph(const Tensor& input, const Tensor& weight, const
                       destinationArray:outNDArray];
         getMPSProfiler().endProfileKernel(kernel);
       } else {
-        auto cachedKernel = LookUpOrCreateCachedKernel<MPSCachedKernel>(key, [&]() {
-          return [[[MPSNDArrayMatrixMultiplication alloc] initWithDevice:device sourceCount:2] autorelease];
-        });
-        auto kernel = cachedKernel->kernel<MPSNDArrayMatrixMultiplication>();
         getMPSProfiler().beginProfileKernel(kernel, "mps_linear", {input, weight, bias});
         [kernel encodeToCommandEncoder:computeEncoder
                          commandBuffer:commandBuffer
@@ -118,7 +137,23 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const std::opt
   // No-graph execution causes nonsense if these are non-contiguous.
   const bool is_contiguous = input.is_contiguous() && weight.is_contiguous() && bias.is_contiguous();
 
-  if (is_macos_13_or_newer(MacOSVersion::MACOS_VER_15_0_PLUS) && is_contiguous) {
+  // THREAD-SAFETY: Prefer the graph path when parallel streams are active.
+  // The no-graph path requires a global mutex due to Apple's internal thread-safety issues.
+  // Set MPS_FORCE_GRAPH_PATH=1 to always use the graph path.
+  // Trade-off: Graph path has compilation overhead but better parallelism.
+  static const bool force_graph_path_env = []() {
+    auto val = c10::utils::get_env("MPS_FORCE_GRAPH_PATH");
+    return val.has_value() && val.value() == "1";
+  }();
+
+  // 32.82 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed and accessing instance() would cause undefined behavior.
+  // If pool is not alive, assume single-threaded (parallel_streams_active = false).
+  const bool parallel_streams_active = at::mps::MPSStreamPool::isPoolAlive() &&
+                                       at::mps::MPSStreamPool::instance().getActiveStreamCount() > 1;
+  const bool force_graph_path = force_graph_path_env || parallel_streams_active;
+
+  if (!force_graph_path && is_macos_13_or_newer(MacOSVersion::MACOS_VER_15_0_PLUS) && is_contiguous) {
     _mps_linear_nograph(input, weight, bias, output);
     // Squeeze last dim of 1D linear
     return weight_arg.dim() != 1 ? output : output.squeeze(-1);
diff --git a/aten/src/ATen/native/mps/operations/LinearAlgebra.mm b/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
index 7a3dde67..173e4e89 100644
--- a/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
+++ b/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
@@ -2,6 +2,7 @@
 
 #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
 #include <ATen/mps/MPSProfiler.h>
+#include <mutex>
 #include <ATen/native/BatchLinearAlgebra.h>
 #include <ATen/native/LinearAlgebra.h>
 #include <ATen/native/LinearAlgebraUtils.h>
@@ -40,6 +41,16 @@
 namespace at::native {
 namespace mps {
 namespace {
+
+// THREAD-SAFETY: Global mutexes for MPS matrix operations encoding.
+// Apple's MPS framework has internal shared state that makes concurrent encoding
+// of certain MPS kernel types unsafe, even with per-thread instances.
+// These mutexes serialize the encoding paths to prevent crashes.
+static std::mutex s_bmm_tiled_mutex;           // MPSNDArrayMatrixMultiplication
+static std::mutex s_lu_decomposition_mutex;    // MPSMatrixDecompositionLU
+static std::mutex s_lu_solve_mutex;            // MPSMatrixSolveLU
+static std::mutex s_solve_triangular_mutex;    // MPSMatrixSolveTriangular
+
 #ifndef PYTORCH_JIT_COMPILE_SHADERS
 static auto& lib = MetalShaderLibrary::getBundledLibrary();
 #else
@@ -274,6 +285,10 @@ static void linalg_lu_factor_ex_out_mps_impl(const Tensor& A,
                                                                                 matrixBytes:numPivots * sizeof(uint32_t)
                                                                                    dataType:MPSDataTypeUInt32];
 
+      // THREAD-SAFETY: Serialize MPSMatrixDecompositionLU encoding.
+      // Apple's MPS framework may have internal shared state.
+      std::lock_guard<std::mutex> lock(s_lu_decomposition_mutex);
+
       for (const auto i : c10::irange(batchSize)) {
         const uint64_t aBatchOffset = i * aRows * aCols;
         MPSMatrix* sourceMatrix = [[[MPSMatrix alloc] initWithBuffer:aBuffer
@@ -439,6 +454,10 @@ static void linalg_solve_out_mps_impl(const Tensor& A,
                                                                                 matrixBytes:numPivots * sizeof(uint32_t)
                                                                                    dataType:MPSDataTypeUInt32];
 
+      // THREAD-SAFETY: Serialize MPSMatrixDecompositionLU + MPSMatrixSolveLU encoding.
+      // Apple's MPS framework may have internal shared state.
+      std::lock_guard<std::mutex> lock(s_lu_solve_mutex);
+
       for (const auto i : c10::irange(batchSize)) {
         const uint64_t batchOffsetA = i * aRows * aCols;
         const uint64_t batchOffsetB = i * aRows * numberOfRightHandSides;
@@ -883,6 +902,12 @@ static Tensor& tiled_bmm_out_mps_impl(const Tensor& batch1, const Tensor& batch2
         auto aDesc = aDesc_;
         auto bDesc = bDesc_;
         auto resDesc = resDesc_;
+
+        // THREAD-SAFETY: Serialize only the kernel encoding.
+        // Apple's MPSNDArrayMatrixMultiplication has internal shared state that is not thread-safe.
+        // We minimize the critical section to just the encoding calls.
+        std::lock_guard<std::mutex> lock(s_bmm_tiled_mutex);
+
         for (const auto i : c10::irange(requiredIterations)) {
           if (i == requiredIterations - 1 && lastBatchSize != 0) {
             aDesc = aDescLastBatch_;
@@ -1082,6 +1107,11 @@ static Tensor& linalg_solve_triangular_mps_impl(const Tensor& A,
                                                rowBytes:bCols * bElemSize
                                             matrixBytes:bRows * bCols * bElemSize
                                                dataType:getMPSDataType(B_)];
+
+      // THREAD-SAFETY: Serialize MPSMatrixSolveTriangular encoding.
+      // Apple's MPS framework may have internal shared state.
+      std::lock_guard<std::mutex> lock(s_solve_triangular_mutex);
+
       for (const auto i : c10::irange(batchSize)) {
         const uint64_t aBatchOffset = i * aRows * aCols;
         const uint64_t bBatchOffset = i * bRows * bCols;
diff --git a/aten/src/ATen/native/mps/operations/MultiTensorApply.h b/aten/src/ATen/native/mps/operations/MultiTensorApply.h
index 71575189..a3944be1 100644
--- a/aten/src/ATen/native/mps/operations/MultiTensorApply.h
+++ b/aten/src/ATen/native/mps/operations/MultiTensorApply.h
@@ -163,6 +163,8 @@ static void multi_tensor_apply_for_fused_optimizer(const std::string& kernel_nam
       auto tensorArgumentEncoder = [[fusedOptimizerFunc newArgumentEncoderWithBufferIndex:0] autorelease];
       id<MTLBuffer> tensorArgumentBuffer = [[device newBufferWithLength:tensorArgumentEncoder.encodedLength
                                                                 options:0] autorelease];
+      TORCH_CHECK(tensorArgumentBuffer != nil,
+                  "MPS: Failed to allocate argument buffer of size ", tensorArgumentEncoder.encodedLength);
       [tensorArgumentEncoder setArgumentBuffer:tensorArgumentBuffer offset:0];
 
       int64_t tensor_loc = 0;
@@ -216,6 +218,8 @@ static void multi_tensor_apply_for_fused_optimizer(const std::string& kernel_nam
               tensor_loc = 0;
               tensorArgumentBuffer = [[device newBufferWithLength:tensorArgumentEncoder.encodedLength
                                                           options:0] autorelease];
+              TORCH_CHECK(tensorArgumentBuffer != nil,
+                          "MPS: Failed to allocate argument buffer of size ", tensorArgumentEncoder.encodedLength);
               [tensorArgumentEncoder setArgumentBuffer:tensorArgumentBuffer offset:0];
             } else {
               // reuse the current tensor since the current one isn't done.
@@ -223,6 +227,8 @@ static void multi_tensor_apply_for_fused_optimizer(const std::string& kernel_nam
 
               tensorArgumentBuffer = [[device newBufferWithLength:tensorArgumentEncoder.encodedLength
                                                           options:0] autorelease];
+              TORCH_CHECK(tensorArgumentBuffer != nil,
+                          "MPS: Failed to allocate argument buffer of size ", tensorArgumentEncoder.encodedLength);
               [tensorArgumentEncoder setArgumentBuffer:tensorArgumentBuffer offset:0];
 
               for (const auto& d : c10::irange(depth)) {
@@ -276,6 +282,8 @@ void multi_tensor_apply(const std::string& kernel_name,
 
       id<MTLArgumentEncoder> argumentEncoder = [function newArgumentEncoderWithBufferIndex:0];
       auto tensorArgumentBuffer = [[device newBufferWithLength:argumentEncoder.encodedLength options:0] autorelease];
+      TORCH_CHECK(tensorArgumentBuffer != nil,
+                  "MPS: Failed to allocate argument buffer of size ", argumentEncoder.encodedLength);
       [argumentEncoder setArgumentBuffer:tensorArgumentBuffer offset:0];
 
       int tensor_loc = 0;
@@ -327,6 +335,8 @@ void multi_tensor_apply(const std::string& kernel_name,
             // prepare for the next batch: reset threadgroup count and create a new buffer
             threadgroup_loc = 0;
             tensorArgumentBuffer = [[device newBufferWithLength:argumentEncoder.encodedLength options:0] autorelease];
+            TORCH_CHECK(tensorArgumentBuffer != nil,
+                        "MPS: Failed to allocate argument buffer of size ", argumentEncoder.encodedLength);
             [argumentEncoder setArgumentBuffer:tensorArgumentBuffer offset:0];
 
             if (partial) {
diff --git a/aten/src/ATen/native/mps/operations/Normalization.mm b/aten/src/ATen/native/mps/operations/Normalization.mm
index f5264cf3..be1b2774 100644
--- a/aten/src/ATen/native/mps/operations/Normalization.mm
+++ b/aten/src/ATen/native/mps/operations/Normalization.mm
@@ -5,6 +5,7 @@
 #include <ATen/native/Pool.h>
 #include <ATen/native/layer_norm.h>
 #include <ATen/native/mps/OperationUtils.h>
+#include <mutex>
 
 #ifndef AT_PER_OPERATOR_HEADERS
 #include <ATen/Functions.h>
@@ -23,6 +24,12 @@
 namespace at::native {
 namespace mps {
 
+// THREAD-SAFETY: Global mutex for LayerNorm Metal compute kernel encoding.
+// Apple's Metal framework has internal shared state that makes concurrent
+// encoding of LayerNorm kernels unsafe, causing ~30% crash rate at 4+ threads.
+// This mutex serializes the encoding path to prevent crashes.
+static std::mutex s_layer_norm_mutex;
+
 #ifndef PYTORCH_JIT_COMPILE_SHADERS
 static auto& lib = MetalShaderLibrary::getBundledLibrary();
 #else
@@ -886,11 +893,35 @@ std::tuple<Tensor, Tensor, Tensor> batch_norm_backward_mps(const Tensor& grad_ou
 }
 
 // Layer norm forward for MPS
+// THREAD-SAFETY WARNING: This function uses Metal compute kernels via
+// getPipelineStateForFunc() which have thread-safety issues at 4+ threads.
+// Unlike Linear.mm which has a graph-based fallback, LayerNorm only has the
+// Metal kernel path. Use multi-process parallelism for LayerNorm-heavy workloads.
+// Set MPS_WARN_LAYERNORM_PARALLEL=0 to suppress this warning.
 std::tuple<Tensor, Tensor, Tensor> layer_norm_mps(const Tensor& input,
                                                   IntArrayRef normalized_shape,
                                                   const std::optional<Tensor>& weight_opt,
                                                   const std::optional<Tensor>& bias_opt,
                                                   double eps) {
+  // THREAD-SAFETY: Warn when parallel streams are active.
+  // LayerNorm kernel encoding is serialized via s_layer_norm_mutex to prevent crashes
+  // from Apple Metal framework thread-safety issues. This serialization is safe but
+  // may reduce multi-threaded performance.
+  static const bool warn_parallel = []() {
+    auto val = c10::utils::get_env("MPS_WARN_LAYERNORM_PARALLEL");
+    // Default to warning (1), set to 0 to suppress
+    return !val.has_value() || val.value() != "0";
+  }();
+  // 32.82 fix: Check if pool is alive before accessing. During static destruction,
+  // the pool may be destroyed and accessing instance() would cause undefined behavior.
+  if (warn_parallel && at::mps::MPSStreamPool::isPoolAlive() &&
+      at::mps::MPSStreamPool::instance().getActiveStreamCount() > 1) {
+    TORCH_WARN_ONCE(
+        "LayerNorm is serialized across threads for MPS thread-safety. This may reduce "
+        "multi-threaded performance. Consider using multi-process parallelism for "
+        "LayerNorm-heavy models. Set MPS_WARN_LAYERNORM_PARALLEL=0 to suppress.");
+  }
+
   auto N = c10::multiply_integers(normalized_shape);
   auto out = at::empty_like(input, MemoryFormat::Contiguous);
   auto batch_dim = input.dim() - normalized_shape.size();
@@ -918,6 +949,9 @@ std::tuple<Tensor, Tensor, Tensor> layer_norm_mps(const Tensor& input,
   // NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)
   const int axis = input_ndim - normalized_ndim;
   MPSStream* stream = getCurrentMPSStream();
+  // THREAD-SAFETY: Serialize LayerNorm kernel encoding to prevent crashes at 4+ threads.
+  // Apple's Metal compute kernels have internal shared state issues.
+  std::lock_guard<std::mutex> lock(mps::s_layer_norm_mutex);
   @autoreleasepool {
     mps::dispatch_sync_with_rethrow(stream->queue(), ^() {
       // which kernel variant to use based on the normalized axis N size
diff --git a/aten/src/ATen/native/mps/operations/RenormKernel.mm b/aten/src/ATen/native/mps/operations/RenormKernel.mm
index 8e787f6f..daa5035b 100644
--- a/aten/src/ATen/native/mps/operations/RenormKernel.mm
+++ b/aten/src/ATen/native/mps/operations/RenormKernel.mm
@@ -43,7 +43,7 @@ void renorm_out_mps(const Tensor& self, const Scalar& p, int64_t dim, const Scal
   id<MTLComputeCommandEncoder> computeEncoder = mpsStream->commandEncoder();
   id<MTLComputePipelineState> renormPSO = lib.getPipelineStateForFunc(key);
 
-  dispatch_sync(mpsStream->queue(), ^() {
+  dispatch_sync_with_rethrow(mpsStream->queue(), ^() {
     @autoreleasepool {
       // this function call is a no-op if MPSProfiler is not enabled
       getMPSProfiler().beginProfileKernel(renormPSO, key, {norm});
diff --git a/aten/src/ATen/native/mps/operations/Repeat.mm b/aten/src/ATen/native/mps/operations/Repeat.mm
index 40afa15b..2ceb6359 100644
--- a/aten/src/ATen/native/mps/operations/Repeat.mm
+++ b/aten/src/ATen/native/mps/operations/Repeat.mm
@@ -97,10 +97,13 @@ void computeRepeatIndices(const index_t* repeat_ptr,
                           index_t* result_ptr,
                           int64_t size,
                           int64_t result_size) {
-  id<MTLBuffer> repeatBuffer = reinterpret_cast<id<MTLBuffer>>(repeat_ptr);
-  id<MTLBuffer> cumsumBuffer = reinterpret_cast<id<MTLBuffer>>(cumsum_ptr);
-  id<MTLBuffer> resultBuffer = reinterpret_cast<id<MTLBuffer>>(result_ptr);
-  TORCH_CHECK(repeatBuffer && cumsumBuffer && resultBuffer);
+  // THREAD-SAFETY: These pointers come from MPS tensor storage (guaranteed MTLBuffer).
+  // Use __bridge cast for proper Objective-C bridging from raw pointers.
+  id<MTLBuffer> repeatBuffer = (__bridge id<MTLBuffer>)(const_cast<index_t*>(repeat_ptr));
+  id<MTLBuffer> cumsumBuffer = (__bridge id<MTLBuffer>)(const_cast<int64_t*>(cumsum_ptr));
+  id<MTLBuffer> resultBuffer = (__bridge id<MTLBuffer>)(result_ptr);
+  TORCH_CHECK(repeatBuffer && cumsumBuffer && resultBuffer,
+              "repeat_interleave: failed to get MTLBuffer from MPS tensor storage");
 
   std::string scalar_type;
   if constexpr (std::is_same_v<index_t, int32_t>) {
@@ -112,7 +115,7 @@ void computeRepeatIndices(const index_t* repeat_ptr,
   }
 
   MPSStream* mpsStream = getCurrentMPSStream();
-  dispatch_sync(mpsStream->queue(), ^() {
+  mps::dispatch_sync_with_rethrow(mpsStream->queue(), ^() {
     @autoreleasepool {
       auto computeEncoder = mpsStream->commandEncoder();
       auto pipelineState = lib.getPipelineStateForFunc(fmt::format("repeat_interleave_{}", scalar_type));
diff --git a/aten/src/ATen/native/mps/operations/RnnOps.mm b/aten/src/ATen/native/mps/operations/RnnOps.mm
index d72ead5a..4413fbac 100644
--- a/aten/src/ATen/native/mps/operations/RnnOps.mm
+++ b/aten/src/ATen/native/mps/operations/RnnOps.mm
@@ -176,10 +176,10 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> _lstm_mps(const Tenso
 
       MPSGraphTensor* inputTensor_ = inputTensor;
       NSArray<MPSGraphTensor*>* outputs = nil;
-      NSMutableArray<MPSGraphTensor*>* outputStateArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* outputCellStateArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* outputZStateArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* outputCellStateFwdArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
+      NSMutableArray<MPSGraphTensor*>* outputStateArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* outputCellStateArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* outputZStateArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* outputCellStateFwdArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
       for (int i = 0; i < num_layers; i++) {
         auto tensorsData = getMPSTensorsFromPytorchTensors(mpsGraph,
                                                            stateTensor,
@@ -477,11 +477,11 @@ std::tuple<Tensor, std::vector<Tensor>, std::vector<Tensor>> lstm_mps_backward(c
 
       NSArray<MPSGraphTensor*>* outputs = nil;
 
-      NSMutableArray<MPSGraphTensor*>* gradRecWeightsArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* gradWeightsArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* gradBiasArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* gradStateArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
-      NSMutableArray<MPSGraphTensor*>* gradCellStateArray = [[NSMutableArray alloc] initWithCapacity:num_layers];
+      NSMutableArray<MPSGraphTensor*>* gradRecWeightsArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* gradWeightsArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* gradBiasArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* gradStateArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
+      NSMutableArray<MPSGraphTensor*>* gradCellStateArray = [[[NSMutableArray alloc] initWithCapacity:num_layers] autorelease];
 
       for (int i = num_layers - 1; i >= 0; i--) {
         MPSGraphTensor* zState = [mpsGraph sliceTensor:zStateTensor dimension:0 start:i length:1 name:nil];
diff --git a/aten/src/ATen/native/mps/operations/ScatterGather.mm b/aten/src/ATen/native/mps/operations/ScatterGather.mm
index ce65421c..60a90f77 100644
--- a/aten/src/ATen/native/mps/operations/ScatterGather.mm
+++ b/aten/src/ATen/native/mps/operations/ScatterGather.mm
@@ -94,7 +94,7 @@ TORCH_IMPL_FUNC(gather_out_mps)
       if (workaroundSingleDim and !isMacos15_2) {
         const int64_t dims = self_arg.sizes().size();
         int64_t size = self_arg.squeeze().sizes()[0];
-        auto shape = [[NSMutableArray alloc] initWithCapacity:dims];
+        auto shape = [[[NSMutableArray alloc] initWithCapacity:dims] autorelease];
         for (int i = 0; i < dims; ++i) {
           [shape addObject:[NSNumber numberWithInt:size]];
         }
diff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml
index abb061af..361071ef 100644
--- a/aten/src/ATen/native/native_functions.yaml
+++ b/aten/src/ATen/native/native_functions.yaml
@@ -13284,6 +13284,7 @@
   variants: method
   dispatch:
     CUDA: record_stream_cuda
+    MPS: record_stream_mps
 
 - func: isposinf(Tensor self) -> Tensor
   variants: function, method
diff --git a/torch/csrc/mps/Module.cpp b/torch/csrc/mps/Module.cpp
index 51c77aba..cec596cd 100644
--- a/torch/csrc/mps/Module.cpp
+++ b/torch/csrc/mps/Module.cpp
@@ -70,6 +70,15 @@ static PyObject* MPSModule_deviceSynchronize(
   END_HANDLE_TH_ERRORS
 }
 
+static PyObject* MPSModule_releaseCurrentThreadSlot(
+    PyObject* _unused,
+    PyObject* noargs) {
+  HANDLE_TH_ERRORS
+  at::detail::getMPSHooks().releaseCurrentThreadSlot();
+  Py_RETURN_NONE;
+  END_HANDLE_TH_ERRORS
+}
+
 static PyObject* MPSModule_emptyCache(PyObject* _unused, PyObject* noargs) {
   HANDLE_TH_ERRORS
   at::detail::getMPSHooks().emptyCache();
@@ -217,6 +226,10 @@ static struct PyMethodDef _MPSModule_methods[] = {
      MPSModule_deviceSynchronize,
      METH_NOARGS,
      nullptr},
+    {"_mps_releaseCurrentThreadSlot",
+     MPSModule_releaseCurrentThreadSlot,
+     METH_NOARGS,
+     nullptr},
     {"_mps_is_in_bad_fork", MPSModule_isInBadFork, METH_NOARGS, nullptr},
     {"_mps_is_available", MPSModule_isAvailable, METH_NOARGS, nullptr},
     {"_mps_is_on_macos_or_newer",
diff --git a/torch/mps/__init__.py b/torch/mps/__init__.py
index cdbf6b16..50b4d71f 100644
--- a/torch/mps/__init__.py
+++ b/torch/mps/__init__.py
@@ -34,6 +34,35 @@ def synchronize() -> None:
     return torch._C._mps_deviceSynchronize()
 
 
+def release_current_thread_slot() -> None:
+    r"""Clears the current thread's cached MPS stream pointer.
+
+    PyTorch's MPS backend uses a stream pool for parallel inference. Each thread
+    caches a selected stream pointer in thread-local storage. Calling this
+    function clears that cache so the next MPS operation on this thread
+    reselects a stream from the pool.
+
+    Example::
+
+        import torch
+        import threading
+
+        def worker():
+            x = torch.randn(100, device='mps')
+            torch.mps.synchronize()
+            torch.mps.release_current_thread_slot()  # Clear before exit
+
+        threads = [threading.Thread(target=worker) for _ in range(40)]
+        for t in threads: t.start()
+        for t in threads: t.join()
+
+    Note:
+        Safe to call multiple times; subsequent calls are no-ops.
+        The thread can continue to use MPS after calling this function.
+    """
+    return torch._C._mps_releaseCurrentThreadSlot()
+
+
 def get_rng_state(device: Union[int, str, torch.device] = "mps") -> Tensor:
     r"""Returns the random number generator state as a ByteTensor.
 
@@ -183,6 +212,7 @@ __all__ = [
     "seed",
     "set_rng_state",
     "synchronize",
+    "release_current_thread_slot",
     "empty_cache",
     "set_per_process_memory_fraction",
     "current_allocated_memory",
