# AI Guidance: Completing MPS Parallel Inference (Phase 6+)

**Date**: 2025-12-12 14:00 (local)
**Branch**: `main`
**Current Plan**: `MPS_PARALLEL_INFERENCE_PLAN.md`

## 1) Current status (factual)

### What works

- **MPSStreamPool** is implemented and validated for basic tensor ops.
  - Reference: `reports/main/phase5_build_and_test_2025-12-12.md`
  - PASS coverage includes multi-thread `randn/mm/relu/synchronize`:
    - `tests/test_parallel_mps.py`
    - `tests/test_stress_extended.py`

### What is blocked

- **Parallel `torch.nn.Module` inference is still crashing** (segfault / KernelDAG errors).
  - Reference: `reports/main/phase6_real_model_testing_2025-12-12.md`
  - Plan directive: `MPS_PARALLEL_INFERENCE_PLAN.md` (“FIX nn.Module Parallel Crashes - MANDATORY”)

### Live worker state (do not assume committed)

- A worker loop is currently running (see `worker_status.json`).
- Latest log file (as of this report): `worker_logs/worker_iter_4_claude_20251212_134439.jsonl`
  - It shows experiments around **per-thread caches** in `aten/src/ATen/native/mps/OperationUtils.h/.mm`
  - The minimal `nn.Linear` parallel repro still exits **139 (SIGSEGV)** after those changes.

## 2) Project goal (what “done” means)

From `CLAUDE.md` goals: **8+ concurrent `forward()` calls without crashes** and **near-linear throughput scaling until GPU saturation**, with **no global hot-path mutex**.

Definition of done (minimum):
- `tests/test_parallel_mps.py` PASS
- The minimal repro from `MPS_PARALLEL_INFERENCE_PLAN.md` (“nn.Linear parallel works”) PASS
- `tests/test_real_models_parallel.py` PASS (MLP, Conv1D, cross-model)

## 3) The core technical problem to solve

“Stream pool” fixes command-buffer contention, but **real models exercise additional shared state**:
- MPSGraph encode/compile paths (`MPSStream::executeMPSGraph` does `encodeToCommandBuffer`).
- Metal shader / pipeline compilation and caches (e.g., `native/mps/MetalShaderLibrary.*`).
- Potential other global caches in `aten/src/ATen/native/mps/operations/*`.

The failure mode matters:
- If it only fails on **first-run compilation** under concurrency, you can fix with **thread-safe one-time initialization** (compile under lock; run afterwards in parallel).
- If it fails even after warmup, the underlying API may be **globally not thread-safe for concurrent encode**, which would force a more invasive design choice (and may conflict with project goals).

## 4) Next AI: high-signal execution plan

### Step A — Identify whether crashes are “compile-time” or “steady-state”

Create two targeted repros (keep them tiny and deterministic):
1. **Cold-start race**: use a barrier so all threads do their *first* `nn.Linear` forward simultaneously.
2. **Warm-start race**: run a single-thread warmup that exercises the exact same ops/shapes, then run the barrier test again.

If warm-start becomes stable, your main job is **make compilation/caching thread-safe** (not serialize steady-state execution).

### Step B — Audit and fix obvious shared caches used by nn.Module ops

Start with caches that are currently **not thread-safe by construction** (unordered_map with no lock).

High priority candidates:
- `pytorch-mps-fork/aten/src/ATen/native/mps/MetalShaderLibrary.h`
  - `MetalShaderLibrary::libMap` and `MetalShaderLibrary::cplMap` are plain `std::unordered_map` with no locking.
  - Any path that calls `getLibrary(...)` / `compileLibrary(...)` or `getLibraryPipelineState(...)` concurrently can corrupt these maps.

Suggested fix shape:
- Add a lock guarding *all* accesses/mutations of `libMap` and `cplMap`.
- Keep the lock scope small: check cache → if miss, compile/construct → insert → return.
- Re-test warm-start and cold-start cases.

### Step C — Only if needed: constrain MPSGraph compilation, not encoding

`pytorch-mps-fork/aten/src/ATen/mps/MPSStream.mm`:
- `MPSStream::executeMPSGraph(...)` calls `[mpsGraph encodeToCommandBuffer:...]` on the stream’s serial queue.

If you confirm the crash is during compilation inside `encodeToCommandBuffer` (common on first call), prefer:
- **Serialize compilation per-graph-key** (or per-op-key), not all graph encoding globally.
- Keep steady-state parallelism.

Concrete tactics to try (in this order):
1. Add a per-key “compile once” guard in the graph cache layer (requires storing “compiled” state alongside the cached graph, with a lock/once-flag).
2. If MPSGraph exposes an explicit “compile/executable” API (search for `MPSGraphExecutable` in headers / docs), compile under lock and then encode executables.

Avoid as final solution unless proven unavoidable:
- A global mutex around every `encodeToCommandBuffer` call (will likely destroy parallel model throughput).

### Step D — Validate against the project goal

After each candidate fix:
- Run `tests/test_parallel_mps.py` (regression safety)
- Run the plan’s minimal `nn.Linear` parallel snippet (fast signal)
- Run `tests/test_real_models_parallel.py` (goal-level signal)

If you introduce locks:
- Measure if the lock is hit only on first-use (acceptable) vs every forward (likely unacceptable).

## 5) Repo hygiene rules that matter in practice

- `pytorch-mps-fork/` is gitignored: **capture real work as patch files** in `patches/` and commit those.
- Prefer updating existing docs (plan + reports) over creating many new files; keep guidance consolidated.
- If a worker is live, read `worker_status.json` and the active `worker_logs/*.jsonl` before repeating work.

