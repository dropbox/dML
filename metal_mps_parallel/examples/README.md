# MPS Parallel Inference Examples

Examples demonstrating optimal usage patterns for PyTorch MPS on Apple Silicon.

## Key Insight

**Batching is dramatically faster than threading** for MPS inference.

| Approach | Throughput | Notes |
|----------|------------|-------|
| Baseline (naive threading) | ~1,100 samples/s | 8 threads, batch=1, fp32, sync every op |
| Optimized (all techniques) | ~50,600 samples/s | **62x improvement** |
| Maximum batching | ~1,400,000 samples/s | batch=256, single thread |

GPUs are designed for parallel data (batching), not parallel tasks (threading).

## Optimization Stack (62x Improvement)

| Optimization | Impact | Cumulative |
|--------------|--------|------------|
| 1. Dynamic batch sizing (2 threads Ã— batch 32) | 34x | ~38,000 samples/s |
| 2. Pipelined async execution (depth 8) | +10% | ~42,000 samples/s |
| 3. Reduced precision (float16) | +14% | ~48,000 samples/s |
| 4. torch.compile(backend="eager") | +5-8% | ~50,600 samples/s |

## Files

### `optimized_mps_inference.py` (NEW - 62x improvement)

Complete optimization engine implementing all discovered techniques:

```python
from examples.optimized_mps_inference import OptimizedMPSInference, MPSConfig

# Configure optimizations
config = MPSConfig(
    num_threads=2,       # Fewer threads, larger batches
    batch_size=32,       # GPU parallelizes within batch
    pipeline_depth=8,    # Queue ops before sync
    use_float16=True,    # Native Apple Silicon support
    use_compile=False,   # Set True if Python < 3.14
    use_safe_sync=True,  # Use .cpu() instead of synchronize()
)

# Create engine
model = YourModel().to("mps").eval()
engine = OptimizedMPSInference(model, config)

# Warmup (important for torch.compile and Metal shader compilation)
engine.warmup(sample_input)

# Process batches
batches = [torch.randn(32, 256) for _ in range(100)]
results = engine.run(batches)
```

**Also includes:**
- `DynamicBatchingServer` - Collects requests and batches them automatically
- `benchmark_optimizations()` - Compare baseline vs optimized throughput

### `tensor_pool.py`

Memory pooling for MPS parallel inference. Pre-allocates tensors to reduce allocation overhead.

```python
from tensor_pool import TensorPool

# Create pool for input tensors
pool = TensorPool(shape=(32, 256), device='mps', pool_size=8)

# Use with context manager (automatic acquire/release)
with pool.context() as tensor:
    tensor.copy_(input_data)
    result = model(tensor)
```

**Measured speedup: 35% with inference** (Apple M4 Max, N=3682)

### `batched_inference.py`

Demonstrates why batching achieves near-linear scaling while threading plateaus at ~4,000 ops/s.

```bash
# Basic batching benchmark
python batched_inference.py

# Compare batching vs threading
python batched_inference.py --compare-threading

# Custom batch sizes
python batched_inference.py --batch-sizes 1,4,8,16,32 --compare-threading
```

**Output (example - actual numbers vary by hardware):**
```
At 8 parallel units:
  Batching:  ~23,000 samples/s
  Threading: ~4,000 ops/s
  Batching is ~5x faster at batch 8!

At batch 256, batching achieves ~1.4M samples/s (373x faster than threading).
```

## Why Batching Wins

### Batching (recommended)
- Single GPU dispatch per batch
- GPU parallelizes work within the batch
- No mutex contention
- ~95% efficiency achievable

### Threading (limited - plateaus)
- Threading plateaus at ~4,000 ops/s total regardless of thread count
- GPU command queue becomes the bottleneck
- Use ThreadPoolExecutor or persistent workers (avoid thread creation overhead)
- Still dramatically slower than batching (~373x at batch 256)

## Recommendations

1. **Use batching** for maximum throughput on Apple Silicon
2. **Group inputs** into batches rather than processing individually
3. **Prefer batching** for parallel inference - threading works but batching is ~373x faster at scale (batch 256)
4. **Use threading only** when inputs arrive asynchronously and batching isn't possible

## Background

Our thread-safety patches (201 fixes) make MPS threading *safe* for concurrent use. Threading is safe but plateaus at ~3,800 ops/s regardless of thread count. Batching remains dramatically faster because it uses GPU-internal parallelism (~373x throughput vs threading at batch 256).

For details, see:
- `apple_feedback/APPLE_FEEDBACK_BUG_REPORT.md` - Bug report for Apple
- `FINAL_COMPLETION_REPORT.md` - Project summary
