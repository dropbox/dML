# Copyright 2024-2025 Andrew Yates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
DashVoice Prometheus Metrics

Provides observability for the DashVoice voice processing server.

Metrics exposed:
- Request latency (histogram) by endpoint
- Request count (counter) by endpoint and status
- TTS generation metrics (latency, RTF, samples)
- STT transcription metrics (latency, audio duration)
- VAD detection metrics
- Pipeline processing metrics
- Active WebSocket sessions (gauge)
- Model warm-up status (gauge)

Usage:
    from tools.dashvoice.metrics import (
        instrument_app,
        track_tts_generation,
        track_stt_transcription,
        track_pipeline_processing,
    )

    # Instrument FastAPI app
    instrument_app(app)

    # Track TTS generation
    with track_tts_generation(voice="af_bella"):
        # Generate audio
        pass
"""

import time
from contextlib import contextmanager

try:
    from prometheus_client import (
        CONTENT_TYPE_LATEST,
        CollectorRegistry,  # noqa: F401
        Counter,
        Gauge,
        Histogram,
        generate_latest,
        multiprocess,  # noqa: F401
    )
    from prometheus_client.core import REGISTRY
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    REGISTRY = None


# =============================================================================
# Metric Definitions
# =============================================================================

if PROMETHEUS_AVAILABLE:
    # Request metrics
    REQUEST_LATENCY = Histogram(
        "dashvoice_request_latency_seconds",
        "Request latency in seconds",
        ["endpoint", "method"],
        buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
    )

    REQUEST_COUNT = Counter(
        "dashvoice_requests_total",
        "Total number of requests",
        ["endpoint", "method", "status"],
    )

    REQUEST_IN_PROGRESS = Gauge(
        "dashvoice_requests_in_progress",
        "Number of requests currently being processed",
        ["endpoint"],
    )

    # TTS metrics
    TTS_GENERATION_LATENCY = Histogram(
        "dashvoice_tts_generation_seconds",
        "TTS generation latency in seconds",
        ["voice"],
        buckets=(0.05, 0.1, 0.25, 0.5, 1.0, 2.0, 5.0),
    )

    TTS_AUDIO_DURATION = Histogram(
        "dashvoice_tts_audio_duration_seconds",
        "Duration of generated TTS audio in seconds",
        ["voice"],
        buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0),
    )

    TTS_RTF = Histogram(
        "dashvoice_tts_rtf",
        "TTS Real-Time Factor (lower is faster)",
        ["voice"],
        buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0),
    )

    TTS_SAMPLES_GENERATED = Counter(
        "dashvoice_tts_samples_total",
        "Total audio samples generated by TTS",
        ["voice"],
    )

    # STT metrics
    STT_TRANSCRIPTION_LATENCY = Histogram(
        "dashvoice_stt_transcription_seconds",
        "STT transcription latency in seconds",
        buckets=(0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0),
    )

    STT_AUDIO_PROCESSED = Counter(
        "dashvoice_stt_audio_seconds_total",
        "Total seconds of audio processed by STT",
    )

    STT_TRANSCRIPTION_LENGTH = Histogram(
        "dashvoice_stt_transcription_chars",
        "Length of transcriptions in characters",
        buckets=(10, 50, 100, 250, 500, 1000, 2500),
    )

    # VAD metrics
    VAD_SEGMENTS_DETECTED = Counter(
        "dashvoice_vad_segments_total",
        "Total speech segments detected by VAD",
        ["type"],  # "speech" or "silence"
    )

    VAD_PROCESSING_LATENCY = Histogram(
        "dashvoice_vad_processing_seconds",
        "VAD processing latency in seconds",
        buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1),
    )

    # Pipeline metrics
    PIPELINE_PROCESSING_LATENCY = Histogram(
        "dashvoice_pipeline_processing_seconds",
        "Full pipeline processing latency in seconds",
        ["features"],  # e.g., "vad+stt+diarization"
        buckets=(0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0),
    )

    PIPELINE_SPEAKERS_DETECTED = Histogram(
        "dashvoice_pipeline_speakers_detected",
        "Number of speakers detected per audio file",
        buckets=(1, 2, 3, 4, 5, 10),
    )

    PIPELINE_DASHVOICE_DETECTED = Counter(
        "dashvoice_pipeline_dashvoice_detections_total",
        "Total DashVoice (TTS) audio detections",
        ["voice"],
    )

    # WebSocket metrics
    WEBSOCKET_SESSIONS_ACTIVE = Gauge(
        "dashvoice_websocket_sessions_active",
        "Number of active WebSocket sessions",
        ["endpoint"],  # "stream", "transcribe", "synthesize"
    )

    WEBSOCKET_MESSAGES_RECEIVED = Counter(
        "dashvoice_websocket_messages_received_total",
        "Total WebSocket messages received",
        ["endpoint", "type"],  # type: "audio", "config", "end", "text"
    )

    WEBSOCKET_AUDIO_BYTES_RECEIVED = Counter(
        "dashvoice_websocket_audio_bytes_total",
        "Total audio bytes received via WebSocket",
        ["endpoint"],
    )

    # Model status metrics
    MODEL_LOADED = Gauge(
        "dashvoice_model_loaded",
        "Whether a model is loaded (1) or not (0)",
        ["model"],  # "tts", "stt", "vad", "diarization"
    )

    MODEL_WARMUP_LATENCY = Gauge(
        "dashvoice_model_warmup_seconds",
        "Model warm-up latency in seconds",
        ["model"],
    )

    # System metrics
    STARTUP_TIME = Gauge(
        "dashvoice_startup_timestamp_seconds",
        "Server startup timestamp",
    )

    PREWARM_COMPLETE = Gauge(
        "dashvoice_prewarm_complete",
        "Whether pre-warming is complete (1) or not (0)",
    )

    # Concurrency metrics
    SEMAPHORE_ACTIVE = Gauge(
        "dashvoice_semaphore_active",
        "Number of active requests holding the semaphore",
        ["type"],  # "tts" or "pipeline"
    )

    SEMAPHORE_WAIT_TIME = Histogram(
        "dashvoice_semaphore_wait_seconds",
        "Time spent waiting for semaphore",
        ["type"],
        buckets=(0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0),
    )


# =============================================================================
# Metric Tracking Functions
# =============================================================================

@contextmanager
def track_tts_generation(voice: str = "unknown"):
    """Context manager to track TTS generation metrics."""
    if not PROMETHEUS_AVAILABLE:
        yield
        return

    start_time = time.perf_counter()
    try:
        yield
    finally:
        latency = time.perf_counter() - start_time
        TTS_GENERATION_LATENCY.labels(voice=voice).observe(latency)


def record_tts_output(voice: str, samples: int, sample_rate: int, generation_time_s: float):
    """Record TTS output metrics."""
    if not PROMETHEUS_AVAILABLE:
        return

    duration_s = samples / sample_rate
    rtf = generation_time_s / duration_s if duration_s > 0 else 0

    TTS_AUDIO_DURATION.labels(voice=voice).observe(duration_s)
    TTS_RTF.labels(voice=voice).observe(rtf)
    TTS_SAMPLES_GENERATED.labels(voice=voice).inc(samples)


@contextmanager
def track_stt_transcription():
    """Context manager to track STT transcription metrics."""
    if not PROMETHEUS_AVAILABLE:
        yield
        return

    start_time = time.perf_counter()
    try:
        yield
    finally:
        latency = time.perf_counter() - start_time
        STT_TRANSCRIPTION_LATENCY.observe(latency)


def record_stt_output(audio_duration_s: float, transcription: str):
    """Record STT output metrics."""
    if not PROMETHEUS_AVAILABLE:
        return

    STT_AUDIO_PROCESSED.inc(audio_duration_s)
    STT_TRANSCRIPTION_LENGTH.observe(len(transcription))


@contextmanager
def track_vad_processing():
    """Context manager to track VAD processing metrics."""
    if not PROMETHEUS_AVAILABLE:
        yield
        return

    start_time = time.perf_counter()
    try:
        yield
    finally:
        latency = time.perf_counter() - start_time
        VAD_PROCESSING_LATENCY.observe(latency)


def record_vad_segment(is_speech: bool):
    """Record VAD segment detection."""
    if not PROMETHEUS_AVAILABLE:
        return

    segment_type = "speech" if is_speech else "silence"
    VAD_SEGMENTS_DETECTED.labels(type=segment_type).inc()


@contextmanager
def track_pipeline_processing(features: str):
    """Context manager to track pipeline processing metrics."""
    if not PROMETHEUS_AVAILABLE:
        yield
        return

    start_time = time.perf_counter()
    try:
        yield
    finally:
        latency = time.perf_counter() - start_time
        PIPELINE_PROCESSING_LATENCY.labels(features=features).observe(latency)


def record_pipeline_output(num_speakers: int, dashvoice_voice: str | None = None):
    """Record pipeline output metrics."""
    if not PROMETHEUS_AVAILABLE:
        return

    PIPELINE_SPEAKERS_DETECTED.observe(num_speakers)
    if dashvoice_voice:
        PIPELINE_DASHVOICE_DETECTED.labels(voice=dashvoice_voice).inc()


def record_model_loaded(model: str, loaded: bool):
    """Record model loaded status."""
    if not PROMETHEUS_AVAILABLE:
        return

    MODEL_LOADED.labels(model=model).set(1 if loaded else 0)


def record_model_warmup(model: str, latency_s: float):
    """Record model warm-up latency."""
    if not PROMETHEUS_AVAILABLE:
        return

    MODEL_WARMUP_LATENCY.labels(model=model).set(latency_s)


def record_prewarm_complete():
    """Mark pre-warming as complete."""
    if not PROMETHEUS_AVAILABLE:
        return

    PREWARM_COMPLETE.set(1)


def record_startup():
    """Record server startup."""
    if not PROMETHEUS_AVAILABLE:
        return

    STARTUP_TIME.set(time.time())


@contextmanager
def track_semaphore_wait(semaphore_type: str):
    """Context manager to track semaphore wait time.

    Args:
        semaphore_type: "tts" or "pipeline"
    """
    if not PROMETHEUS_AVAILABLE:
        yield
        return

    start_time = time.perf_counter()
    try:
        yield
    finally:
        wait_time = time.perf_counter() - start_time
        SEMAPHORE_WAIT_TIME.labels(type=semaphore_type).observe(wait_time)


def semaphore_acquired(semaphore_type: str):
    """Record semaphore acquired."""
    if not PROMETHEUS_AVAILABLE:
        return

    SEMAPHORE_ACTIVE.labels(type=semaphore_type).inc()


def semaphore_released(semaphore_type: str):
    """Record semaphore released."""
    if not PROMETHEUS_AVAILABLE:
        return

    SEMAPHORE_ACTIVE.labels(type=semaphore_type).dec()


# WebSocket tracking
def websocket_session_opened(endpoint: str):
    """Record WebSocket session opened."""
    if not PROMETHEUS_AVAILABLE:
        return

    WEBSOCKET_SESSIONS_ACTIVE.labels(endpoint=endpoint).inc()


def websocket_session_closed(endpoint: str):
    """Record WebSocket session closed."""
    if not PROMETHEUS_AVAILABLE:
        return

    WEBSOCKET_SESSIONS_ACTIVE.labels(endpoint=endpoint).dec()


def record_websocket_message(endpoint: str, msg_type: str):
    """Record WebSocket message received."""
    if not PROMETHEUS_AVAILABLE:
        return

    WEBSOCKET_MESSAGES_RECEIVED.labels(endpoint=endpoint, type=msg_type).inc()


def record_websocket_audio(endpoint: str, num_bytes: int):
    """Record WebSocket audio bytes received."""
    if not PROMETHEUS_AVAILABLE:
        return

    WEBSOCKET_AUDIO_BYTES_RECEIVED.labels(endpoint=endpoint).inc(num_bytes)


# =============================================================================
# FastAPI Middleware
# =============================================================================

def instrument_app(app):
    """Add Prometheus metrics middleware to FastAPI app.

    Args:
        app: FastAPI application instance
    """
    if not PROMETHEUS_AVAILABLE:
        print("Warning: prometheus_client not installed. Metrics disabled.")
        return

    from starlette.middleware.base import BaseHTTPMiddleware
    from starlette.requests import Request
    from starlette.responses import Response

    class PrometheusMiddleware(BaseHTTPMiddleware):
        """Middleware to track request metrics."""

        async def dispatch(self, request: Request, call_next):
            # Skip metrics endpoint to avoid recursion
            if request.url.path == "/metrics":
                return await call_next(request)

            endpoint = request.url.path
            method = request.method

            REQUEST_IN_PROGRESS.labels(endpoint=endpoint).inc()
            start_time = time.perf_counter()

            try:
                response = await call_next(request)
                status = str(response.status_code)
            except Exception:
                status = "500"
                raise
            finally:
                latency = time.perf_counter() - start_time
                REQUEST_LATENCY.labels(endpoint=endpoint, method=method).observe(latency)
                REQUEST_COUNT.labels(endpoint=endpoint, method=method, status=status).inc()
                REQUEST_IN_PROGRESS.labels(endpoint=endpoint).dec()

            return response

    app.add_middleware(PrometheusMiddleware)

    # Add metrics endpoint
    @app.get("/metrics")
    async def metrics():
        """Prometheus metrics endpoint."""
        return Response(
            content=generate_latest(REGISTRY),
            media_type=CONTENT_TYPE_LATEST,
        )

    record_startup()
    print("Prometheus metrics enabled at /metrics")


# =============================================================================
# Utility Functions
# =============================================================================

def get_metrics_summary() -> dict:
    """Get a summary of current metrics as a dictionary.

    Useful for debugging and testing without Prometheus.
    """
    if not PROMETHEUS_AVAILABLE:
        return {"error": "prometheus_client not installed"}

    return {
        "prometheus_available": True,
        "note": "Use /metrics endpoint for full Prometheus format",
    }
