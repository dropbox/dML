# Copyright 2024-2025 Andrew Yates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
MADLAD-400 Model Converter

MADLAD-400 (Massively Multilingual Document-level Automatic Dialogue) provides
Apache 2.0 licensed translation for 400+ languages.

This converter replaces NLLB (CC-BY-NC) for commercial use cases.

Reference:
- Model: https://huggingface.co/google/madlad400-3b-mt
- Paper: https://arxiv.org/abs/2309.04662

OPT-7: N-gram Lookahead Decoding
================================
Uses patterns from source text and generation history to predict likely
continuations. Particularly effective for:
- Repetitive patterns (technical documents, legal text)
- Copy mechanisms (source words appearing in output)
- Common phrases and patterns

Achieves 1.5-2x speedup for suitable workloads without requiring a draft model.
"""

import time
from dataclasses import dataclass
from pathlib import Path

try:
    import mlx.core as mx

    from .models.t5_mlx import T5KVCache, T5Model, copy_cache_list  # noqa: F401

    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False

try:
    from transformers import T5Tokenizer

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


# MADLAD supported language codes (subset - full list has 400+)
MADLAD_LANGUAGES = {
    "en": "English",
    "fr": "French",
    "de": "German",
    "es": "Spanish",
    "it": "Italian",
    "pt": "Portuguese",
    "ru": "Russian",
    "zh": "Chinese",
    "ja": "Japanese",
    "ko": "Korean",
    "ar": "Arabic",
    "hi": "Hindi",
    "tr": "Turkish",
    "pl": "Polish",
    "nl": "Dutch",
    "sv": "Swedish",
    "da": "Danish",
    "fi": "Finnish",
    "no": "Norwegian",
    "cs": "Czech",
    "el": "Greek",
    "he": "Hebrew",
    "th": "Thai",
    "vi": "Vietnamese",
    "id": "Indonesian",
    "ms": "Malay",
    "uk": "Ukrainian",
    "ro": "Romanian",
    "hu": "Hungarian",
    "bg": "Bulgarian",
    # ... 400+ languages supported
}


@dataclass
class TranslationResult:
    """Result of a translation."""

    text: str
    source_lang: str
    target_lang: str
    latency_ms: float
    tokens_generated: int


@dataclass
class SpeculativeTranslationResult:
    """Result of speculative decoding translation."""

    text: str
    source_lang: str
    target_lang: str
    latency_ms: float
    tokens_generated: int
    accepted_tokens: int  # Tokens accepted from draft model
    total_draft_tokens: int  # Total tokens generated by draft
    acceptance_rate: float  # accepted / total_draft
    speedup: float  # Speedup vs baseline (if measured)


@dataclass
class NgramLookaheadResult:
    """Result of n-gram lookahead decoding translation."""

    text: str
    source_lang: str
    target_lang: str
    latency_ms: float
    tokens_generated: int
    accepted_tokens: int  # Tokens accepted from n-gram lookahead
    total_lookahead_tokens: int  # Total tokens proposed by lookahead
    acceptance_rate: float  # accepted / total_lookahead
    ngram_hits: int  # Number of n-gram cache hits
    speedup: float  # Speedup vs baseline (if measured)


class NgramCache:
    """
    N-gram cache for lookahead decoding.

    Builds a trie-like structure mapping n-gram prefixes to likely continuations.
    Uses both source tokens and generated tokens to build predictions.

    Optimized with confidence tracking:
    - Only proposes continuations that have been seen multiple times
    - Tracks acceptance rate to avoid low-confidence lookups
    - Uses count-based confidence threshold
    """

    def __init__(
        self, n: int = 3, max_lookahead: int = 2, min_confidence: int = 2,
    ):
        """
        Args:
            n: Size of n-gram prefix to match (optimal: 3)
            max_lookahead: Maximum continuation tokens to propose (optimal: 2)
            min_confidence: Minimum times an n-gram must be seen to propose (default: 2)
        """
        self.n = n
        self.max_lookahead = max_lookahead
        self.min_confidence = min_confidence
        # Maps tuple of n tokens -> (continuation_tokens, count)
        self.cache: dict[tuple[int, ...], tuple[list[int], int]] = {}

    def add_sequence(self, tokens: list[int]):
        """Add a token sequence to the cache, tracking frequency."""
        if len(tokens) < self.n + 1:
            return

        for i in range(len(tokens) - self.n):
            prefix = tuple(tokens[i : i + self.n])
            continuation = tokens[i + self.n : i + self.n + self.max_lookahead]
            if continuation:
                if prefix in self.cache:
                    existing_cont, count = self.cache[prefix]
                    # If same continuation, increment count
                    if existing_cont == continuation:
                        self.cache[prefix] = (continuation, count + 1)
                    # If different continuation and higher count, replace
                    elif count <= 1:
                        self.cache[prefix] = (continuation, 1)
                else:
                    self.cache[prefix] = (continuation, 1)

    def lookup(self, last_n_tokens: list[int]) -> list[int]:
        """
        Look up likely continuations for the last n tokens.

        Only returns continuations that meet the minimum confidence threshold.
        Returns up to max_lookahead continuation tokens, or empty list if no confident match.
        """
        if len(last_n_tokens) < self.n:
            return []

        prefix = tuple(last_n_tokens[-self.n :])
        if prefix in self.cache:
            continuation, count = self.cache[prefix]
            # Only return if we've seen this pattern enough times
            if count >= self.min_confidence:
                return continuation[: self.max_lookahead]
        return []

    def clear(self):
        """Clear the cache."""
        self.cache.clear()


@dataclass
class BenchmarkResult:
    """Benchmark comparison result."""

    median_latency_ms: float
    mean_latency_ms: float
    min_latency_ms: float
    max_latency_ms: float
    tokens_per_second: float


@dataclass
class BatchTranslationResult:
    """Result of batched translation with throughput metrics."""

    results: list[TranslationResult]
    total_latency_ms: float
    throughput_texts_per_second: float
    throughput_tokens_per_second: float
    batch_size: int
    speedup_vs_sequential: float  # Compared to translating one by one


class MADLADConverter:
    """
    MADLAD-400 translation using MLX.

    Apache 2.0 licensed - safe for commercial use.
    Supports 400+ languages.

    Example:
        converter = MADLADConverter()
        result = converter.translate("Hello world", tgt_lang="fr")
        print(result.text)  # "Bonjour le monde"

        # Or load from specific path
        converter = MADLADConverter(model_path="./madlad_mlx")
        result = converter.translate("Hello", tgt_lang="de")
    """

    # Default HuggingFace model
    DEFAULT_MODEL = "google/madlad400-3b-mt"

    def __init__(
        self,
        model_path: str | None = None,
        dtype: str = "bfloat16",
        quantize: int | None = 8,
    ):
        """
        Initialize MADLAD converter.

        Args:
            model_path: Path to model (local or HuggingFace repo).
                       If None, uses default HuggingFace model.
            dtype: Model dtype ("bfloat16", "float16", "float32")
            quantize: Quantization bits (4, 8, or "mixed"). None for no quantization.
                     - 8: 8-bit quantization (lossless quality, recommended)
                     - 4: 4-bit quantization (faster but quality issues with CJK)
                     - "mixed": Mixed precision (8-bit attention, 4-bit FFN)
                       Provides better quality than 4-bit with improved speed over 8-bit.
                     Default 8-bit for quality.
        """
        if not MLX_AVAILABLE:
            raise ImportError("MLX is required. Install with: pip install mlx")

        self.model_path = model_path or self.DEFAULT_MODEL
        self.dtype = getattr(mx, dtype)
        # Support "mixed" string or int bits
        if isinstance(quantize, str) and quantize.lower() == "mixed":
            self.quantize_bits = "mixed"
        else:
            self.quantize_bits = quantize
        self.model: T5Model | None = None
        self.tokenizer = None
        self._loaded = False
        # Early exit head for speculative decoding (OPT-4)
        self.early_exit_head = None
        self.early_exit_layer = None

    def _is_3b_model(self) -> bool:
        """Check if using the 3B model (which has Hebrew defects)."""
        return "3b" in self.model_path.lower()

    def _preprocess_hebrew(self, text: str, tgt_lang: str) -> str:
        """
        Apply Hebrew-specific preprocessing for MADLAD-3B.

        The 3B model has a known defect where "Hello" + certain continuations
        triggers a repetition loop in Hebrew translations. This workaround
        substitutes "Hello" with "Hi" which works correctly.

        Only applied when:
        - Target language is Hebrew ("he")
        - Using the 3B model (not 7B which handles Hebrew correctly)

        Args:
            text: Input text to preprocess
            tgt_lang: Target language code

        Returns:
            Preprocessed text with substitutions applied
        """
        # Only apply for Hebrew with 3B model
        if tgt_lang != "he" or not self._is_3b_model():
            return text

        # Replace problematic "Hello" patterns with "Hi"
        # Case-insensitive replacement at word boundaries
        import re
        # Handle common patterns: "Hello", "Hello,", "Hello!"
        return re.sub(r'\bHello\b', 'Hi', text, flags=re.IGNORECASE)

    def load(self):
        """Load model and tokenizer."""
        if self._loaded:
            return

        if not TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "transformers is required for tokenizer. "
                "Install with: pip install transformers",
            )

        print(f"Loading MADLAD-400 from {self.model_path}...")

        # Load tokenizer from HuggingFace
        # MADLAD uses the T5 tokenizer with special language tokens
        tokenizer_path = self.model_path
        if isinstance(tokenizer_path, Path):
            tokenizer_path = str(tokenizer_path)

        self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)

        # Load MLX model
        self.model = T5Model.from_pretrained(self.model_path, dtype=self.dtype)

        # Apply quantization if requested
        if self.quantize_bits is not None:
            if self.quantize_bits == "mixed":
                self._apply_mixed_quantization()
            else:
                self._apply_quantization()

        # Warmup
        self._warmup()
        self._loaded = True
        print("MADLAD-400 loaded successfully.")

    def _apply_quantization(self):
        """Apply quantization to model for faster inference."""
        import mlx.nn as nn
        from mlx.utils import tree_flatten

        bits = self.quantize_bits
        if bits not in (4, 8):
            raise ValueError(f"Quantization bits must be 4 or 8, got {bits}")

        print(f"Applying {bits}-bit quantization...")

        # Predicate to quantize only Linear and compatible Embedding layers
        # Skips RMSNorm, RelativePositionBias, and small layers
        def should_quantize(path: str, module) -> bool:
            if isinstance(module, nn.Linear):
                # Check shape compatibility with group_size=64
                if module.weight.shape[-1] % 64 == 0:
                    return True
            if isinstance(module, nn.Embedding):
                # Check shape compatibility
                if module.weight.shape[-1] % 64 == 0:
                    return True
            return False

        nn.quantize(self.model, bits=bits, group_size=64, class_predicate=should_quantize)
        mx.eval(tree_flatten(self.model.parameters()))
        print(f"{bits}-bit quantization complete.")

    def _apply_mixed_quantization(self):
        """
        Apply mixed precision quantization (OPT-9).

        Uses 8-bit for attention layers (quality-critical) and 4-bit for FFN layers
        (largest but less quality-sensitive). This provides:
        - Better quality than pure 4-bit (attention preserved at 8-bit)
        - Faster than pure 8-bit (FFN layers are 4-bit, ~60% of params)

        Layer classification:
        - Attention: query_proj, key_proj, value_proj, out_proj -> 8-bit
        - FFN: wi_0, wi_1, wi, wo -> 4-bit
        - Embeddings: -> 8-bit (quality-critical for vocabulary)
        """
        import mlx.nn as nn
        from mlx.utils import tree_flatten

        print("Applying mixed precision quantization (8-bit attention, 4-bit FFN)...")

        # Attention layer patterns (8-bit for quality)
        attention_patterns = ("query_proj", "key_proj", "value_proj", "out_proj")

        # FFN layer patterns (4-bit for speed)
        ffn_patterns = ("wi_0", "wi_1", "wi", "wo")

        # Track quantization stats
        attn_count = 0
        ffn_count = 0
        emb_count = 0

        def is_attention_layer(path: str) -> bool:
            """Check if path indicates an attention layer."""
            return any(pattern in path for pattern in attention_patterns)

        def is_ffn_layer(path: str) -> bool:
            """Check if path indicates an FFN layer."""
            # FFN layers: wi_0, wi_1, wi, wo
            # Make sure we don't match attention layers that might contain "wi"
            path_lower = path.lower()
            for pattern in ffn_patterns:
                # Check for exact match with . or _ boundary
                if f".{pattern}." in f".{path_lower}." or path_lower.endswith(pattern):
                    return True
            return False

        def should_quantize_8bit(path: str, module) -> bool:
            """Predicate for 8-bit quantization (attention + embeddings)."""
            nonlocal attn_count, emb_count
            if isinstance(module, nn.Linear):
                if module.weight.shape[-1] % 64 == 0:
                    if is_attention_layer(path):
                        attn_count += 1
                        return True
            if isinstance(module, nn.Embedding):
                if module.weight.shape[-1] % 64 == 0:
                    emb_count += 1
                    return True
            return False

        def should_quantize_4bit(path: str, module) -> bool:
            """Predicate for 4-bit quantization (FFN layers)."""
            nonlocal ffn_count
            if isinstance(module, nn.Linear):
                if module.weight.shape[-1] % 64 == 0:
                    if is_ffn_layer(path):
                        ffn_count += 1
                        return True
            return False

        # First pass: 8-bit for attention and embeddings
        nn.quantize(
            self.model, bits=8, group_size=64, class_predicate=should_quantize_8bit,
        )

        # Second pass: 4-bit for FFN layers
        nn.quantize(
            self.model, bits=4, group_size=64, class_predicate=should_quantize_4bit,
        )

        mx.eval(tree_flatten(self.model.parameters()))

        print("Mixed precision quantization complete:")
        print(f"  - Attention layers (8-bit): {attn_count}")
        print(f"  - FFN layers (4-bit): {ffn_count}")
        print(f"  - Embedding layers (8-bit): {emb_count}")

    def _warmup(self):
        """Warmup model for accurate benchmarking."""
        if self.model is None or self.tokenizer is None:
            return

        # Run a short translation to warm up
        input_text = "<2en> Test"
        inputs = self.tokenizer(input_text, return_tensors="np")
        input_ids = mx.array(inputs["input_ids"])

        # Encode
        encoder_output = self.model.encode(input_ids)
        mx.eval(encoder_output)

        # Decode a few tokens
        decoder_start_id = getattr(self.model.config, "decoder_start_token_id", 0)
        decoder_start = mx.array([[decoder_start_id]])
        logits, _ = self.model.decode(decoder_start, encoder_output)
        mx.eval(logits)

    def load_early_exit_head(
        self,
        head_path: str,
        config_path: str | None = None,
        exit_layer: int = 4,
    ):
        """
        Load a trained early exit head for speculative decoding (OPT-4).

        The early exit head is trained to predict full model outputs from
        early layer hidden states, enabling high-acceptance-rate speculative decoding.

        Args:
            head_path: Path to saved head weights (.safetensors)
            config_path: Optional path to config JSON (auto-detected if not provided)
            exit_layer: Which decoder layer to exit from (overridden by config if available)
        """
        import json
        from pathlib import Path

        import mlx.nn as nn

        head_path = Path(head_path)

        # Try to load config
        if config_path is None:
            config_path = head_path.with_suffix(".json").with_name(
                head_path.stem.replace("_head", "_config") + ".json",
            )
            if not config_path.exists():
                config_path = head_path.with_suffix(".json")

        config = {}
        if config_path and Path(config_path).exists():
            with open(config_path) as f:
                config = json.load(f)
            exit_layer = config.get("exit_layer", exit_layer)
            print(f"Loaded early exit config: exit_layer={exit_layer}")

        # Get model dimensions
        d_model = self.model.config.d_model
        vocab_size = self.model.config.vocab_size

        # Verify dimensions match config if available
        if config.get("d_model") and config["d_model"] != d_model:
            raise ValueError(
                f"Head d_model ({config['d_model']}) doesn't match model ({d_model})",
            )

        # Create head architecture
        use_ffn = config.get("use_ffn", False)

        if use_ffn:
            ffn_dim = config.get("ffn_dim", d_model)

            class EarlyExitHead(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.ffn = nn.Linear(d_model, ffn_dim, bias=False)
                    self.out = nn.Linear(ffn_dim, vocab_size, bias=False)

                def __call__(self, x):
                    return self.out(nn.gelu(self.ffn(x)))
        else:
            class EarlyExitHead(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.out = nn.Linear(d_model, vocab_size, bias=False)

                def __call__(self, x):
                    return self.out(x)

        # Load weights
        self.early_exit_head = EarlyExitHead()
        weights = mx.load(str(head_path))
        self.early_exit_head.load_weights(list(weights.items()))
        mx.eval(self.early_exit_head.parameters())

        self.early_exit_layer = exit_layer
        print(f"Loaded early exit head from {head_path}")
        print(f"  Exit layer: {self.early_exit_layer}")
        if config.get("final_acceptance_rate"):
            print(f"  Trained acceptance rate: {config['final_acceptance_rate']:.1%}")

    def translate(
        self,
        text: str,
        tgt_lang: str,
        src_lang: str | None = None,
        max_tokens: int = 256,
    ) -> TranslationResult:
        """
        Translate text to target language.

        Args:
            text: Text to translate
            tgt_lang: Target language code (e.g., "fr", "de", "zh")
            src_lang: Source language (auto-detected if not specified)
            max_tokens: Maximum tokens to generate

        Returns:
            TranslationResult with translated text and metadata
        """
        if not self._loaded:
            self.load()

        # These are guaranteed to be set after load() - add assertions for mypy
        assert self.tokenizer is not None, "Tokenizer not loaded"
        assert self.model is not None, "Model not loaded"

        # Early return for empty/whitespace input to prevent hallucination
        # (transformers 5.0+ regression: empty input produces garbage)
        if not text or not text.strip():
            return TranslationResult(
                text="",
                source_lang=src_lang or "auto",
                target_lang=tgt_lang,
                latency_ms=0.0,
                tokens_generated=0,
            )

        # Apply Hebrew-specific preprocessing for 3B model
        text = self._preprocess_hebrew(text, tgt_lang)

        # MADLAD uses <2xx> prefix for target language
        # Format: "<2tgt_lang> source_text"
        input_text = f"<2{tgt_lang}> {text}"

        # Tokenize
        inputs = self.tokenizer(input_text, return_tensors="np")
        input_ids = mx.array(inputs["input_ids"])

        # Encode
        start_time = time.time()
        encoder_output = self.model.encode(input_ids)
        mx.eval(encoder_output)

        # Generate with greedy decoding
        # MADLAD uses decoder_start_token_id=0 from config
        decoder_start_id = getattr(self.model.config, "decoder_start_token_id", 0)

        generated = [decoder_start_id]
        cache = None

        # Initial decode
        decoder_ids = mx.array([generated])
        logits, cache = self.model.decode(decoder_ids, encoder_output, cache=None)
        mx.eval(logits, cache)

        # Generate tokens
        for _ in range(max_tokens):
            next_token = int(mx.argmax(logits[0, -1]))
            generated.append(next_token)

            if next_token == self.tokenizer.eos_token_id:
                break

            # Decode next token with cache
            decoder_ids = mx.array([[next_token]])
            logits, cache = self.model.decode(decoder_ids, encoder_output, cache=cache)
            mx.eval(logits, cache)

        latency_ms = (time.time() - start_time) * 1000

        # Decode output
        # Skip the first token (decoder_start_id)
        output_text = self.tokenizer.decode(generated[1:], skip_special_tokens=True)

        return TranslationResult(
            text=output_text.strip(),
            source_lang=src_lang or "auto",
            target_lang=tgt_lang,
            latency_ms=latency_ms,
            tokens_generated=len(generated) - 1,
        )

    def translate_batch(
        self,
        texts: list[str],
        tgt_lang: str,
        src_lang: str | None = None,
        max_tokens: int = 256,
    ) -> list[TranslationResult]:
        """
        Translate multiple texts to target language.

        Note: For encoder-decoder models like T5/MADLAD, the autoregressive
        decoder is the bottleneck. Batch encoding helps minimally since decode
        time dominates. This method provides a convenient API for translating
        multiple texts.

        Performance: ~60ms per short sentence with 4-bit quantization.
        4 sentences complete in ~240ms.

        Args:
            texts: List of texts to translate
            tgt_lang: Target language code (e.g., "fr", "de", "zh")
            src_lang: Source language (auto-detected if not specified)
            max_tokens: Maximum tokens to generate per text

        Returns:
            List of TranslationResult with translated texts and metadata

        Example:
            >>> converter = MADLADConverter()
            >>> texts = ["Hello world", "How are you?", "Good morning"]
            >>> results = converter.translate_batch(texts, tgt_lang="fr")
            >>> for r in results:
            ...     print(r.text)
        """
        if not texts:
            return []

        if not self._loaded:
            self.load()

        # Sequential translation - decoder bottleneck prevents batch speedup
        return [self.translate(text, tgt_lang, src_lang, max_tokens) for text in texts]

    def translate_batch_continuous(
        self,
        texts: list[str],
        tgt_lang: str,
        src_lang: str | None = None,
        max_tokens: int = 256,
    ) -> BatchTranslationResult:
        """
        Translate multiple texts using continuous batching (OPT-6).

        This implementation batches the encoder pass and uses padded batched
        decoding for throughput improvement. Each sequence generates until
        it produces EOS, at which point it's masked out but remains in batch
        for efficiency.

        Key optimizations:
        1. Batched encoder pass - encode all texts together
        2. Padded batched decoder - decode all sequences in parallel
        3. Dynamic masking - completed sequences stop affecting others
        4. Left-padding for consistent attention masking

        Args:
            texts: List of texts to translate
            tgt_lang: Target language code (e.g., "fr", "de", "zh")
            src_lang: Source language (auto-detected if not specified)
            max_tokens: Maximum tokens to generate per text

        Returns:
            BatchTranslationResult with all translations and throughput metrics
        """
        if not texts:
            return BatchTranslationResult(
                results=[],
                total_latency_ms=0,
                throughput_texts_per_second=0,
                throughput_tokens_per_second=0,
                batch_size=0,
                speedup_vs_sequential=0,
            )

        if not self._loaded:
            self.load()

        assert self.tokenizer is not None, "Tokenizer not loaded"
        assert self.model is not None, "Model not loaded"

        batch_size = len(texts)

        # Apply Hebrew-specific preprocessing for 3B model
        texts = [self._preprocess_hebrew(text, tgt_lang) for text in texts]

        # Prepare input texts with target language prefix
        input_texts = [f"<2{tgt_lang}> {text}" for text in texts]

        # Tokenize all texts
        batch_inputs = self.tokenizer(
            input_texts,
            return_tensors="np",
            padding=True,
            truncation=True,
        )
        input_ids = mx.array(batch_inputs["input_ids"])  # [B, max_src_len]
        # Note: attention_mask is available in batch_inputs["attention_mask"] but T5 encoder
        # doesn't use it directly - it's implicit from padding tokens (pad_token_id=0)

        start_time = time.time()

        # === Batched Encoding ===
        # Encode all sequences at once
        encoder_output = self.model.encode(input_ids)  # [B, max_src_len, d_model]
        mx.eval(encoder_output)

        # === Batched Decoding with Continuous Processing ===
        decoder_start_id = getattr(self.model.config, "decoder_start_token_id", 0)
        eos_token_id = self.tokenizer.eos_token_id
        pad_token_id = self.tokenizer.pad_token_id

        # Initialize generated sequences: [B, 1] with decoder start token
        generated = mx.full((batch_size, 1), decoder_start_id, dtype=mx.int32)

        # Track which sequences have finished (produced EOS)
        finished = mx.zeros((batch_size,), dtype=mx.bool_)
        finished_lengths = mx.full((batch_size,), max_tokens + 1, dtype=mx.int32)

        # Initialize KV cache as None - will be created on first decode
        cache = None

        # Decode loop
        for step in range(max_tokens):
            # Get last token for each sequence (for incremental decode)
            if step == 0:
                # First step: use all generated tokens
                decoder_input = generated  # [B, 1]
            else:
                # Subsequent steps: only last token (with cache)
                decoder_input = generated[:, -1:]  # [B, 1]

            # Decode step with cache
            logits, cache = self.model.decode(
                decoder_input, encoder_output, cache=cache,
            )  # logits: [B, 1, vocab]
            mx.eval(logits, cache)

            # Get next tokens via greedy sampling
            next_tokens = mx.argmax(logits[:, -1, :], axis=-1)  # [B]

            # Mask already-finished sequences: replace their next token with PAD
            next_tokens = mx.where(finished, pad_token_id, next_tokens)

            # Append to generated sequences
            generated = mx.concatenate(
                [generated, next_tokens[:, None]], axis=1,
            )  # [B, seq+1]

            # Update finished status: sequences that just produced EOS
            just_finished = (next_tokens == eos_token_id) & ~finished
            finished = finished | just_finished

            # Record finish lengths
            finished_lengths = mx.where(
                just_finished,
                mx.full((batch_size,), step + 1, dtype=mx.int32),
                finished_lengths,
            )

            # Check if all sequences finished
            if mx.all(finished):
                break

        total_latency_ms = (time.time() - start_time) * 1000

        # === Extract Results ===
        results = []
        total_tokens = 0

        # Convert to numpy for easier indexing
        generated_np = generated.tolist()
        finished_lengths_np = finished_lengths.tolist()

        for i in range(batch_size):
            # Get sequence up to its finish length (excluding start token)
            seq_len = min(finished_lengths_np[i], len(generated_np[i]) - 1)
            seq_tokens = generated_np[i][1 : seq_len + 1]  # Skip decoder_start_id

            # Remove trailing PAD and EOS tokens
            clean_tokens = []
            for tok in seq_tokens:
                if tok == eos_token_id or tok == pad_token_id:
                    break
                clean_tokens.append(tok)

            # Decode to text
            output_text = self.tokenizer.decode(clean_tokens, skip_special_tokens=True)
            total_tokens += len(clean_tokens)

            results.append(
                TranslationResult(
                    text=output_text.strip(),
                    source_lang=src_lang or "auto",
                    target_lang=tgt_lang,
                    latency_ms=total_latency_ms / batch_size,  # Average per text
                    tokens_generated=len(clean_tokens),
                ),
            )

        # Calculate throughput
        throughput_texts = (batch_size / total_latency_ms) * 1000
        throughput_tokens = (total_tokens / total_latency_ms) * 1000

        # Estimate sequential time for speedup calculation
        # This is a rough estimate: batch latency / batch_size gives ~per-text time
        # Sequential would be batch_size * (batch_latency / batch_size) but with
        # additional overhead for each separate call
        estimated_sequential_ms = sum(r.tokens_generated for r in results) * 10  # ~10ms/token baseline
        speedup = estimated_sequential_ms / total_latency_ms if total_latency_ms > 0 else 1.0

        return BatchTranslationResult(
            results=results,
            total_latency_ms=total_latency_ms,
            throughput_texts_per_second=throughput_texts,
            throughput_tokens_per_second=throughput_tokens,
            batch_size=batch_size,
            speedup_vs_sequential=speedup,
        )

    def translate_speculative(
        self,
        text: str,
        tgt_lang: str,
        src_lang: str | None = None,
        max_tokens: int = 256,
        num_draft_tokens: int = 4,
        early_exit_layers: int | None = None,
    ) -> SpeculativeTranslationResult:
        """
        Translate text using early-exit speculative decoding for faster inference.

        Uses first N decoder layers as a "draft" model to speculate multiple tokens,
        then verifies them with the full model. Accepted tokens are kept, rejected
        tokens trigger re-generation from the last accepted position.

        This can provide 1.3-2x speedup when acceptance rate is high (>60%).

        For best results, load a trained early_exit_head first using
        load_early_exit_head(). Without a trained head, acceptance rates are low
        (typically 1-7%) and speculative decoding is slower than baseline.

        Args:
            text: Text to translate
            tgt_lang: Target language code (e.g., "fr", "de", "zh")
            src_lang: Source language (auto-detected if not specified)
            max_tokens: Maximum tokens to generate
            num_draft_tokens: Number of tokens to draft per iteration
            early_exit_layers: Number of decoder layers for draft.
                             If None, uses self.early_exit_layer (from loaded head) or 4.

        Returns:
            SpeculativeTranslationResult with translation and speculative stats
        """
        if not self._loaded:
            self.load()

        assert self.tokenizer is not None, "Tokenizer not loaded"
        assert self.model is not None, "Model not loaded"

        # Use loaded head's exit layer if available
        if early_exit_layers is None:
            early_exit_layers = self.early_exit_layer or 4

        # Apply Hebrew-specific preprocessing for 3B model
        text = self._preprocess_hebrew(text, tgt_lang)

        # MADLAD uses <2xx> prefix for target language
        input_text = f"<2{tgt_lang}> {text}"

        # Tokenize
        inputs = self.tokenizer(input_text, return_tensors="np")
        input_ids = mx.array(inputs["input_ids"])

        # Encode once (shared between draft and full model)
        start_time = time.time()
        encoder_output = self.model.encode(input_ids)
        mx.eval(encoder_output)

        # Initialize
        decoder_start_id = getattr(self.model.config, "decoder_start_token_id", 0)
        eos_token_id = self.tokenizer.eos_token_id

        generated = [decoder_start_id]
        full_cache = None
        draft_cache = None

        # Stats for measuring speculative performance
        accepted_tokens = 0
        total_draft_tokens = 0

        # Initial decode with full model to prime cache
        decoder_ids = mx.array([generated])
        logits, full_cache = self.model.decode(
            decoder_ids, encoder_output, cache=None,
        )
        mx.eval(logits, full_cache)

        # Speculative decoding loop
        while len(generated) - 1 < max_tokens:
            # How many draft tokens to generate this iteration
            remaining = max_tokens - (len(generated) - 1)
            num_draft = min(num_draft_tokens, remaining)

            if num_draft == 0:
                break

            # 1. Generate draft tokens using early-exit decoder
            draft_tokens = []
            last_token = generated[-1]

            # Initialize draft cache from first N layers of full cache
            # Use deep copy to avoid sharing mutable state (OPT-5 compatibility)
            if full_cache is not None:
                draft_cache = copy_cache_list(full_cache[:early_exit_layers])

            for _ in range(num_draft):
                decoder_ids = mx.array([[last_token]])
                draft_logits, draft_cache = self.model.decode_early_exit(
                    decoder_ids,
                    encoder_output,
                    cache=draft_cache,
                    num_layers=early_exit_layers,
                    early_exit_head=self.early_exit_head,  # Use trained head if loaded
                )
                mx.eval(draft_logits, draft_cache)
                next_token = int(mx.argmax(draft_logits[0, -1]))
                draft_tokens.append(next_token)
                last_token = next_token

                if next_token == eos_token_id:
                    break

            total_draft_tokens += len(draft_tokens)

            if not draft_tokens:
                # Generate single token with full model
                decoder_ids = mx.array([[generated[-1]]])
                logits, full_cache = self.model.decode(
                    decoder_ids, encoder_output, cache=full_cache,
                )
                mx.eval(logits, full_cache)
                next_token = int(mx.argmax(logits[0, -1]))
                generated.append(next_token)
                if next_token == eos_token_id:
                    break
                continue

            # 2. Verify all draft tokens with full model in parallel
            # Include the last accepted token + all draft tokens
            verify_tokens = [generated[-1]] + draft_tokens
            verify_ids = mx.array([verify_tokens])

            # Run full model on all tokens at once
            verify_logits, new_full_cache = self.model.decode(
                verify_ids, encoder_output, cache=full_cache,
            )
            mx.eval(verify_logits, new_full_cache)

            # 3. Check which draft tokens match full model predictions
            num_accepted = 0
            for i, draft_token in enumerate(draft_tokens):
                # verify_logits[0, i] gives logits for position after verify_tokens[i]
                full_model_token = int(mx.argmax(verify_logits[0, i]))
                if full_model_token == draft_token:
                    num_accepted += 1
                    generated.append(draft_token)
                    accepted_tokens += 1
                    if draft_token == eos_token_id:
                        break
                else:
                    # First mismatch - use full model's prediction instead
                    generated.append(full_model_token)
                    break

            # 4. Update cache - trim rejected tokens
            num_rejected = len(draft_tokens) - num_accepted
            if num_rejected > 0:
                # We need to trim the cache to only include accepted tokens
                # The full cache was computed for all draft tokens, so trim it
                full_cache = self.model.trim_decoder_cache(
                    new_full_cache, num_rejected,
                )
            else:
                full_cache = new_full_cache

            if generated[-1] == eos_token_id:
                break

        latency_ms = (time.time() - start_time) * 1000

        # Decode output
        output_text = self.tokenizer.decode(generated[1:], skip_special_tokens=True)

        return SpeculativeTranslationResult(
            text=output_text.strip(),
            source_lang=src_lang or "auto",
            target_lang=tgt_lang,
            latency_ms=latency_ms,
            tokens_generated=len(generated) - 1,
            accepted_tokens=accepted_tokens,
            total_draft_tokens=total_draft_tokens,
            acceptance_rate=accepted_tokens / total_draft_tokens
            if total_draft_tokens > 0
            else 0.0,
            speedup=0.0,  # Will be calculated by benchmark
        )

    def translate_ngram(
        self,
        text: str,
        tgt_lang: str,
        src_lang: str | None = None,
        max_tokens: int = 256,
        ngram_n: int = 3,
        max_lookahead: int = 2,
        min_confidence: int = 2,
    ) -> NgramLookaheadResult:
        """
        Translate text using n-gram lookahead decoding (OPT-7).

        Uses patterns from the source text and generated output to predict
        likely continuations. Verifies predictions with the full model in parallel.

        This approach is particularly effective for:
        - Repetitive patterns (technical documents, legal text)
        - Copy mechanisms (source words appearing in output)
        - Common phrases and patterns

        Args:
            text: Text to translate
            tgt_lang: Target language code (e.g., "fr", "de", "zh")
            src_lang: Source language (auto-detected if not specified)
            max_tokens: Maximum tokens to generate
            ngram_n: Size of n-gram prefix for lookahead matching (optimal: 3)
            max_lookahead: Maximum continuation tokens to propose (optimal: 2)
            min_confidence: Minimum times n-gram must be seen before proposing

        Returns:
            NgramLookaheadResult with translation and lookahead statistics
        """
        if not self._loaded:
            self.load()

        assert self.tokenizer is not None, "Tokenizer not loaded"
        assert self.model is not None, "Model not loaded"

        # Apply Hebrew-specific preprocessing for 3B model
        text = self._preprocess_hebrew(text, tgt_lang)

        # MADLAD uses <2xx> prefix for target language
        input_text = f"<2{tgt_lang}> {text}"

        # Tokenize
        inputs = self.tokenizer(input_text, return_tensors="np")
        input_ids = mx.array(inputs["input_ids"])
        source_tokens = input_ids[0].tolist()

        # Build n-gram cache from source tokens with confidence tracking
        ngram_cache = NgramCache(
            n=ngram_n, max_lookahead=max_lookahead, min_confidence=min_confidence,
        )
        ngram_cache.add_sequence(source_tokens)

        # Encode
        start_time = time.time()
        encoder_output = self.model.encode(input_ids)
        mx.eval(encoder_output)

        # Initialize
        decoder_start_id = getattr(self.model.config, "decoder_start_token_id", 0)
        eos_token_id = self.tokenizer.eos_token_id

        generated = [decoder_start_id]
        cache = None

        # Stats for measuring lookahead performance
        accepted_tokens = 0
        total_lookahead_tokens = 0
        ngram_hits = 0

        # Initial decode
        decoder_ids = mx.array([generated])
        logits, cache = self.model.decode(decoder_ids, encoder_output, cache=None)
        mx.eval(logits, cache)

        # Generation loop with n-gram lookahead
        while len(generated) - 1 < max_tokens:
            # Update n-gram cache with generated tokens
            if len(generated) >= ngram_n:
                ngram_cache.add_sequence(generated)

            # Look up continuation in n-gram cache
            lookahead_tokens = ngram_cache.lookup(generated)

            if lookahead_tokens:
                ngram_hits += 1
                total_lookahead_tokens += len(lookahead_tokens)

                # Verify lookahead tokens with full model
                # Include the last token + all lookahead tokens
                verify_tokens = [generated[-1]] + lookahead_tokens
                verify_ids = mx.array([verify_tokens])

                # Run full model on all tokens at once
                verify_logits, new_cache = self.model.decode(
                    verify_ids, encoder_output, cache=cache,
                )
                mx.eval(verify_logits, new_cache)

                # Check which lookahead tokens match full model predictions
                num_accepted = 0
                for i, lookahead_token in enumerate(lookahead_tokens):
                    full_model_token = int(mx.argmax(verify_logits[0, i]))
                    if full_model_token == lookahead_token:
                        num_accepted += 1
                        generated.append(lookahead_token)
                        accepted_tokens += 1
                        if lookahead_token == eos_token_id:
                            break
                    else:
                        # First mismatch - use full model's prediction
                        generated.append(full_model_token)
                        break

                # Update cache - trim rejected tokens
                num_rejected = len(lookahead_tokens) - num_accepted
                if num_rejected > 0:
                    cache = self.model.trim_decoder_cache(new_cache, num_rejected)
                else:
                    cache = new_cache

                if generated[-1] == eos_token_id:
                    break
            else:
                # No n-gram match - generate single token with full model
                decoder_ids = mx.array([[generated[-1]]])
                logits, cache = self.model.decode(
                    decoder_ids, encoder_output, cache=cache,
                )
                mx.eval(logits, cache)
                next_token = int(mx.argmax(logits[0, -1]))
                generated.append(next_token)
                if next_token == eos_token_id:
                    break

        latency_ms = (time.time() - start_time) * 1000

        # Decode output
        output_text = self.tokenizer.decode(generated[1:], skip_special_tokens=True)

        return NgramLookaheadResult(
            text=output_text.strip(),
            source_lang=src_lang or "auto",
            target_lang=tgt_lang,
            latency_ms=latency_ms,
            tokens_generated=len(generated) - 1,
            accepted_tokens=accepted_tokens,
            total_lookahead_tokens=total_lookahead_tokens,
            acceptance_rate=accepted_tokens / total_lookahead_tokens
            if total_lookahead_tokens > 0
            else 0.0,
            ngram_hits=ngram_hits,
            speedup=0.0,  # Will be calculated by benchmark
        )

    def benchmark_ngram(
        self,
        text: str = "Hello, how are you today? I hope you are doing well.",
        tgt_lang: str = "fr",
        iterations: int = 10,
        warmup: int = 3,
        ngram_n: int = 3,
        max_lookahead: int = 2,
        min_confidence: int = 2,
    ) -> dict:
        """
        Benchmark n-gram lookahead vs baseline translation.

        Args:
            text: Text to translate
            tgt_lang: Target language
            iterations: Number of iterations
            warmup: Warmup iterations
            ngram_n: N-gram size for lookahead
            max_lookahead: Max lookahead tokens
            min_confidence: Minimum n-gram confidence threshold

        Returns:
            Dict with baseline and n-gram lookahead benchmark results
        """
        if not self._loaded:
            self.load()

        # Warmup both methods
        for _ in range(warmup):
            _ = self.translate(text, tgt_lang)
            _ = self.translate_ngram(
                text, tgt_lang, ngram_n=ngram_n, max_lookahead=max_lookahead,
                min_confidence=min_confidence,
            )

        # Benchmark baseline
        baseline_latencies = []
        for _ in range(iterations):
            result = self.translate(text, tgt_lang)
            baseline_latencies.append(result.latency_ms)

        # Benchmark n-gram lookahead
        ngram_latencies = []
        acceptance_rates = []
        ngram_hit_counts = []
        for _ in range(iterations):
            result = self.translate_ngram(
                text,
                tgt_lang,
                ngram_n=ngram_n,
                max_lookahead=max_lookahead,
                min_confidence=min_confidence,
            )
            ngram_latencies.append(result.latency_ms)
            acceptance_rates.append(result.acceptance_rate)
            ngram_hit_counts.append(result.ngram_hits)

        baseline_mean = sum(baseline_latencies) / len(baseline_latencies)
        ngram_mean = sum(ngram_latencies) / len(ngram_latencies)

        return {
            "baseline": {
                "mean_latency_ms": baseline_mean,
                "median_latency_ms": sorted(baseline_latencies)[len(baseline_latencies) // 2],
            },
            "ngram_lookahead": {
                "mean_latency_ms": ngram_mean,
                "median_latency_ms": sorted(ngram_latencies)[len(ngram_latencies) // 2],
                "mean_acceptance_rate": sum(acceptance_rates) / len(acceptance_rates)
                if acceptance_rates
                else 0.0,
                "mean_ngram_hits": sum(ngram_hit_counts) / len(ngram_hit_counts)
                if ngram_hit_counts
                else 0.0,
            },
            "speedup": baseline_mean / ngram_mean if ngram_mean > 0 else 0,
            "ngram_n": ngram_n,
            "max_lookahead": max_lookahead,
        }

    def benchmark_speculative(
        self,
        text: str = "Hello, how are you today? I hope you are doing well.",
        tgt_lang: str = "fr",
        iterations: int = 10,
        warmup: int = 3,
        num_draft_tokens: int = 4,
        early_exit_layers: int = 4,
    ) -> dict:
        """
        Benchmark speculative vs baseline translation.

        Args:
            text: Text to translate
            tgt_lang: Target language
            iterations: Number of iterations
            warmup: Warmup iterations
            num_draft_tokens: Draft tokens per speculation
            early_exit_layers: Layers for draft model

        Returns:
            Dict with baseline and speculative benchmark results
        """
        if not self._loaded:
            self.load()

        # Warmup both methods
        for _ in range(warmup):
            _ = self.translate(text, tgt_lang)
            _ = self.translate_speculative(
                text, tgt_lang, num_draft_tokens=num_draft_tokens,
            )

        # Benchmark baseline
        baseline_latencies = []
        for _ in range(iterations):
            result = self.translate(text, tgt_lang)
            baseline_latencies.append(result.latency_ms)

        # Benchmark speculative
        speculative_latencies = []
        acceptance_rates = []
        for _ in range(iterations):
            result = self.translate_speculative(
                text,
                tgt_lang,
                num_draft_tokens=num_draft_tokens,
                early_exit_layers=early_exit_layers,
            )
            speculative_latencies.append(result.latency_ms)
            acceptance_rates.append(result.acceptance_rate)

        baseline_mean = sum(baseline_latencies) / len(baseline_latencies)
        speculative_mean = sum(speculative_latencies) / len(speculative_latencies)

        return {
            "baseline": {
                "mean_latency_ms": baseline_mean,
                "median_latency_ms": sorted(baseline_latencies)[len(baseline_latencies) // 2],
            },
            "speculative": {
                "mean_latency_ms": speculative_mean,
                "median_latency_ms": sorted(speculative_latencies)[
                    len(speculative_latencies) // 2
                ],
                "mean_acceptance_rate": sum(acceptance_rates) / len(acceptance_rates),
            },
            "speedup": baseline_mean / speculative_mean if speculative_mean > 0 else 0,
            "num_draft_tokens": num_draft_tokens,
            "early_exit_layers": early_exit_layers,
        }

    def benchmark(
        self,
        text: str = "Hello, how are you today? I hope you are doing well.",
        tgt_lang: str = "fr",
        iterations: int = 10,
        warmup: int = 3,
    ) -> BenchmarkResult:
        """
        Benchmark translation latency.

        Args:
            text: Text to translate
            tgt_lang: Target language
            iterations: Number of iterations to measure
            warmup: Number of warmup iterations

        Returns:
            BenchmarkResult with latency statistics
        """
        if not self._loaded:
            self.load()

        # Warmup
        for _ in range(warmup):
            _ = self.translate(text, tgt_lang)

        # Benchmark
        latencies = []
        total_tokens = 0

        for _ in range(iterations):
            result = self.translate(text, tgt_lang)
            latencies.append(result.latency_ms)
            total_tokens += result.tokens_generated

        latencies.sort()

        return BenchmarkResult(
            median_latency_ms=latencies[len(latencies) // 2],
            mean_latency_ms=sum(latencies) / len(latencies),
            min_latency_ms=min(latencies),
            max_latency_ms=max(latencies),
            tokens_per_second=(total_tokens / sum(latencies)) * 1000,
        )

    def benchmark_continuous_batching(
        self,
        texts: list[str] | None = None,
        tgt_lang: str = "fr",
        iterations: int = 5,
        warmup: int = 2,
    ) -> dict:
        """
        Benchmark continuous batching vs sequential translation (OPT-6).

        Compares:
        1. Sequential: translate texts one by one
        2. Continuous batching: translate all texts in parallel

        Args:
            texts: List of texts to translate. If None, uses default test set.
            tgt_lang: Target language
            iterations: Number of iterations to measure
            warmup: Warmup iterations

        Returns:
            Dict with sequential vs batched comparison and throughput metrics
        """
        if not self._loaded:
            self.load()

        # Default test texts with varying lengths
        if texts is None:
            texts = [
                "Hello world.",
                "How are you today? I hope everything is going well.",
                "The quick brown fox jumps over the lazy dog.",
                "Machine translation has improved significantly with neural networks.",
                "This is a longer sentence that will help us understand the performance characteristics of continuous batching versus sequential translation.",
                "Good morning!",
                "What time is it?",
                "I love programming in Python.",
            ]

        batch_size = len(texts)

        # Warmup
        for _ in range(warmup):
            _ = self.translate_batch(texts, tgt_lang)
            _ = self.translate_batch_continuous(texts, tgt_lang)

        # Benchmark sequential translation
        sequential_latencies = []
        sequential_tokens = []
        for _ in range(iterations):
            start = time.time()
            results = self.translate_batch(texts, tgt_lang)  # Sequential internally
            latency = (time.time() - start) * 1000
            sequential_latencies.append(latency)
            sequential_tokens.append(sum(r.tokens_generated for r in results))

        # Benchmark continuous batching
        continuous_latencies = []
        continuous_tokens = []
        for _ in range(iterations):
            result = self.translate_batch_continuous(texts, tgt_lang)
            continuous_latencies.append(result.total_latency_ms)
            continuous_tokens.append(
                sum(r.tokens_generated for r in result.results),
            )

        # Calculate statistics
        seq_mean = sum(sequential_latencies) / len(sequential_latencies)
        cont_mean = sum(continuous_latencies) / len(continuous_latencies)
        seq_tokens_mean = sum(sequential_tokens) / len(sequential_tokens)
        cont_tokens_mean = sum(continuous_tokens) / len(continuous_tokens)

        seq_throughput_texts = (batch_size / seq_mean) * 1000
        cont_throughput_texts = (batch_size / cont_mean) * 1000
        seq_throughput_tokens = (seq_tokens_mean / seq_mean) * 1000
        cont_throughput_tokens = (cont_tokens_mean / cont_mean) * 1000

        return {
            "batch_size": batch_size,
            "sequential": {
                "mean_latency_ms": seq_mean,
                "median_latency_ms": sorted(sequential_latencies)[
                    len(sequential_latencies) // 2
                ],
                "throughput_texts_per_second": seq_throughput_texts,
                "throughput_tokens_per_second": seq_throughput_tokens,
                "mean_tokens_generated": seq_tokens_mean,
            },
            "continuous_batching": {
                "mean_latency_ms": cont_mean,
                "median_latency_ms": sorted(continuous_latencies)[
                    len(continuous_latencies) // 2
                ],
                "throughput_texts_per_second": cont_throughput_texts,
                "throughput_tokens_per_second": cont_throughput_tokens,
                "mean_tokens_generated": cont_tokens_mean,
            },
            "speedup_latency": seq_mean / cont_mean if cont_mean > 0 else 0,
            "speedup_throughput": cont_throughput_texts / seq_throughput_texts
            if seq_throughput_texts > 0
            else 0,
        }

    @staticmethod
    def list_supported_languages() -> dict:
        """
        List commonly used language codes.

        MADLAD supports 400+ languages. This returns a subset of common ones.

        Returns:
            Dict mapping language code to language name
        """
        return MADLAD_LANGUAGES.copy()

    @staticmethod
    def list_supported_models() -> list[str]:
        """
        List known supported MADLAD model variants.

        Returns:
            List of HuggingFace model paths
        """
        return [
            "google/madlad400-3b-mt",  # 3B params, best quality
            "google/madlad400-7b-mt",  # 7B params
            "google/madlad400-10b-mt",  # 10B params
        ]
