/opt/homebrew/lib/python3.14/site-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
Train: 19734 samples
Val: 2193 samples
Cache-only mode: 9409/19734 samples (47.7% cached)
Cache-only mode: 0/2193 samples (0.0% cached)
WARNING: No cached validation samples. Validation will be skipped.
Using cached encoder features from: data/v3_multitask/encoder_cache
Prosody features: ENABLED (from cache)
Model parameters:
  Total: 4,769,288
  Encoder LoRA: 1,474,560
  Classification head: 3,294,728
  Converted model parameters to bfloat16
Loading weights from checkpoints/rich_decoder_v4/epoch_1.npz
  Loaded 78 weight tensors

============================================================
Training RichDecoder v4 (Optimized)
============================================================
  Epochs: 10
  Batch size: 4 x 4 = 16
  Total steps: 23530
  bfloat16: True
  Prefetch: 4 batches
  Early stopping: True (patience=3)

Epoch 1/10
  Step 50: loss=0.0063, acc=100.00%, lr=5.00e-06
  Step 100: loss=0.0684, acc=100.00%, lr=1.00e-05
  Step 150: loss=0.7661, acc=75.00%, lr=1.50e-05
  Step 200: loss=0.0937, acc=100.00%, lr=2.00e-05
  Step 250: loss=0.0234, acc=100.00%, lr=2.50e-05
  Step 300: loss=0.3122, acc=100.00%, lr=3.00e-05
  Step 350: loss=0.0088, acc=100.00%, lr=3.50e-05
  Step 400: loss=0.2632, acc=75.00%, lr=4.00e-05
  Step 450: loss=0.6565, acc=75.00%, lr=4.50e-05
  Step 500: loss=1.1755, acc=50.00%, lr=5.00e-05
  Step 550: loss=0.1123, acc=100.00%, lr=5.00e-05
  Step 600: loss=0.9060, acc=50.00%, lr=5.00e-05
  Step 650: loss=0.9089, acc=75.00%, lr=5.00e-05
  Step 700: loss=0.7212, acc=75.00%, lr=5.00e-05
  Step 750: loss=1.2430, acc=50.00%, lr=5.00e-05
  Step 800: loss=0.7960, acc=75.00%, lr=5.00e-05
  Step 850: loss=0.4788, acc=75.00%, lr=5.00e-05
  Step 900: loss=0.6609, acc=50.00%, lr=5.00e-05
  Step 950: loss=0.0023, acc=100.00%, lr=5.00e-05
  Step 1000: loss=1.2415, acc=50.00%, lr=4.99e-05
  Step 1050: loss=0.3122, acc=75.00%, lr=4.99e-05
  Step 1100: loss=0.5470, acc=75.00%, lr=4.99e-05
  Step 1150: loss=0.4734, acc=100.00%, lr=4.99e-05
  Step 1200: loss=0.0002, acc=100.00%, lr=4.99e-05
  Step 1250: loss=0.4508, acc=75.00%, lr=4.99e-05
  Step 1300: loss=0.5331, acc=75.00%, lr=4.99e-05
  Step 1350: loss=0.8819, acc=50.00%, lr=4.98e-05
  Step 1400: loss=0.1881, acc=100.00%, lr=4.98e-05
  Step 1450: loss=1.3179, acc=25.00%, lr=4.98e-05
  Step 1500: loss=0.2069, acc=100.00%, lr=4.98e-05
  Step 1550: loss=0.7786, acc=50.00%, lr=4.97e-05
  Step 1600: loss=0.0060, acc=100.00%, lr=4.97e-05
  Step 1650: loss=0.1574, acc=100.00%, lr=4.97e-05
  Step 1700: loss=0.0089, acc=100.00%, lr=4.97e-05
  Step 1750: loss=0.8623, acc=75.00%, lr=4.96e-05
  Step 1800: loss=0.5287, acc=75.00%, lr=4.96e-05
  Step 1850: loss=0.4144, acc=75.00%, lr=4.96e-05
  Step 1900: loss=1.3044, acc=50.00%, lr=4.96e-05
  Step 1950: loss=0.0760, acc=100.00%, lr=4.95e-05
  Step 2000: loss=0.5538, acc=75.00%, lr=4.95e-05
  Step 2050: loss=1.6048, acc=50.00%, lr=4.95e-05
  Step 2100: loss=0.0002, acc=100.00%, lr=4.94e-05
  Step 2150: loss=0.2483, acc=75.00%, lr=4.94e-05
  Step 2200: loss=0.4934, acc=75.00%, lr=4.93e-05
  Step 2250: loss=0.3549, acc=75.00%, lr=4.93e-05
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1014, in <module>
    main()
    ~~~~^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1010, in main
    trainer.train(train_loader, val_loader)
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 798, in train
    self.optimizer.update(self.model, accumulated_grads)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 29, in update
    model.update(self.apply_gradients(gradients, model))
                 ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 109, in apply_gradients
    return tree_map(self.apply_single, gradients, parameters, self.state)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 2 more times]
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 58, in tree_map
    return fn(tree, *rest)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 586, in apply_single
    return super().apply_single(
           ~~~~~~~~~~~~~~~~~~~~^
        gradient, parameter * (1 - lr * self.weight_decay), state
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 535, in apply_single
    return parameter - lr * m / (mx.sqrt(v) + eps)
                                 ~~~~~~~~~~~^~~~~
RuntimeError: [metal::malloc] Resource limit (499000) exceeded.
