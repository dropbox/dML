/opt/homebrew/lib/python3.14/site-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
Train: 19734 samples
Val: 2193 samples
Cache-only mode: 9409/19734 samples (47.7% cached)
Cache-only mode: 0/2193 samples (0.0% cached)
WARNING: No cached validation samples. Validation will be skipped.
Using cached encoder features from: data/v3_multitask/encoder_cache
Prosody features: ENABLED (from cache)
Model parameters:
  Total: 4,769,288
  Encoder LoRA: 1,474,560
  Classification head: 3,294,728
  Converted model parameters to bfloat16
Loading weights from checkpoints/rich_decoder_v4/epoch_1.npz
  Loaded 78 weight tensors

============================================================
Training RichDecoder v4 (Optimized)
============================================================
  Epochs: 10
  Batch size: 2 x 8 = 16
  Total steps: 47050
  bfloat16: True
  Prefetch: 4 batches
  Early stopping: True (patience=3)

Epoch 1/10
  Step 50: loss=1.7464, acc=0.00%, lr=5.00e-06
  Step 100: loss=1.2326, acc=50.00%, lr=1.00e-05
  Step 150: loss=0.0003, acc=100.00%, lr=1.50e-05
  Step 200: loss=1.3151, acc=50.00%, lr=2.00e-05
  Step 250: loss=0.4403, acc=100.00%, lr=2.50e-05
  Step 300: loss=0.1463, acc=100.00%, lr=3.00e-05
  Step 350: loss=0.1235, acc=100.00%, lr=3.50e-05
  Step 400: loss=0.0091, acc=100.00%, lr=4.00e-05
  Step 450: loss=0.8736, acc=50.00%, lr=4.50e-05
  Step 500: loss=0.0183, acc=100.00%, lr=5.00e-05
  Step 550: loss=0.5416, acc=50.00%, lr=5.00e-05
  Step 600: loss=0.6813, acc=50.00%, lr=5.00e-05
  Step 650: loss=0.0233, acc=100.00%, lr=5.00e-05
  Step 700: loss=0.0775, acc=100.00%, lr=5.00e-05
  Step 750: loss=0.6807, acc=100.00%, lr=5.00e-05
  Step 800: loss=1.1267, acc=50.00%, lr=5.00e-05
  Step 850: loss=0.1975, acc=100.00%, lr=5.00e-05
  Step 900: loss=2.2463, acc=0.00%, lr=5.00e-05
  Step 950: loss=0.0381, acc=100.00%, lr=5.00e-05
  Step 1000: loss=1.1668, acc=50.00%, lr=5.00e-05
  Step 1050: loss=2.1085, acc=0.00%, lr=5.00e-05
  Step 1100: loss=1.4602, acc=0.00%, lr=5.00e-05
  Step 1150: loss=0.0001, acc=100.00%, lr=5.00e-05
  Step 1200: loss=0.9529, acc=50.00%, lr=5.00e-05
  Step 1250: loss=0.9634, acc=50.00%, lr=5.00e-05
  Step 1300: loss=1.1797, acc=50.00%, lr=5.00e-05
  Step 1350: loss=0.5971, acc=100.00%, lr=5.00e-05
  Step 1400: loss=0.4622, acc=50.00%, lr=5.00e-05
  Step 1450: loss=0.0250, acc=100.00%, lr=4.99e-05
  Step 1500: loss=0.1043, acc=100.00%, lr=4.99e-05
  Step 1550: loss=0.4012, acc=100.00%, lr=4.99e-05
  Step 1600: loss=1.7812, acc=0.00%, lr=4.99e-05
  Step 1650: loss=0.0002, acc=100.00%, lr=4.99e-05
  Step 1700: loss=0.0318, acc=100.00%, lr=4.99e-05
  Step 1750: loss=0.9374, acc=50.00%, lr=4.99e-05
  Step 1800: loss=1.5716, acc=50.00%, lr=4.99e-05
  Step 1850: loss=0.0015, acc=100.00%, lr=4.99e-05
  Step 1900: loss=0.0001, acc=100.00%, lr=4.99e-05
  Step 1950: loss=1.2585, acc=50.00%, lr=4.99e-05
  Step 2000: loss=1.1621, acc=50.00%, lr=4.99e-05
  Step 2050: loss=0.0001, acc=100.00%, lr=4.99e-05
  Step 2100: loss=1.1085, acc=50.00%, lr=4.99e-05
  Step 2150: loss=0.0010, acc=100.00%, lr=4.98e-05
  Step 2200: loss=1.0115, acc=50.00%, lr=4.98e-05
  Step 2250: loss=0.5479, acc=100.00%, lr=4.98e-05
  Step 2300: loss=1.5647, acc=50.00%, lr=4.98e-05
  Step 2350: loss=1.4781, acc=0.00%, lr=4.98e-05
  Step 2400: loss=0.4500, acc=100.00%, lr=4.98e-05
  Step 2450: loss=0.3201, acc=100.00%, lr=4.98e-05
  Step 2500: loss=0.8505, acc=50.00%, lr=4.98e-05
  Step 2550: loss=0.0420, acc=100.00%, lr=4.98e-05
  Step 2600: loss=0.0002, acc=100.00%, lr=4.98e-05
  Step 2650: loss=1.2147, acc=50.00%, lr=4.97e-05
  Step 2700: loss=0.0002, acc=100.00%, lr=4.97e-05
  Step 2750: loss=0.0012, acc=100.00%, lr=4.97e-05
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1014, in <module>
    main()
    ~~~~^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1010, in main
    trainer.train(train_loader, val_loader)
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 798, in train
    self.optimizer.update(self.model, accumulated_grads)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 29, in update
    model.update(self.apply_gradients(gradients, model))
                 ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 109, in apply_gradients
    return tree_map(self.apply_single, gradients, parameters, self.state)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 51, in tree_map
    return TreeType(*subtrees) if hasattr(tree, "_fields") else TreeType(subtrees)
                                                                ~~~~~~~~^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 48, in <genexpr>
    tree_map(fn, child, *(r[i] for r in rest), is_leaf=is_leaf)
    ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 58, in tree_map
    return fn(tree, *rest)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 587, in apply_single
    gradient, parameter * (1 - lr * self.weight_decay), state
                               ~~~^~~~~~~~~~~~~~~~~~~
RuntimeError: [metal::malloc] Resource limit (499000) exceeded.
