/opt/homebrew/lib/python3.14/site-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
Train: 19734 samples
Val: 2193 samples
Cache-only mode: 10578/19734 samples (53.6% cached)
Cache-only mode: 1232/2193 samples (56.2% cached)
Using cached encoder features from: data/v3_multitask/encoder_cache
Prosody features: ENABLED (from cache)
Model parameters:
  Total: 4,032,008
  Encoder LoRA: 737,280
  Classification head: 3,294,728
  Converted model parameters to bfloat16

============================================================
Training RichDecoder v4 (Optimized)
============================================================
  Epochs: 10
  Batch size: 4 x 4 = 16
  Total steps: 26450
  bfloat16: True
  Prefetch: 4 batches
  Early stopping: True (patience=3)

Epoch 1/10
  Step 50: loss=1.9014, acc=75.00%, lr=5.00e-06
  Step 100: loss=1.9641, acc=25.00%, lr=1.00e-05
  Step 150: loss=2.6004, acc=25.00%, lr=1.50e-05
  Step 200: loss=0.9798, acc=50.00%, lr=2.00e-05
  Step 250: loss=1.8919, acc=0.00%, lr=2.50e-05
  Step 300: loss=1.6570, acc=50.00%, lr=3.00e-05
  Step 350: loss=1.4930, acc=50.00%, lr=3.50e-05
  Step 400: loss=1.3227, acc=75.00%, lr=4.00e-05
  Step 450: loss=1.4258, acc=75.00%, lr=4.50e-05
  Step 500: loss=1.3225, acc=50.00%, lr=5.00e-05
  Step 550: loss=1.7052, acc=50.00%, lr=5.00e-05
  Step 600: loss=1.2948, acc=50.00%, lr=5.00e-05
  Step 650: loss=2.1322, acc=0.00%, lr=5.00e-05
  Step 700: loss=0.9077, acc=100.00%, lr=5.00e-05
  Step 750: loss=0.6986, acc=75.00%, lr=5.00e-05
  Step 800: loss=1.8053, acc=0.00%, lr=5.00e-05
  Step 850: loss=0.5560, acc=75.00%, lr=5.00e-05
  Step 900: loss=1.8141, acc=25.00%, lr=5.00e-05
  Step 950: loss=0.1832, acc=100.00%, lr=5.00e-05
  Step 1000: loss=1.0556, acc=50.00%, lr=5.00e-05
  Validation: loss=0.7933, acc=65.58%
  New best model! acc=65.58%
  Step 1050: loss=0.5312, acc=100.00%, lr=4.99e-05
  Step 1100: loss=0.9988, acc=50.00%, lr=4.99e-05
  Step 1150: loss=0.9805, acc=50.00%, lr=4.99e-05
  Step 1200: loss=1.0044, acc=50.00%, lr=4.99e-05
  Step 1250: loss=1.0859, acc=50.00%, lr=4.99e-05
  Step 1300: loss=0.4505, acc=100.00%, lr=4.99e-05
  Step 1350: loss=1.4936, acc=25.00%, lr=4.99e-05
  Step 1400: loss=0.9983, acc=75.00%, lr=4.99e-05
  Step 1450: loss=0.7558, acc=100.00%, lr=4.98e-05
  Step 1500: loss=1.6594, acc=25.00%, lr=4.98e-05
  Step 1550: loss=0.8776, acc=75.00%, lr=4.98e-05
  Step 1600: loss=1.3120, acc=50.00%, lr=4.98e-05
  Step 1650: loss=0.5028, acc=75.00%, lr=4.98e-05
  Step 1700: loss=0.2989, acc=100.00%, lr=4.97e-05
  Step 1750: loss=0.5233, acc=75.00%, lr=4.97e-05
  Step 1800: loss=1.2455, acc=75.00%, lr=4.97e-05
  Step 1850: loss=0.8971, acc=75.00%, lr=4.97e-05
  Step 1900: loss=0.5889, acc=75.00%, lr=4.96e-05
  Step 1950: loss=0.6780, acc=75.00%, lr=4.96e-05
  Step 2000: loss=0.9298, acc=50.00%, lr=4.96e-05
  Validation: loss=0.7444, acc=69.81%
  New best model! acc=69.81%
  Step 2050: loss=1.0597, acc=75.00%, lr=4.96e-05
  Step 2100: loss=0.8267, acc=75.00%, lr=4.95e-05
  Step 2150: loss=1.3119, acc=50.00%, lr=4.95e-05
  Step 2200: loss=0.5490, acc=75.00%, lr=4.95e-05
  Step 2250: loss=0.5072, acc=75.00%, lr=4.95e-05
  Step 2300: loss=0.4762, acc=100.00%, lr=4.94e-05
  Step 2350: loss=0.2363, acc=100.00%, lr=4.94e-05
  Step 2400: loss=0.4897, acc=75.00%, lr=4.94e-05
  Step 2450: loss=1.3017, acc=25.00%, lr=4.93e-05
  Step 2500: loss=0.5439, acc=75.00%, lr=4.93e-05
  Step 2550: loss=0.5367, acc=100.00%, lr=4.92e-05
  Step 2600: loss=1.2772, acc=25.00%, lr=4.92e-05
  Epoch 1 complete: avg_loss=0.9906, time=451.4s
  Validation: loss=0.7611, acc=70.70%
  New best model! acc=70.70%
Epoch 2/10
  Step 2650: loss=0.7949, acc=75.00%, lr=4.92e-05
  Step 2700: loss=0.0277, acc=100.00%, lr=4.91e-05
  Step 2750: loss=1.3530, acc=25.00%, lr=4.91e-05
  Step 2800: loss=0.0157, acc=100.00%, lr=4.91e-05
  Step 2850: loss=1.0699, acc=50.00%, lr=4.90e-05
  Step 2900: loss=0.4714, acc=75.00%, lr=4.90e-05
  Step 2950: loss=0.7200, acc=75.00%, lr=4.89e-05
  Step 3000: loss=0.5143, acc=75.00%, lr=4.89e-05
  Validation: loss=0.7225, acc=69.89%
  Step 3050: loss=1.4134, acc=25.00%, lr=4.88e-05
  Step 3100: loss=1.2059, acc=50.00%, lr=4.88e-05
  Step 3150: loss=0.0703, acc=100.00%, lr=4.87e-05
  Step 3200: loss=0.0442, acc=100.00%, lr=4.87e-05
  Step 3250: loss=0.4422, acc=75.00%, lr=4.87e-05
  Step 3300: loss=0.4036, acc=75.00%, lr=4.86e-05
  Step 3350: loss=0.4696, acc=75.00%, lr=4.86e-05
  Step 3400: loss=1.0723, acc=75.00%, lr=4.85e-05
  Step 3450: loss=1.4126, acc=50.00%, lr=4.85e-05
  Step 3500: loss=0.1866, acc=100.00%, lr=4.84e-05
  Step 3550: loss=1.2712, acc=50.00%, lr=4.83e-05
  Step 3600: loss=0.1237, acc=100.00%, lr=4.83e-05
  Step 3650: loss=1.0585, acc=75.00%, lr=4.82e-05
  Step 3700: loss=1.1205, acc=50.00%, lr=4.82e-05
  Step 3750: loss=0.7651, acc=75.00%, lr=4.81e-05
  Step 3800: loss=0.4766, acc=75.00%, lr=4.81e-05
  Step 3850: loss=1.4422, acc=75.00%, lr=4.80e-05
  Step 3900: loss=1.8936, acc=25.00%, lr=4.80e-05
  Step 3950: loss=0.7704, acc=75.00%, lr=4.79e-05
  Step 4000: loss=0.2387, acc=75.00%, lr=4.78e-05
  Validation: loss=0.7134, acc=71.27%
  New best model! acc=71.27%
  Step 4050: loss=1.6671, acc=50.00%, lr=4.78e-05
  Step 4100: loss=1.5827, acc=25.00%, lr=4.77e-05
  Step 4150: loss=0.6268, acc=75.00%, lr=4.76e-05
  Step 4200: loss=1.0624, acc=50.00%, lr=4.76e-05
  Step 4250: loss=0.9384, acc=50.00%, lr=4.75e-05
  Step 4300: loss=1.2465, acc=50.00%, lr=4.75e-05
  Step 4350: loss=0.0454, acc=100.00%, lr=4.74e-05
  Step 4400: loss=0.5000, acc=75.00%, lr=4.73e-05
  Step 4450: loss=1.1905, acc=100.00%, lr=4.73e-05
  Step 4500: loss=0.4288, acc=75.00%, lr=4.72e-05
  Step 4550: loss=0.3828, acc=100.00%, lr=4.71e-05
  Step 4600: loss=0.4914, acc=75.00%, lr=4.70e-05
  Step 4650: loss=1.3202, acc=50.00%, lr=4.70e-05
  Step 4700: loss=1.5559, acc=50.00%, lr=4.69e-05
  Step 4750: loss=1.2459, acc=50.00%, lr=4.68e-05
  Step 4800: loss=0.0647, acc=100.00%, lr=4.68e-05
  Step 4850: loss=0.1345, acc=100.00%, lr=4.67e-05
  Step 4900: loss=1.0136, acc=50.00%, lr=4.66e-05
  Step 4950: loss=0.4407, acc=75.00%, lr=4.65e-05
  Step 5000: loss=0.8457, acc=75.00%, lr=4.65e-05
  Validation: loss=0.7133, acc=68.91%
  Step 5050: loss=0.5331, acc=75.00%, lr=4.64e-05
  Step 5100: loss=1.0193, acc=50.00%, lr=4.63e-05
  Step 5150: loss=1.0630, acc=25.00%, lr=4.62e-05
  Step 5200: loss=0.1330, acc=100.00%, lr=4.61e-05
  Step 5250: loss=1.0860, acc=50.00%, lr=4.61e-05
  Epoch 2 complete: avg_loss=0.7639, time=399.0s
  Validation: loss=0.6969, acc=70.37%
Epoch 3/10
  Step 5300: loss=0.4282, acc=75.00%, lr=4.60e-05
  Step 5350: loss=1.1358, acc=75.00%, lr=4.59e-05
  Step 5400: loss=0.4927, acc=75.00%, lr=4.58e-05
  Step 5450: loss=1.3564, acc=50.00%, lr=4.57e-05
  Step 5500: loss=0.0168, acc=100.00%, lr=4.56e-05
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1018, in <module>
    main()
    ~~~~^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1014, in main
    trainer.train(train_loader, val_loader)
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 800, in train
    self.optimizer.update(self.model, accumulated_grads)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 29, in update
    model.update(self.apply_gradients(gradients, model))
                 ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 109, in apply_gradients
    return tree_map(self.apply_single, gradients, parameters, self.state)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 2 more times]
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 58, in tree_map
    return fn(tree, *rest)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 586, in apply_single
    return super().apply_single(
           ~~~~~~~~~~~~~~~~~~~~^
        gradient, parameter * (1 - lr * self.weight_decay), state
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 524, in apply_single
    v = b2 * v + (1 - b2) * mx.square(gradient)
        ~~~^~~
RuntimeError: [metal::malloc] Resource limit (499000) exceeded.
