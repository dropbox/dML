2026-01-07 19:31:27,787 [INFO] Acquired training lock
2026-01-07 19:31:27,788 [INFO] Loading encoder from checkpoints/zipformer/en-streaming/exp/pretrained.pt
2026-01-07 19:31:27,867 [INFO] Encoder loaded: output_dim=512
2026-01-07 19:31:27,867 [INFO] Loading data from data/paralinguistics/vocalsound_labeled (dataset=crema-d)
2026-01-07 19:31:29,043 [INFO] Train batches: 973
2026-01-07 19:31:29,043 [INFO] Val batches: 224
2026-01-07 19:31:29,043 [INFO] Head num_classes: 6
2026-01-07 19:31:29,045 [INFO] Created paralinguistics head with 988,166 parameters
2026-01-07 19:31:29,045 [INFO] Stage 4: 11,754,487 params (UNFROZEN)
2026-01-07 19:31:29,045 [INFO] Stage 5: 3,906,660 params (UNFROZEN)
2026-01-07 19:31:29,045 [INFO] Total unfrozen encoder params: 15,661,147
2026-01-07 19:31:29,045 [INFO] Total trainable parameters: 16,649,313
2026-01-07 19:31:29,045 [INFO] Fine-tuning with encoder unfreezing: lr=1.5e-05 (encoder_lr parameter is ignored - both encoder and head use same lr)
2026-01-07 19:31:29,045 [INFO] Adaptive LR enabled: will reduce LR by 0.5x if NaN rate > 10%
2026-01-07 19:31:29,045 [INFO] ============================================================
2026-01-07 19:31:29,045 [INFO] Training Configuration
2026-01-07 19:31:29,045 [INFO] ============================================================
2026-01-07 19:31:29,045 [INFO] Head type: paralinguistics
2026-01-07 19:31:29,045 [INFO] Encoder dim: 512
2026-01-07 19:31:29,045 [INFO] Batch size: 16
2026-01-07 19:31:29,045 [INFO] Head learning rate: 1.5e-05
2026-01-07 19:31:29,045 [INFO] Encoder learning rate: 1e-05
2026-01-07 19:31:29,045 [INFO] Unfrozen encoder stages: 2
2026-01-07 19:31:29,045 [INFO] Gradient clipping: 1.0
2026-01-07 19:31:29,045 [INFO] Cache clearing: every 100 steps
2026-01-07 19:31:29,045 [INFO] Label smoothing: 0.1
2026-01-07 19:31:29,045 [INFO] Loss function: Cross-Entropy
2026-01-07 19:31:29,045 [INFO] SpecAugment: True
2026-01-07 19:31:29,045 [INFO] Param dtype: float32
2026-01-07 19:31:29,045 [INFO] Epochs: 20
2026-01-07 19:31:29,045 [INFO] Max steps: 19460
2026-01-07 19:31:29,045 [INFO] Label key: paralinguistic_labels
2026-01-07 19:31:29,045 [INFO] ============================================================
2026-01-07 19:31:29,046 [INFO] Created EncoderHeadModel for fine-tuning (reused across all steps)
2026-01-07 19:31:29,046 [INFO] Epoch 1/20
2026-01-07 19:31:32,164 [INFO] Step 10/19460 | Loss: 1.7962 | Acc: 0.0625 | LR: 2.70e-07
2026-01-07 19:31:35,226 [INFO] Step 20/19460 | Loss: 1.8012 | Acc: 0.0000 | LR: 5.70e-07
2026-01-07 19:31:38,482 [INFO] Step 30/19460 | Loss: 1.7947 | Acc: 0.1250 | LR: 8.70e-07
2026-01-07 19:31:41,514 [INFO] Step 40/19460 | Loss: 1.7925 | Acc: 0.1875 | LR: 1.17e-06
2026-01-07 19:31:44,695 [INFO] Step 50/19460 | Loss: 1.7900 | Acc: 0.2500 | LR: 1.47e-06
2026-01-07 19:31:47,787 [INFO] Step 60/19460 | Loss: 1.7865 | Acc: 0.2500 | LR: 1.77e-06
2026-01-07 19:31:51,023 [INFO] Step 70/19460 | Loss: 1.7881 | Acc: 0.2500 | LR: 2.07e-06
2026-01-07 19:31:54,247 [INFO] Step 80/19460 | Loss: 1.7781 | Acc: 0.3750 | LR: 2.37e-06
2026-01-07 19:31:57,384 [INFO] Step 90/19460 | Loss: 1.7728 | Acc: 0.3750 | LR: 2.67e-06
2026-01-07 19:32:01,049 [INFO] Step 100/19460 | Loss: 1.7809 | Acc: 0.1875 | LR: 2.97e-06
2026-01-07 19:32:09,687 [INFO] [EVAL] Step 100 | Val Loss: 1.7856 | Val Acc: 0.2350
2026-01-07 19:32:09,720 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:32:09,722 [INFO] New best validation accuracy: 0.2350
2026-01-07 19:32:12,819 [INFO] Step 110/19460 | Loss: 1.7879 | Acc: 0.3125 | LR: 3.27e-06
2026-01-07 19:32:12,960 [WARNING] Skipping batch due to non-finite loss at step=110 (loss=nan, epoch=1).
2026-01-07 19:32:16,098 [INFO] Step 120/19460 | Loss: 1.7952 | Acc: 0.2500 | LR: 3.57e-06
2026-01-07 19:32:19,282 [INFO] Step 130/19460 | Loss: 1.7896 | Acc: 0.2500 | LR: 3.87e-06
2026-01-07 19:32:22,394 [INFO] Step 140/19460 | Loss: 1.7855 | Acc: 0.2500 | LR: 4.17e-06
2026-01-07 19:32:25,550 [INFO] Step 150/19460 | Loss: 1.7898 | Acc: 0.2500 | LR: 4.47e-06
2026-01-07 19:32:28,825 [INFO] Step 160/19460 | Loss: 1.7637 | Acc: 0.4375 | LR: 4.77e-06
2026-01-07 19:32:31,879 [INFO] Step 170/19460 | Loss: 1.7477 | Acc: 0.4375 | LR: 5.07e-06
2026-01-07 19:32:34,889 [INFO] Step 180/19460 | Loss: 1.7768 | Acc: 0.1250 | LR: 5.37e-06
2026-01-07 19:32:37,873 [INFO] Step 190/19460 | Loss: 1.7117 | Acc: 0.4375 | LR: 5.67e-06
2026-01-07 19:32:41,579 [INFO] Step 200/19460 | Loss: 1.7946 | Acc: 0.0625 | LR: 5.97e-06
2026-01-07 19:32:50,179 [INFO] [EVAL] Step 200 | Val Loss: 1.7447 | Val Acc: 0.2687
2026-01-07 19:32:50,215 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:32:50,217 [INFO] New best validation accuracy: 0.2687
2026-01-07 19:32:53,502 [INFO] Step 210/19460 | Loss: 1.7555 | Acc: 0.3125 | LR: 6.27e-06
2026-01-07 19:32:56,634 [INFO] Step 220/19460 | Loss: 1.7525 | Acc: 0.3750 | LR: 6.57e-06
2026-01-07 19:32:59,833 [INFO] Step 230/19460 | Loss: 1.7890 | Acc: 0.3125 | LR: 6.87e-06
2026-01-07 19:33:02,848 [INFO] Step 240/19460 | Loss: 1.7488 | Acc: 0.1875 | LR: 7.17e-06
2026-01-07 19:33:06,154 [INFO] Step 250/19460 | Loss: 1.7429 | Acc: 0.2500 | LR: 7.47e-06
2026-01-07 19:33:09,203 [INFO] Step 260/19460 | Loss: 1.7884 | Acc: 0.2500 | LR: 7.77e-06
2026-01-07 19:33:12,336 [INFO] Step 270/19460 | Loss: 1.6698 | Acc: 0.4375 | LR: 8.07e-06
2026-01-07 19:33:15,378 [INFO] Step 280/19460 | Loss: 1.6380 | Acc: 0.2500 | LR: 8.37e-06
2026-01-07 19:33:18,487 [INFO] Step 290/19460 | Loss: 1.5454 | Acc: 0.5000 | LR: 8.67e-06
2026-01-07 19:33:21,972 [INFO] Step 300/19460 | Loss: 1.6331 | Acc: 0.4375 | LR: 8.97e-06
2026-01-07 19:33:30,573 [INFO] [EVAL] Step 300 | Val Loss: 1.5309 | Val Acc: 0.3894
2026-01-07 19:33:30,608 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:33:30,610 [INFO] New best validation accuracy: 0.3894
2026-01-07 19:33:33,746 [INFO] Step 310/19460 | Loss: 1.5697 | Acc: 0.5000 | LR: 9.27e-06
2026-01-07 19:33:36,699 [INFO] Step 320/19460 | Loss: 1.4818 | Acc: 0.5000 | LR: 9.57e-06
2026-01-07 19:33:39,706 [INFO] Step 330/19460 | Loss: 1.5218 | Acc: 0.3750 | LR: 9.87e-06
2026-01-07 19:33:42,837 [INFO] Step 340/19460 | Loss: 1.5833 | Acc: 0.2500 | LR: 1.02e-05
2026-01-07 19:33:45,899 [INFO] Step 350/19460 | Loss: 1.6251 | Acc: 0.3125 | LR: 1.05e-05
2026-01-07 19:33:49,068 [INFO] Step 360/19460 | Loss: 1.4589 | Acc: 0.5000 | LR: 1.08e-05
2026-01-07 19:33:52,122 [INFO] Step 370/19460 | Loss: 1.4561 | Acc: 0.5000 | LR: 1.11e-05
2026-01-07 19:33:55,340 [INFO] Step 380/19460 | Loss: 1.4941 | Acc: 0.4375 | LR: 1.14e-05
2026-01-07 19:33:58,482 [INFO] Step 390/19460 | Loss: 1.4348 | Acc: 0.5625 | LR: 1.17e-05
2026-01-07 19:34:01,916 [INFO] Step 400/19460 | Loss: 1.6149 | Acc: 0.3750 | LR: 1.20e-05
2026-01-07 19:34:10,626 [INFO] [EVAL] Step 400 | Val Loss: 1.3136 | Val Acc: 0.5062
2026-01-07 19:34:10,652 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:34:10,654 [INFO] New best validation accuracy: 0.5062
2026-01-07 19:34:13,811 [INFO] Step 410/19460 | Loss: 1.3500 | Acc: 0.5625 | LR: 1.23e-05
2026-01-07 19:34:16,987 [INFO] Step 420/19460 | Loss: 1.7024 | Acc: 0.3750 | LR: 1.26e-05
2026-01-07 19:34:20,175 [INFO] Step 430/19460 | Loss: 1.6254 | Acc: 0.3750 | LR: 1.29e-05
2026-01-07 19:34:23,249 [INFO] Step 440/19460 | Loss: 1.5371 | Acc: 0.5000 | LR: 1.32e-05
2026-01-07 19:34:26,270 [INFO] Step 450/19460 | Loss: 1.4566 | Acc: 0.5625 | LR: 1.35e-05
2026-01-07 19:34:29,345 [INFO] Step 460/19460 | Loss: 1.3801 | Acc: 0.5625 | LR: 1.38e-05
2026-01-07 19:34:32,342 [INFO] Step 470/19460 | Loss: 1.5694 | Acc: 0.5000 | LR: 1.41e-05
2026-01-07 19:34:35,044 [WARNING] Skipping batch due to non-finite loss at step=478 (loss=nan, epoch=1).
2026-01-07 19:34:35,659 [INFO] Step 480/19460 | Loss: 1.5932 | Acc: 0.2500 | LR: 1.44e-05
2026-01-07 19:34:38,789 [INFO] Step 490/19460 | Loss: 1.2121 | Acc: 0.5625 | LR: 1.47e-05
2026-01-07 19:34:42,223 [INFO] Step 500/19460 | Loss: 1.1217 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:34:50,925 [INFO] [EVAL] Step 500 | Val Loss: 1.0432 | Val Acc: 0.6281
2026-01-07 19:34:50,955 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:34:50,957 [INFO] New best validation accuracy: 0.6281
2026-01-07 19:34:50,973 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_500.npz
2026-01-07 19:34:54,059 [INFO] Step 510/19460 | Loss: 1.3315 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:34:57,095 [INFO] Step 520/19460 | Loss: 1.3543 | Acc: 0.3125 | LR: 1.50e-05
2026-01-07 19:35:00,109 [INFO] Step 530/19460 | Loss: 1.5805 | Acc: 0.4375 | LR: 1.50e-05
2026-01-07 19:35:03,109 [INFO] Step 540/19460 | Loss: 1.5144 | Acc: 0.3125 | LR: 1.50e-05
2026-01-07 19:35:06,284 [INFO] Step 550/19460 | Loss: 1.1566 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:35:09,227 [INFO] Step 560/19460 | Loss: 1.0478 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:35:12,206 [INFO] Step 570/19460 | Loss: 1.3269 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:35:15,328 [INFO] Step 580/19460 | Loss: 0.9855 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:35:18,527 [INFO] Step 590/19460 | Loss: 1.3931 | Acc: 0.4375 | LR: 1.50e-05
2026-01-07 19:35:22,140 [INFO] Step 600/19460 | Loss: 1.2375 | Acc: 0.5000 | LR: 1.50e-05
2026-01-07 19:35:30,801 [INFO] [EVAL] Step 600 | Val Loss: 0.9519 | Val Acc: 0.6506
2026-01-07 19:35:30,829 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:35:30,831 [INFO] New best validation accuracy: 0.6506
2026-01-07 19:35:33,988 [INFO] Step 610/19460 | Loss: 1.3044 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:35:37,198 [INFO] Step 620/19460 | Loss: 1.3574 | Acc: 0.3125 | LR: 1.50e-05
2026-01-07 19:35:40,492 [INFO] Step 630/19460 | Loss: 1.2049 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:35:43,445 [INFO] Step 640/19460 | Loss: 1.1429 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:35:46,584 [INFO] Step 650/19460 | Loss: 1.3142 | Acc: 0.4375 | LR: 1.50e-05
2026-01-07 19:35:49,794 [INFO] Step 660/19460 | Loss: 0.8577 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:35:52,833 [INFO] Step 670/19460 | Loss: 1.0304 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:35:55,812 [INFO] Step 680/19460 | Loss: 1.0277 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:35:58,993 [INFO] Step 690/19460 | Loss: 1.3385 | Acc: 0.4375 | LR: 1.50e-05
2026-01-07 19:36:02,477 [INFO] Step 700/19460 | Loss: 1.0609 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:36:11,143 [INFO] [EVAL] Step 700 | Val Loss: 0.8244 | Val Acc: 0.7188
2026-01-07 19:36:11,166 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:36:11,167 [INFO] New best validation accuracy: 0.7188
2026-01-07 19:36:14,318 [INFO] Step 710/19460 | Loss: 1.0605 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:36:17,334 [INFO] Step 720/19460 | Loss: 1.0728 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:36:20,445 [INFO] Step 730/19460 | Loss: 1.0693 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:36:21,535 [WARNING] Skipping batch due to non-finite loss at step=733 (loss=nan, epoch=1).
2026-01-07 19:36:23,664 [INFO] Step 740/19460 | Loss: 0.9093 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:36:26,669 [INFO] Step 750/19460 | Loss: 1.3232 | Acc: 0.4375 | LR: 1.50e-05
2026-01-07 19:36:29,830 [INFO] Step 760/19460 | Loss: 1.3235 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:36:32,872 [INFO] Step 770/19460 | Loss: 1.3273 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:36:34,608 [WARNING] Skipping batch due to non-finite loss at step=775 (loss=nan, epoch=1).
2026-01-07 19:36:36,065 [INFO] Step 780/19460 | Loss: 1.1139 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:36:39,187 [INFO] Step 790/19460 | Loss: 1.0257 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:36:42,206 [WARNING] Skipping batch due to non-finite loss at step=799 (loss=nan, epoch=1).
2026-01-07 19:36:43,028 [INFO] Step 800/19460 | Loss: 1.2450 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:36:51,712 [INFO] [EVAL] Step 800 | Val Loss: 0.7644 | Val Acc: 0.7494
2026-01-07 19:36:51,740 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:36:51,742 [INFO] New best validation accuracy: 0.7494
2026-01-07 19:36:54,850 [INFO] Step 810/19460 | Loss: 0.9826 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:36:58,007 [INFO] Step 820/19460 | Loss: 0.9739 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:37:01,138 [INFO] Step 830/19460 | Loss: 1.1774 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:37:04,390 [INFO] Step 840/19460 | Loss: 1.2146 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:37:07,438 [INFO] Step 850/19460 | Loss: 1.1017 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:37:10,638 [INFO] Step 860/19460 | Loss: 1.0262 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:37:13,844 [INFO] Step 870/19460 | Loss: 0.8557 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:37:17,065 [INFO] Step 880/19460 | Loss: 0.9785 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:37:19,129 [WARNING] Skipping batch due to non-finite loss at step=886 (loss=nan, epoch=1).
2026-01-07 19:37:20,407 [INFO] Step 890/19460 | Loss: 1.2624 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:37:23,973 [INFO] Step 900/19460 | Loss: 1.1588 | Acc: 0.5000 | LR: 1.50e-05
2026-01-07 19:37:32,641 [INFO] [EVAL] Step 900 | Val Loss: 0.7182 | Val Acc: 0.7650
2026-01-07 19:37:32,677 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:37:32,679 [INFO] New best validation accuracy: 0.7650
2026-01-07 19:37:35,891 [INFO] Step 910/19460 | Loss: 0.8735 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:37:39,123 [INFO] Step 920/19460 | Loss: 1.1814 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:37:42,162 [INFO] Step 930/19460 | Loss: 1.0919 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:37:45,447 [INFO] Step 940/19460 | Loss: 1.1669 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:37:48,490 [INFO] Step 950/19460 | Loss: 0.9920 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:37:51,544 [INFO] Step 960/19460 | Loss: 1.1195 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:37:53,167 [INFO] Epoch 1 complete | Avg Loss: 1.4239 | Avg Acc: 0.4690 | Updates: 965 | Micro-batches: 971 | Skipped: 6 (loss=6, logits=0, grads=0)
2026-01-07 19:37:53,167 [INFO] Epoch 2/20
2026-01-07 19:37:54,766 [INFO] Step 970/19460 | Loss: 0.9401 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:37:57,878 [INFO] Step 980/19460 | Loss: 0.7686 | Acc: 0.9375 | LR: 1.50e-05
2026-01-07 19:38:00,989 [INFO] Step 990/19460 | Loss: 1.1085 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:38:04,487 [INFO] Step 1000/19460 | Loss: 1.3095 | Acc: 0.5000 | LR: 1.50e-05
2026-01-07 19:38:13,176 [INFO] [EVAL] Step 1000 | Val Loss: 0.6216 | Val Acc: 0.8119
2026-01-07 19:38:13,199 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:38:13,201 [INFO] New best validation accuracy: 0.8119
2026-01-07 19:38:13,225 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_1000.npz
2026-01-07 19:38:16,415 [INFO] Step 1010/19460 | Loss: 1.2618 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:38:19,493 [INFO] Step 1020/19460 | Loss: 1.1410 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:38:22,571 [INFO] Step 1030/19460 | Loss: 0.8790 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:38:25,652 [INFO] Step 1040/19460 | Loss: 1.2448 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:38:28,645 [INFO] Step 1050/19460 | Loss: 0.7886 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:38:31,784 [INFO] Step 1060/19460 | Loss: 1.3766 | Acc: 0.5000 | LR: 1.50e-05
2026-01-07 19:38:34,816 [INFO] Step 1070/19460 | Loss: 1.1979 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:38:37,907 [INFO] Step 1080/19460 | Loss: 1.1017 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:38:41,143 [INFO] Step 1090/19460 | Loss: 1.2844 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:38:44,795 [INFO] Step 1100/19460 | Loss: 0.9555 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:38:53,429 [INFO] [EVAL] Step 1100 | Val Loss: 0.6832 | Val Acc: 0.7800
2026-01-07 19:38:56,612 [INFO] Step 1110/19460 | Loss: 1.2148 | Acc: 0.5000 | LR: 1.50e-05
2026-01-07 19:38:59,683 [INFO] Step 1120/19460 | Loss: 1.3110 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:39:02,839 [INFO] Step 1130/19460 | Loss: 0.8855 | Acc: 0.8750 | LR: 1.50e-05
2026-01-07 19:39:06,082 [INFO] Step 1140/19460 | Loss: 0.9727 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:39:09,273 [INFO] Step 1150/19460 | Loss: 0.9946 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:39:12,443 [INFO] Step 1160/19460 | Loss: 1.2516 | Acc: 0.5625 | LR: 1.50e-05
2026-01-07 19:39:15,444 [INFO] Step 1170/19460 | Loss: 0.9234 | Acc: 0.8125 | LR: 1.50e-05
2026-01-07 19:39:18,724 [INFO] Step 1180/19460 | Loss: 1.0898 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:39:21,803 [INFO] Step 1190/19460 | Loss: 1.0766 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:39:25,461 [INFO] Step 1200/19460 | Loss: 0.9370 | Acc: 0.7500 | LR: 1.50e-05
2026-01-07 19:39:34,058 [INFO] [EVAL] Step 1200 | Val Loss: 0.5933 | Val Acc: 0.8231
2026-01-07 19:39:34,085 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:39:34,087 [INFO] New best validation accuracy: 0.8231
2026-01-07 19:39:37,352 [INFO] Step 1210/19460 | Loss: 1.0622 | Acc: 0.6250 | LR: 1.50e-05
2026-01-07 19:39:40,480 [INFO] Step 1220/19460 | Loss: 1.0444 | Acc: 0.6875 | LR: 1.50e-05
2026-01-07 19:39:43,672 [INFO] Step 1230/19460 | Loss: 0.9436 | Acc: 0.8750 | LR: 1.49e-05
2026-01-07 19:39:46,926 [INFO] Step 1240/19460 | Loss: 0.8710 | Acc: 0.8750 | LR: 1.49e-05
2026-01-07 19:39:50,038 [INFO] Step 1250/19460 | Loss: 1.0065 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:39:53,080 [INFO] Step 1260/19460 | Loss: 1.5414 | Acc: 0.5000 | LR: 1.49e-05
2026-01-07 19:39:55,935 [WARNING] Skipping batch due to non-finite loss at step=1269 (loss=nan, epoch=2).
2026-01-07 19:39:56,282 [INFO] Step 1270/19460 | Loss: 1.1026 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:39:59,388 [INFO] Step 1280/19460 | Loss: 0.9635 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:40:02,504 [INFO] Step 1290/19460 | Loss: 1.2835 | Acc: 0.6250 | LR: 1.49e-05
2026-01-07 19:40:06,107 [INFO] Step 1300/19460 | Loss: 1.1017 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:40:14,781 [INFO] [EVAL] Step 1300 | Val Loss: 0.6278 | Val Acc: 0.8025
2026-01-07 19:40:18,095 [INFO] Step 1310/19460 | Loss: 0.7041 | Acc: 0.9375 | LR: 1.49e-05
2026-01-07 19:40:19,615 [WARNING] Skipping batch due to non-finite loss at step=1314 (loss=nan, epoch=2).
2026-01-07 19:40:21,405 [INFO] Step 1320/19460 | Loss: 0.9338 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:40:24,462 [INFO] Step 1330/19460 | Loss: 1.0528 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:40:24,635 [WARNING] Skipping batch due to non-finite loss at step=1330 (loss=nan, epoch=2).
2026-01-07 19:40:27,715 [INFO] Step 1340/19460 | Loss: 1.2973 | Acc: 0.4375 | LR: 1.49e-05
2026-01-07 19:40:30,993 [INFO] Step 1350/19460 | Loss: 0.9340 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:40:34,138 [INFO] Step 1360/19460 | Loss: 0.9560 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:40:37,329 [INFO] Step 1370/19460 | Loss: 1.0573 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:40:40,391 [INFO] Step 1380/19460 | Loss: 1.4005 | Acc: 0.5625 | LR: 1.49e-05
2026-01-07 19:40:43,466 [INFO] Step 1390/19460 | Loss: 0.5674 | Acc: 0.8750 | LR: 1.49e-05
2026-01-07 19:40:46,926 [INFO] Step 1400/19460 | Loss: 0.7412 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:40:55,577 [INFO] [EVAL] Step 1400 | Val Loss: 0.5389 | Val Acc: 0.8369
2026-01-07 19:40:55,601 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:40:55,603 [INFO] New best validation accuracy: 0.8369
2026-01-07 19:40:58,829 [INFO] Step 1410/19460 | Loss: 1.0950 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:41:02,057 [INFO] Step 1420/19460 | Loss: 0.8485 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:41:05,168 [INFO] Step 1430/19460 | Loss: 0.8945 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:41:08,129 [WARNING] Skipping batch due to non-finite loss at step=1439 (loss=nan, epoch=2).
2026-01-07 19:41:08,490 [INFO] Step 1440/19460 | Loss: 1.0852 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:41:11,485 [INFO] Step 1450/19460 | Loss: 1.0335 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:41:14,691 [INFO] Step 1460/19460 | Loss: 0.9100 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:41:17,840 [INFO] Step 1470/19460 | Loss: 0.9733 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:41:20,974 [INFO] Step 1480/19460 | Loss: 1.0833 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:41:24,244 [INFO] Step 1490/19460 | Loss: 0.8057 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:41:27,877 [INFO] Step 1500/19460 | Loss: 1.1079 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:41:36,556 [INFO] [EVAL] Step 1500 | Val Loss: 0.5127 | Val Acc: 0.8488
2026-01-07 19:41:36,581 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:41:36,583 [INFO] New best validation accuracy: 0.8488
2026-01-07 19:41:36,604 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_1500.npz
2026-01-07 19:41:39,583 [INFO] Step 1510/19460 | Loss: 0.9621 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:41:42,769 [INFO] Step 1520/19460 | Loss: 1.0685 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:41:43,571 [WARNING] Skipping batch due to non-finite loss at step=1522 (loss=nan, epoch=2).
2026-01-07 19:41:46,002 [INFO] Step 1530/19460 | Loss: 1.2770 | Acc: 0.6250 | LR: 1.49e-05
2026-01-07 19:41:49,063 [INFO] Step 1540/19460 | Loss: 1.1281 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:41:52,150 [INFO] Step 1550/19460 | Loss: 0.8210 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:41:55,245 [INFO] Step 1560/19460 | Loss: 0.8045 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:41:57,639 [WARNING] Skipping batch due to non-finite loss at step=1567 (loss=nan, epoch=2).
2026-01-07 19:41:57,785 [WARNING] Skipping batch due to non-finite loss at step=1567 (loss=nan, epoch=2).
2026-01-07 19:41:58,798 [INFO] Step 1570/19460 | Loss: 0.9509 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:42:01,824 [INFO] Step 1580/19460 | Loss: 0.9327 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:42:04,929 [INFO] Step 1590/19460 | Loss: 1.1748 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:42:08,576 [INFO] Step 1600/19460 | Loss: 0.6479 | Acc: 0.8750 | LR: 1.49e-05
2026-01-07 19:42:17,241 [INFO] [EVAL] Step 1600 | Val Loss: 0.5268 | Val Acc: 0.8444
2026-01-07 19:42:20,546 [INFO] Step 1610/19460 | Loss: 0.7864 | Acc: 0.8750 | LR: 1.49e-05
2026-01-07 19:42:23,634 [INFO] Step 1620/19460 | Loss: 0.6721 | Acc: 0.9375 | LR: 1.49e-05
2026-01-07 19:42:26,755 [INFO] Step 1630/19460 | Loss: 1.1787 | Acc: 0.6250 | LR: 1.49e-05
2026-01-07 19:42:29,960 [INFO] Step 1640/19460 | Loss: 0.8467 | Acc: 0.8125 | LR: 1.49e-05
2026-01-07 19:42:32,989 [INFO] Step 1650/19460 | Loss: 1.0290 | Acc: 0.6250 | LR: 1.49e-05
2026-01-07 19:42:36,221 [INFO] Step 1660/19460 | Loss: 0.9460 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:42:39,295 [INFO] Step 1670/19460 | Loss: 0.9554 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:42:42,401 [INFO] Step 1680/19460 | Loss: 1.1912 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:42:45,618 [INFO] Step 1690/19460 | Loss: 1.1297 | Acc: 0.6250 | LR: 1.49e-05
2026-01-07 19:42:49,291 [INFO] Step 1700/19460 | Loss: 0.9789 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:42:58,005 [INFO] [EVAL] Step 1700 | Val Loss: 0.5323 | Val Acc: 0.8363
2026-01-07 19:43:01,177 [INFO] Step 1710/19460 | Loss: 0.8411 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:43:04,266 [INFO] Step 1720/19460 | Loss: 0.7145 | Acc: 0.8750 | LR: 1.49e-05
2026-01-07 19:43:07,420 [INFO] Step 1730/19460 | Loss: 0.8445 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:43:10,510 [INFO] Step 1740/19460 | Loss: 0.9769 | Acc: 0.6875 | LR: 1.49e-05
2026-01-07 19:43:13,542 [INFO] Step 1750/19460 | Loss: 1.0261 | Acc: 0.7500 | LR: 1.49e-05
2026-01-07 19:43:16,747 [INFO] Step 1760/19460 | Loss: 0.5745 | Acc: 1.0000 | LR: 1.48e-05
2026-01-07 19:43:19,886 [INFO] Step 1770/19460 | Loss: 0.8848 | Acc: 0.6875 | LR: 1.48e-05
2026-01-07 19:43:22,982 [INFO] Step 1780/19460 | Loss: 0.8903 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:43:25,938 [INFO] Step 1790/19460 | Loss: 1.1486 | Acc: 0.6250 | LR: 1.48e-05
2026-01-07 19:43:29,576 [INFO] Step 1800/19460 | Loss: 0.7309 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:43:38,201 [INFO] [EVAL] Step 1800 | Val Loss: 0.4986 | Val Acc: 0.8469
2026-01-07 19:43:41,389 [INFO] Step 1810/19460 | Loss: 0.9709 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:43:44,535 [INFO] Step 1820/19460 | Loss: 0.7298 | Acc: 0.8750 | LR: 1.48e-05
2026-01-07 19:43:47,709 [INFO] Step 1830/19460 | Loss: 0.8860 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:43:50,848 [INFO] Step 1840/19460 | Loss: 0.8645 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:43:54,088 [INFO] Step 1850/19460 | Loss: 0.9831 | Acc: 0.6875 | LR: 1.48e-05
2026-01-07 19:43:57,122 [INFO] Step 1860/19460 | Loss: 1.3085 | Acc: 0.6250 | LR: 1.48e-05
2026-01-07 19:44:00,325 [INFO] Step 1870/19460 | Loss: 0.8936 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:44:03,546 [INFO] Step 1880/19460 | Loss: 1.0023 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:44:06,730 [INFO] Step 1890/19460 | Loss: 0.8107 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:44:07,191 [WARNING] Skipping batch due to non-finite loss at step=1891 (loss=nan, epoch=2).
2026-01-07 19:44:10,529 [INFO] Step 1900/19460 | Loss: 0.8160 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:44:19,195 [INFO] [EVAL] Step 1900 | Val Loss: 0.4987 | Val Acc: 0.8481
2026-01-07 19:44:22,395 [INFO] Step 1910/19460 | Loss: 0.8171 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:44:25,519 [INFO] Step 1920/19460 | Loss: 0.7097 | Acc: 0.9375 | LR: 1.48e-05
2026-01-07 19:44:27,928 [INFO] Epoch 2 complete | Avg Loss: 0.9786 | Avg Acc: 0.7307 | Updates: 963 | Micro-batches: 971 | Skipped: 8 (loss=8, logits=0, grads=0)
2026-01-07 19:44:27,928 [INFO] Epoch 3/20
2026-01-07 19:44:28,578 [INFO] Step 1930/19460 | Loss: 0.8374 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:44:31,787 [INFO] Step 1940/19460 | Loss: 1.0577 | Acc: 0.6250 | LR: 1.48e-05
2026-01-07 19:44:34,872 [INFO] Step 1950/19460 | Loss: 0.6659 | Acc: 0.8750 | LR: 1.48e-05
2026-01-07 19:44:38,101 [INFO] Step 1960/19460 | Loss: 0.8139 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:44:41,180 [INFO] Step 1970/19460 | Loss: 0.7209 | Acc: 0.8750 | LR: 1.48e-05
2026-01-07 19:44:44,356 [INFO] Step 1980/19460 | Loss: 0.9281 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:44:47,510 [INFO] Step 1990/19460 | Loss: 0.6836 | Acc: 0.8750 | LR: 1.48e-05
2026-01-07 19:44:51,166 [INFO] Step 2000/19460 | Loss: 0.9736 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:44:59,801 [INFO] [EVAL] Step 2000 | Val Loss: 0.4734 | Val Acc: 0.8550
2026-01-07 19:44:59,825 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:44:59,828 [INFO] New best validation accuracy: 0.8550
2026-01-07 19:44:59,854 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_2000.npz
2026-01-07 19:45:03,068 [INFO] Step 2010/19460 | Loss: 1.0473 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:45:06,359 [INFO] Step 2020/19460 | Loss: 0.7282 | Acc: 0.8750 | LR: 1.48e-05
2026-01-07 19:45:09,618 [INFO] Step 2030/19460 | Loss: 0.8300 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:45:12,793 [INFO] Step 2040/19460 | Loss: 0.8915 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:45:15,917 [INFO] Step 2050/19460 | Loss: 0.9757 | Acc: 0.6875 | LR: 1.48e-05
2026-01-07 19:45:19,130 [INFO] Step 2060/19460 | Loss: 0.7911 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:45:22,332 [INFO] Step 2070/19460 | Loss: 0.6956 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:45:25,396 [INFO] Step 2080/19460 | Loss: 0.9927 | Acc: 0.6875 | LR: 1.48e-05
2026-01-07 19:45:25,565 [WARNING] Skipping batch due to non-finite loss at step=2080 (loss=nan, epoch=3).
2026-01-07 19:45:28,653 [INFO] Step 2090/19460 | Loss: 1.2241 | Acc: 0.6875 | LR: 1.48e-05
2026-01-07 19:45:32,387 [INFO] Step 2100/19460 | Loss: 0.9017 | Acc: 0.8125 | LR: 1.48e-05
2026-01-07 19:45:41,114 [INFO] [EVAL] Step 2100 | Val Loss: 0.4724 | Val Acc: 0.8575
2026-01-07 19:45:41,144 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:45:41,146 [INFO] New best validation accuracy: 0.8575
2026-01-07 19:45:44,386 [INFO] Step 2110/19460 | Loss: 0.8917 | Acc: 0.7500 | LR: 1.48e-05
2026-01-07 19:45:47,366 [INFO] Step 2120/19460 | Loss: 0.7078 | Acc: 1.0000 | LR: 1.47e-05
2026-01-07 19:45:50,597 [INFO] Step 2130/19460 | Loss: 0.8745 | Acc: 0.6875 | LR: 1.47e-05
2026-01-07 19:45:53,769 [INFO] Step 2140/19460 | Loss: 0.7936 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:45:56,875 [INFO] Step 2150/19460 | Loss: 0.9476 | Acc: 0.8125 | LR: 1.47e-05
2026-01-07 19:46:00,212 [INFO] Step 2160/19460 | Loss: 0.8635 | Acc: 0.8125 | LR: 1.47e-05
2026-01-07 19:46:03,223 [INFO] Step 2170/19460 | Loss: 0.9952 | Acc: 0.6875 | LR: 1.47e-05
2026-01-07 19:46:06,517 [INFO] Step 2180/19460 | Loss: 0.9057 | Acc: 0.7500 | LR: 1.47e-05
2026-01-07 19:46:09,521 [INFO] Step 2190/19460 | Loss: 0.7595 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:46:13,034 [INFO] Step 2200/19460 | Loss: 0.6723 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:46:21,725 [INFO] [EVAL] Step 2200 | Val Loss: 0.4496 | Val Acc: 0.8669
2026-01-07 19:46:21,748 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:46:21,750 [INFO] New best validation accuracy: 0.8669
2026-01-07 19:46:25,061 [INFO] Step 2210/19460 | Loss: 1.2235 | Acc: 0.6875 | LR: 1.47e-05
2026-01-07 19:46:28,251 [INFO] Step 2220/19460 | Loss: 0.7883 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:46:31,380 [INFO] Step 2230/19460 | Loss: 1.1086 | Acc: 0.6875 | LR: 1.47e-05
2026-01-07 19:46:34,536 [INFO] Step 2240/19460 | Loss: 0.6008 | Acc: 0.9375 | LR: 1.47e-05
2026-01-07 19:46:37,645 [INFO] Step 2250/19460 | Loss: 1.0621 | Acc: 0.6875 | LR: 1.47e-05
2026-01-07 19:46:40,726 [INFO] Step 2260/19460 | Loss: 0.8551 | Acc: 0.8125 | LR: 1.47e-05
2026-01-07 19:46:43,927 [INFO] Step 2270/19460 | Loss: 0.5741 | Acc: 0.9375 | LR: 1.47e-05
2026-01-07 19:46:46,908 [INFO] Step 2280/19460 | Loss: 0.8773 | Acc: 0.7500 | LR: 1.47e-05
2026-01-07 19:46:50,069 [INFO] Step 2290/19460 | Loss: 0.7917 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:46:53,628 [INFO] Step 2300/19460 | Loss: 0.8100 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:47:02,332 [INFO] [EVAL] Step 2300 | Val Loss: 0.4790 | Val Acc: 0.8581
2026-01-07 19:47:05,593 [INFO] Step 2310/19460 | Loss: 0.8601 | Acc: 0.7500 | LR: 1.47e-05
2026-01-07 19:47:08,686 [INFO] Step 2320/19460 | Loss: 0.6908 | Acc: 0.8750 | LR: 1.47e-05
2026-01-07 19:47:11,914 [INFO] Step 2330/19460 | Loss: 1.1512 | Acc: 0.7500 | LR: 1.47e-05
2026-01-07 19:47:14,917 [INFO] Step 2340/19460 | Loss: 0.8483 | Acc: 0.8125 | LR: 1.47e-05
2026-01-07 19:47:18,048 [INFO] Step 2350/19460 | Loss: 1.0919 | Acc: 0.5000 | LR: 1.47e-05
2026-01-07 19:47:21,355 [INFO] Step 2360/19460 | Loss: 1.1896 | Acc: 0.6875 | LR: 1.47e-05
2026-01-07 19:47:24,459 [INFO] Step 2370/19460 | Loss: 0.9764 | Acc: 0.7500 | LR: 1.47e-05
2026-01-07 19:47:27,424 [INFO] Step 2380/19460 | Loss: 0.5536 | Acc: 0.9375 | LR: 1.47e-05
2026-01-07 19:47:30,581 [INFO] Step 2390/19460 | Loss: 0.8445 | Acc: 0.8125 | LR: 1.47e-05
2026-01-07 19:47:34,170 [INFO] Step 2400/19460 | Loss: 0.7446 | Acc: 0.8125 | LR: 1.47e-05
2026-01-07 19:47:42,794 [INFO] [EVAL] Step 2400 | Val Loss: 0.4644 | Val Acc: 0.8700
2026-01-07 19:47:42,815 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:47:42,817 [INFO] New best validation accuracy: 0.8700
2026-01-07 19:47:46,044 [INFO] Step 2410/19460 | Loss: 0.7467 | Acc: 0.9375 | LR: 1.47e-05
2026-01-07 19:47:49,052 [INFO] Step 2420/19460 | Loss: 0.8518 | Acc: 0.7500 | LR: 1.46e-05
2026-01-07 19:47:51,459 [WARNING] Skipping batch due to non-finite loss at step=2427 (loss=nan, epoch=3).
2026-01-07 19:47:52,389 [INFO] Step 2430/19460 | Loss: 1.4654 | Acc: 0.5000 | LR: 1.46e-05
2026-01-07 19:47:55,542 [INFO] Step 2440/19460 | Loss: 0.9016 | Acc: 0.8125 | LR: 1.46e-05
2026-01-07 19:47:58,778 [INFO] Step 2450/19460 | Loss: 0.8865 | Acc: 0.7500 | LR: 1.46e-05
2026-01-07 19:48:01,837 [INFO] Step 2460/19460 | Loss: 0.4619 | Acc: 1.0000 | LR: 1.46e-05
2026-01-07 19:48:04,953 [INFO] Step 2470/19460 | Loss: 0.6758 | Acc: 0.8750 | LR: 1.46e-05
2026-01-07 19:48:08,061 [INFO] Step 2480/19460 | Loss: 1.1033 | Acc: 0.6875 | LR: 1.46e-05
2026-01-07 19:48:11,289 [INFO] Step 2490/19460 | Loss: 1.1510 | Acc: 0.5625 | LR: 1.46e-05
2026-01-07 19:48:14,988 [INFO] Step 2500/19460 | Loss: 1.1083 | Acc: 0.6250 | LR: 1.46e-05
2026-01-07 19:48:23,626 [INFO] [EVAL] Step 2500 | Val Loss: 0.4595 | Val Acc: 0.8662
2026-01-07 19:48:23,654 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_2500.npz
2026-01-07 19:48:26,795 [INFO] Step 2510/19460 | Loss: 1.0611 | Acc: 0.6250 | LR: 1.46e-05
2026-01-07 19:48:29,973 [INFO] Step 2520/19460 | Loss: 0.9165 | Acc: 0.7500 | LR: 1.46e-05
2026-01-07 19:48:33,216 [INFO] Step 2530/19460 | Loss: 0.7595 | Acc: 0.8125 | LR: 1.46e-05
2026-01-07 19:48:36,470 [INFO] Step 2540/19460 | Loss: 0.7013 | Acc: 0.8750 | LR: 1.46e-05
2026-01-07 19:48:39,677 [INFO] Step 2550/19460 | Loss: 0.6253 | Acc: 0.9375 | LR: 1.46e-05
2026-01-07 19:48:42,835 [INFO] Step 2560/19460 | Loss: 0.6631 | Acc: 0.8125 | LR: 1.46e-05
2026-01-07 19:48:45,966 [INFO] Step 2570/19460 | Loss: 0.7203 | Acc: 0.8750 | LR: 1.46e-05
2026-01-07 19:48:49,229 [INFO] Step 2580/19460 | Loss: 1.4181 | Acc: 0.5625 | LR: 1.46e-05
2026-01-07 19:48:52,421 [INFO] Step 2590/19460 | Loss: 0.8993 | Acc: 0.8125 | LR: 1.46e-05
2026-01-07 19:48:56,239 [INFO] Step 2600/19460 | Loss: 1.3871 | Acc: 0.5625 | LR: 1.46e-05
2026-01-07 19:49:04,912 [INFO] [EVAL] Step 2600 | Val Loss: 0.4772 | Val Acc: 0.8462
2026-01-07 19:49:08,116 [INFO] Step 2610/19460 | Loss: 0.8337 | Acc: 0.8125 | LR: 1.46e-05
2026-01-07 19:49:11,285 [INFO] Step 2620/19460 | Loss: 0.7233 | Acc: 0.8125 | LR: 1.46e-05
2026-01-07 19:49:14,556 [INFO] Step 2630/19460 | Loss: 0.9863 | Acc: 0.7500 | LR: 1.46e-05
2026-01-07 19:49:17,764 [INFO] Step 2640/19460 | Loss: 1.1741 | Acc: 0.6875 | LR: 1.46e-05
2026-01-07 19:49:20,906 [INFO] Step 2650/19460 | Loss: 1.3742 | Acc: 0.4375 | LR: 1.46e-05
2026-01-07 19:49:23,933 [INFO] Step 2660/19460 | Loss: 0.9818 | Acc: 0.7500 | LR: 1.46e-05
2026-01-07 19:49:27,081 [INFO] Step 2670/19460 | Loss: 1.0297 | Acc: 0.7500 | LR: 1.46e-05
2026-01-07 19:49:30,222 [INFO] Step 2680/19460 | Loss: 0.8042 | Acc: 0.8750 | LR: 1.45e-05
2026-01-07 19:49:33,433 [INFO] Step 2690/19460 | Loss: 1.0326 | Acc: 0.6250 | LR: 1.45e-05
2026-01-07 19:49:37,049 [INFO] Step 2700/19460 | Loss: 0.8404 | Acc: 0.7500 | LR: 1.45e-05
2026-01-07 19:49:45,705 [INFO] [EVAL] Step 2700 | Val Loss: 0.4213 | Val Acc: 0.8831
2026-01-07 19:49:45,734 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:49:45,736 [INFO] New best validation accuracy: 0.8831
2026-01-07 19:49:48,873 [INFO] Step 2710/19460 | Loss: 0.9107 | Acc: 0.7500 | LR: 1.45e-05
2026-01-07 19:49:52,044 [INFO] Step 2720/19460 | Loss: 0.6773 | Acc: 0.9375 | LR: 1.45e-05
2026-01-07 19:49:55,165 [INFO] Step 2730/19460 | Loss: 0.9728 | Acc: 0.6250 | LR: 1.45e-05
2026-01-07 19:49:58,409 [INFO] Step 2740/19460 | Loss: 0.8180 | Acc: 0.8125 | LR: 1.45e-05
2026-01-07 19:50:01,610 [INFO] Step 2750/19460 | Loss: 0.9029 | Acc: 0.7500 | LR: 1.45e-05
2026-01-07 19:50:04,690 [INFO] Step 2760/19460 | Loss: 0.7022 | Acc: 0.8125 | LR: 1.45e-05
2026-01-07 19:50:07,809 [INFO] Step 2770/19460 | Loss: 0.8077 | Acc: 0.8125 | LR: 1.45e-05
2026-01-07 19:50:11,127 [INFO] Step 2780/19460 | Loss: 1.2179 | Acc: 0.6250 | LR: 1.45e-05
2026-01-07 19:50:14,265 [INFO] Step 2790/19460 | Loss: 0.8094 | Acc: 0.8750 | LR: 1.45e-05
2026-01-07 19:50:17,994 [INFO] Step 2800/19460 | Loss: 0.5351 | Acc: 1.0000 | LR: 1.45e-05
2026-01-07 19:50:26,674 [INFO] [EVAL] Step 2800 | Val Loss: 0.4396 | Val Acc: 0.8769
2026-01-07 19:50:29,131 [WARNING] Skipping batch due to non-finite loss at step=2807 (loss=nan, epoch=3).
2026-01-07 19:50:30,087 [INFO] Step 2810/19460 | Loss: 1.1374 | Acc: 0.7500 | LR: 1.45e-05
2026-01-07 19:50:33,260 [INFO] Step 2820/19460 | Loss: 0.8064 | Acc: 0.8750 | LR: 1.45e-05
2026-01-07 19:50:36,455 [INFO] Step 2830/19460 | Loss: 0.9198 | Acc: 0.6875 | LR: 1.45e-05
2026-01-07 19:50:39,711 [INFO] Step 2840/19460 | Loss: 0.9728 | Acc: 0.6875 | LR: 1.45e-05
2026-01-07 19:50:43,012 [INFO] Step 2850/19460 | Loss: 0.7351 | Acc: 0.8750 | LR: 1.45e-05
2026-01-07 19:50:46,020 [INFO] Step 2860/19460 | Loss: 0.5932 | Acc: 0.9375 | LR: 1.45e-05
2026-01-07 19:50:49,247 [INFO] Step 2870/19460 | Loss: 1.1205 | Acc: 0.7500 | LR: 1.45e-05
2026-01-07 19:50:52,376 [INFO] Step 2880/19460 | Loss: 0.7264 | Acc: 0.8125 | LR: 1.45e-05
2026-01-07 19:50:55,540 [INFO] Step 2890/19460 | Loss: 1.1019 | Acc: 0.6250 | LR: 1.45e-05
2026-01-07 19:50:57,366 [INFO] Epoch 3 complete | Avg Loss: 0.8827 | Avg Acc: 0.7812 | Updates: 968 | Micro-batches: 971 | Skipped: 3 (loss=3, logits=0, grads=0)
2026-01-07 19:50:57,366 [INFO] Epoch 4/20
2026-01-07 19:50:59,248 [INFO] Step 2900/19460 | Loss: 0.9941 | Acc: 0.8125 | LR: 1.45e-05
2026-01-07 19:51:07,943 [INFO] [EVAL] Step 2900 | Val Loss: 0.4418 | Val Acc: 0.8788
2026-01-07 19:51:11,122 [INFO] Step 2910/19460 | Loss: 0.8530 | Acc: 0.8125 | LR: 1.44e-05
2026-01-07 19:51:14,269 [INFO] Step 2920/19460 | Loss: 0.7683 | Acc: 0.8125 | LR: 1.44e-05
2026-01-07 19:51:17,359 [INFO] Step 2930/19460 | Loss: 0.9787 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:51:20,463 [INFO] Step 2940/19460 | Loss: 0.9233 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:51:23,605 [INFO] Step 2950/19460 | Loss: 0.7357 | Acc: 0.8750 | LR: 1.44e-05
2026-01-07 19:51:26,396 [WARNING] Skipping batch due to non-finite loss at step=2958 (loss=nan, epoch=4).
2026-01-07 19:51:26,960 [INFO] Step 2960/19460 | Loss: 0.8819 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:51:30,103 [INFO] Step 2970/19460 | Loss: 0.8884 | Acc: 0.8125 | LR: 1.44e-05
2026-01-07 19:51:33,149 [INFO] Step 2980/19460 | Loss: 0.8857 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:51:36,349 [INFO] Step 2990/19460 | Loss: 0.7910 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:51:39,903 [INFO] Step 3000/19460 | Loss: 1.1495 | Acc: 0.5625 | LR: 1.44e-05
2026-01-07 19:51:48,591 [INFO] [EVAL] Step 3000 | Val Loss: 0.4325 | Val Acc: 0.8781
2026-01-07 19:51:48,619 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_3000.npz
2026-01-07 19:51:51,784 [INFO] Step 3010/19460 | Loss: 1.1895 | Acc: 0.6875 | LR: 1.44e-05
2026-01-07 19:51:52,623 [WARNING] Skipping batch due to non-finite loss at step=3012 (loss=nan, epoch=4).
2026-01-07 19:51:55,186 [INFO] Step 3020/19460 | Loss: 1.0323 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:51:58,226 [INFO] Step 3030/19460 | Loss: 0.8629 | Acc: 0.8125 | LR: 1.44e-05
2026-01-07 19:52:01,417 [INFO] Step 3040/19460 | Loss: 0.7357 | Acc: 0.8750 | LR: 1.44e-05
2026-01-07 19:52:04,510 [INFO] Step 3050/19460 | Loss: 1.4191 | Acc: 0.5000 | LR: 1.44e-05
2026-01-07 19:52:07,771 [INFO] Step 3060/19460 | Loss: 0.9094 | Acc: 0.7500 | LR: 1.44e-05
2026-01-07 19:52:11,037 [INFO] Step 3070/19460 | Loss: 0.6292 | Acc: 0.8750 | LR: 1.44e-05
2026-01-07 19:52:14,173 [INFO] Step 3080/19460 | Loss: 1.1867 | Acc: 0.6875 | LR: 1.44e-05
2026-01-07 19:52:17,361 [INFO] Step 3090/19460 | Loss: 0.7501 | Acc: 0.8750 | LR: 1.44e-05
2026-01-07 19:52:20,954 [INFO] Step 3100/19460 | Loss: 0.9348 | Acc: 0.6875 | LR: 1.44e-05
2026-01-07 19:52:29,606 [INFO] [EVAL] Step 3100 | Val Loss: 0.4278 | Val Acc: 0.8775
2026-01-07 19:52:32,759 [INFO] Step 3110/19460 | Loss: 0.7606 | Acc: 0.8750 | LR: 1.44e-05
2026-01-07 19:52:35,889 [INFO] Step 3120/19460 | Loss: 0.6637 | Acc: 0.9375 | LR: 1.44e-05
2026-01-07 19:52:39,108 [INFO] Step 3130/19460 | Loss: 0.8934 | Acc: 0.8125 | LR: 1.43e-05
2026-01-07 19:52:42,276 [INFO] Step 3140/19460 | Loss: 1.0190 | Acc: 0.7500 | LR: 1.43e-05
2026-01-07 19:52:45,568 [INFO] Step 3150/19460 | Loss: 0.7575 | Acc: 0.8750 | LR: 1.43e-05
2026-01-07 19:52:48,739 [INFO] Step 3160/19460 | Loss: 0.8033 | Acc: 0.8125 | LR: 1.43e-05
2026-01-07 19:52:51,831 [INFO] Step 3170/19460 | Loss: 0.6637 | Acc: 0.8750 | LR: 1.43e-05
2026-01-07 19:52:55,083 [INFO] Step 3180/19460 | Loss: 0.9390 | Acc: 0.8125 | LR: 1.43e-05
2026-01-07 19:52:58,345 [INFO] Step 3190/19460 | Loss: 0.8565 | Acc: 0.7500 | LR: 1.43e-05
2026-01-07 19:53:02,119 [INFO] Step 3200/19460 | Loss: 0.5958 | Acc: 1.0000 | LR: 1.43e-05
2026-01-07 19:53:10,735 [INFO] [EVAL] Step 3200 | Val Loss: 0.4322 | Val Acc: 0.8794
2026-01-07 19:53:13,823 [INFO] Step 3210/19460 | Loss: 0.9497 | Acc: 0.7500 | LR: 1.43e-05
2026-01-07 19:53:16,903 [INFO] Step 3220/19460 | Loss: 0.8307 | Acc: 0.8750 | LR: 1.43e-05
2026-01-07 19:53:20,098 [INFO] Step 3230/19460 | Loss: 1.2403 | Acc: 0.5000 | LR: 1.43e-05
2026-01-07 19:53:23,340 [INFO] Step 3240/19460 | Loss: 1.0783 | Acc: 0.6250 | LR: 1.43e-05
2026-01-07 19:53:26,564 [INFO] Step 3250/19460 | Loss: 0.9217 | Acc: 0.8125 | LR: 1.43e-05
2026-01-07 19:53:29,834 [INFO] Step 3260/19460 | Loss: 0.5755 | Acc: 0.9375 | LR: 1.43e-05
2026-01-07 19:53:33,032 [INFO] Step 3270/19460 | Loss: 0.7082 | Acc: 0.9375 | LR: 1.43e-05
2026-01-07 19:53:36,225 [INFO] Step 3280/19460 | Loss: 0.8919 | Acc: 0.7500 | LR: 1.43e-05
2026-01-07 19:53:39,513 [INFO] Step 3290/19460 | Loss: 0.6202 | Acc: 0.9375 | LR: 1.43e-05
2026-01-07 19:53:43,285 [INFO] Step 3300/19460 | Loss: 0.7080 | Acc: 0.8750 | LR: 1.43e-05
2026-01-07 19:53:51,963 [INFO] [EVAL] Step 3300 | Val Loss: 0.4128 | Val Acc: 0.8869
2026-01-07 19:53:51,988 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:53:51,991 [INFO] New best validation accuracy: 0.8869
2026-01-07 19:53:55,209 [INFO] Step 3310/19460 | Loss: 0.8730 | Acc: 0.8125 | LR: 1.43e-05
2026-01-07 19:53:58,495 [INFO] Step 3320/19460 | Loss: 0.5887 | Acc: 0.9375 | LR: 1.43e-05
2026-01-07 19:54:01,582 [INFO] Step 3330/19460 | Loss: 0.5407 | Acc: 1.0000 | LR: 1.42e-05
2026-01-07 19:54:04,609 [INFO] Step 3340/19460 | Loss: 0.8562 | Acc: 0.7500 | LR: 1.42e-05
2026-01-07 19:54:07,757 [INFO] Step 3350/19460 | Loss: 1.0769 | Acc: 0.6875 | LR: 1.42e-05
2026-01-07 19:54:10,880 [INFO] Step 3360/19460 | Loss: 0.5419 | Acc: 0.9375 | LR: 1.42e-05
2026-01-07 19:54:14,134 [INFO] Step 3370/19460 | Loss: 0.5291 | Acc: 1.0000 | LR: 1.42e-05
2026-01-07 19:54:17,311 [INFO] Step 3380/19460 | Loss: 0.9191 | Acc: 0.8125 | LR: 1.42e-05
2026-01-07 19:54:20,398 [INFO] Step 3390/19460 | Loss: 0.5553 | Acc: 0.9375 | LR: 1.42e-05
2026-01-07 19:54:24,061 [INFO] Step 3400/19460 | Loss: 0.7432 | Acc: 0.8125 | LR: 1.42e-05
2026-01-07 19:54:32,764 [INFO] [EVAL] Step 3400 | Val Loss: 0.4160 | Val Acc: 0.8806
2026-01-07 19:54:35,947 [INFO] Step 3410/19460 | Loss: 0.7308 | Acc: 0.8750 | LR: 1.42e-05
2026-01-07 19:54:39,091 [INFO] Step 3420/19460 | Loss: 0.8573 | Acc: 0.8750 | LR: 1.42e-05
2026-01-07 19:54:42,206 [INFO] Step 3430/19460 | Loss: 0.8231 | Acc: 0.7500 | LR: 1.42e-05
2026-01-07 19:54:45,328 [INFO] Step 3440/19460 | Loss: 1.0338 | Acc: 0.6250 | LR: 1.42e-05
2026-01-07 19:54:48,562 [INFO] Step 3450/19460 | Loss: 0.8156 | Acc: 0.8125 | LR: 1.42e-05
2026-01-07 19:54:51,807 [INFO] Step 3460/19460 | Loss: 0.7851 | Acc: 0.8125 | LR: 1.42e-05
2026-01-07 19:54:54,983 [INFO] Step 3470/19460 | Loss: 0.6811 | Acc: 0.8750 | LR: 1.42e-05
2026-01-07 19:54:58,222 [INFO] Step 3480/19460 | Loss: 0.7165 | Acc: 0.8750 | LR: 1.42e-05
2026-01-07 19:55:01,449 [INFO] Step 3490/19460 | Loss: 0.9084 | Acc: 0.7500 | LR: 1.42e-05
2026-01-07 19:55:05,163 [INFO] Step 3500/19460 | Loss: 0.8233 | Acc: 0.8750 | LR: 1.42e-05
2026-01-07 19:55:13,802 [INFO] [EVAL] Step 3500 | Val Loss: 0.4137 | Val Acc: 0.8800
2026-01-07 19:55:13,833 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_3500.npz
2026-01-07 19:55:16,960 [INFO] Step 3510/19460 | Loss: 1.1179 | Acc: 0.6875 | LR: 1.41e-05
2026-01-07 19:55:20,129 [INFO] Step 3520/19460 | Loss: 0.7576 | Acc: 0.8750 | LR: 1.41e-05
2026-01-07 19:55:23,284 [INFO] Step 3530/19460 | Loss: 0.6745 | Acc: 0.8750 | LR: 1.41e-05
2026-01-07 19:55:26,410 [INFO] Step 3540/19460 | Loss: 0.6676 | Acc: 0.8750 | LR: 1.41e-05
2026-01-07 19:55:29,577 [INFO] Step 3550/19460 | Loss: 0.9491 | Acc: 0.7500 | LR: 1.41e-05
2026-01-07 19:55:32,596 [INFO] Step 3560/19460 | Loss: 0.7258 | Acc: 0.9375 | LR: 1.41e-05
2026-01-07 19:55:35,679 [INFO] Step 3570/19460 | Loss: 0.6212 | Acc: 1.0000 | LR: 1.41e-05
2026-01-07 19:55:37,440 [WARNING] Skipping batch due to non-finite loss at step=3575 (loss=nan, epoch=4).
2026-01-07 19:55:39,092 [INFO] Step 3580/19460 | Loss: 0.8210 | Acc: 0.7500 | LR: 1.41e-05
2026-01-07 19:55:42,353 [INFO] Step 3590/19460 | Loss: 0.7044 | Acc: 0.8750 | LR: 1.41e-05
2026-01-07 19:55:46,064 [INFO] Step 3600/19460 | Loss: 1.1221 | Acc: 0.6875 | LR: 1.41e-05
2026-01-07 19:55:54,712 [INFO] [EVAL] Step 3600 | Val Loss: 0.4162 | Val Acc: 0.8850
2026-01-07 19:55:57,873 [INFO] Step 3610/19460 | Loss: 1.0082 | Acc: 0.6875 | LR: 1.41e-05
2026-01-07 19:56:00,988 [INFO] Step 3620/19460 | Loss: 0.7442 | Acc: 0.8125 | LR: 1.41e-05
2026-01-07 19:56:04,236 [INFO] Step 3630/19460 | Loss: 0.6635 | Acc: 0.8750 | LR: 1.41e-05
2026-01-07 19:56:07,379 [INFO] Step 3640/19460 | Loss: 0.6290 | Acc: 0.9375 | LR: 1.41e-05
2026-01-07 19:56:10,547 [INFO] Step 3650/19460 | Loss: 0.8003 | Acc: 0.8125 | LR: 1.41e-05
2026-01-07 19:56:12,570 [WARNING] Skipping batch due to non-finite loss at step=3656 (loss=nan, epoch=4).
2026-01-07 19:56:13,787 [INFO] Step 3660/19460 | Loss: 0.7918 | Acc: 0.8125 | LR: 1.41e-05
2026-01-07 19:56:17,025 [INFO] Step 3670/19460 | Loss: 0.4751 | Acc: 1.0000 | LR: 1.41e-05
2026-01-07 19:56:20,258 [INFO] Step 3680/19460 | Loss: 0.7596 | Acc: 0.8125 | LR: 1.41e-05
2026-01-07 19:56:23,524 [INFO] Step 3690/19460 | Loss: 1.1393 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:56:25,253 [WARNING] Skipping batch due to non-finite loss at step=3695 (loss=nan, epoch=4).
2026-01-07 19:56:27,303 [INFO] Step 3700/19460 | Loss: 1.0948 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:56:35,863 [INFO] [EVAL] Step 3700 | Val Loss: 0.4138 | Val Acc: 0.8850
2026-01-07 19:56:39,165 [INFO] Step 3710/19460 | Loss: 0.9248 | Acc: 0.6875 | LR: 1.40e-05
2026-01-07 19:56:42,414 [INFO] Step 3720/19460 | Loss: 1.0723 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:56:45,517 [INFO] Step 3730/19460 | Loss: 0.9780 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:56:48,730 [INFO] Step 3740/19460 | Loss: 1.0467 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:56:51,816 [INFO] Step 3750/19460 | Loss: 0.8186 | Acc: 0.8750 | LR: 1.40e-05
2026-01-07 19:56:55,061 [INFO] Step 3760/19460 | Loss: 1.1418 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:56:58,211 [INFO] Step 3770/19460 | Loss: 1.1041 | Acc: 0.6250 | LR: 1.40e-05
2026-01-07 19:57:01,413 [INFO] Step 3780/19460 | Loss: 0.7962 | Acc: 0.8750 | LR: 1.40e-05
2026-01-07 19:57:04,613 [INFO] Step 3790/19460 | Loss: 0.8488 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:57:08,411 [INFO] Step 3800/19460 | Loss: 0.7576 | Acc: 0.8125 | LR: 1.40e-05
2026-01-07 19:57:17,090 [INFO] [EVAL] Step 3800 | Val Loss: 0.4462 | Val Acc: 0.8712
2026-01-07 19:57:18,596 [WARNING] Skipping batch due to non-finite loss at step=3804 (loss=nan, epoch=4).
2026-01-07 19:57:20,492 [INFO] Step 3810/19460 | Loss: 0.8297 | Acc: 0.8125 | LR: 1.40e-05
2026-01-07 19:57:23,809 [INFO] Step 3820/19460 | Loss: 0.9376 | Acc: 0.6875 | LR: 1.40e-05
2026-01-07 19:57:26,892 [INFO] Step 3830/19460 | Loss: 0.6377 | Acc: 0.9375 | LR: 1.40e-05
2026-01-07 19:57:30,157 [INFO] Step 3840/19460 | Loss: 0.9598 | Acc: 0.7500 | LR: 1.40e-05
2026-01-07 19:57:33,399 [INFO] Step 3850/19460 | Loss: 1.0061 | Acc: 0.6875 | LR: 1.39e-05
2026-01-07 19:57:36,497 [INFO] Step 3860/19460 | Loss: 0.6577 | Acc: 0.8750 | LR: 1.39e-05
2026-01-07 19:57:36,858 [INFO] Epoch 4 complete | Avg Loss: 0.8475 | Avg Acc: 0.8005 | Updates: 965 | Micro-batches: 971 | Skipped: 6 (loss=6, logits=0, grads=0)
2026-01-07 19:57:36,858 [INFO] Epoch 5/20
2026-01-07 19:57:39,725 [INFO] Step 3870/19460 | Loss: 0.7186 | Acc: 0.7500 | LR: 1.39e-05
2026-01-07 19:57:42,883 [INFO] Step 3880/19460 | Loss: 1.0518 | Acc: 0.6250 | LR: 1.39e-05
2026-01-07 19:57:46,124 [INFO] Step 3890/19460 | Loss: 0.7161 | Acc: 0.8750 | LR: 1.39e-05
2026-01-07 19:57:49,778 [INFO] Step 3900/19460 | Loss: 0.9985 | Acc: 0.7500 | LR: 1.39e-05
2026-01-07 19:57:58,502 [INFO] [EVAL] Step 3900 | Val Loss: 0.4032 | Val Acc: 0.8875
2026-01-07 19:57:58,536 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 19:57:58,538 [INFO] New best validation accuracy: 0.8875
2026-01-07 19:58:00,946 [WARNING] Skipping batch due to non-finite loss at step=3907 (loss=nan, epoch=5).
2026-01-07 19:58:01,937 [INFO] Step 3910/19460 | Loss: 1.0785 | Acc: 0.6250 | LR: 1.39e-05
2026-01-07 19:58:05,016 [INFO] Step 3920/19460 | Loss: 0.7210 | Acc: 0.8750 | LR: 1.39e-05
2026-01-07 19:58:08,236 [INFO] Step 3930/19460 | Loss: 0.6708 | Acc: 0.9375 | LR: 1.39e-05
2026-01-07 19:58:11,505 [INFO] Step 3940/19460 | Loss: 1.1175 | Acc: 0.5625 | LR: 1.39e-05
2026-01-07 19:58:14,750 [INFO] Step 3950/19460 | Loss: 0.7068 | Acc: 0.8125 | LR: 1.39e-05
2026-01-07 19:58:18,010 [INFO] Step 3960/19460 | Loss: 0.8699 | Acc: 0.8125 | LR: 1.39e-05
2026-01-07 19:58:21,206 [INFO] Step 3970/19460 | Loss: 1.1390 | Acc: 0.6250 | LR: 1.39e-05
2026-01-07 19:58:24,261 [INFO] Step 3980/19460 | Loss: 1.0135 | Acc: 0.6875 | LR: 1.39e-05
2026-01-07 19:58:27,491 [INFO] Step 3990/19460 | Loss: 0.7696 | Acc: 0.8125 | LR: 1.39e-05
2026-01-07 19:58:31,132 [INFO] Step 4000/19460 | Loss: 0.6925 | Acc: 0.9375 | LR: 1.39e-05
2026-01-07 19:58:39,840 [INFO] [EVAL] Step 4000 | Val Loss: 0.4219 | Val Acc: 0.8831
2026-01-07 19:58:39,874 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_4000.npz
2026-01-07 19:58:43,002 [INFO] Step 4010/19460 | Loss: 0.7253 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:58:46,239 [INFO] Step 4020/19460 | Loss: 1.0898 | Acc: 0.6875 | LR: 1.38e-05
2026-01-07 19:58:49,400 [INFO] Step 4030/19460 | Loss: 0.7569 | Acc: 0.8750 | LR: 1.38e-05
2026-01-07 19:58:52,437 [INFO] Step 4040/19460 | Loss: 0.6805 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:58:55,643 [INFO] Step 4050/19460 | Loss: 0.7045 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:58:58,887 [INFO] Step 4060/19460 | Loss: 0.7511 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:59:02,099 [INFO] Step 4070/19460 | Loss: 0.7398 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:59:05,252 [INFO] Step 4080/19460 | Loss: 1.0295 | Acc: 0.7500 | LR: 1.38e-05
2026-01-07 19:59:08,382 [INFO] Step 4090/19460 | Loss: 0.9307 | Acc: 0.8750 | LR: 1.38e-05
2026-01-07 19:59:12,119 [INFO] Step 4100/19460 | Loss: 0.8286 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:59:20,792 [INFO] [EVAL] Step 4100 | Val Loss: 0.4336 | Val Acc: 0.8788
2026-01-07 19:59:23,973 [INFO] Step 4110/19460 | Loss: 0.7395 | Acc: 0.8750 | LR: 1.38e-05
2026-01-07 19:59:27,289 [INFO] Step 4120/19460 | Loss: 0.7259 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:59:30,495 [INFO] Step 4130/19460 | Loss: 0.9733 | Acc: 0.8125 | LR: 1.38e-05
2026-01-07 19:59:33,791 [INFO] Step 4140/19460 | Loss: 0.6644 | Acc: 0.8750 | LR: 1.38e-05
2026-01-07 19:59:37,018 [INFO] Step 4150/19460 | Loss: 0.9330 | Acc: 0.6875 | LR: 1.38e-05
2026-01-07 19:59:40,199 [INFO] Step 4160/19460 | Loss: 0.7822 | Acc: 0.8750 | LR: 1.38e-05
2026-01-07 19:59:43,631 [INFO] Step 4170/19460 | Loss: 0.8490 | Acc: 0.7500 | LR: 1.37e-05
2026-01-07 19:59:46,825 [INFO] Step 4180/19460 | Loss: 0.6025 | Acc: 0.9375 | LR: 1.37e-05
2026-01-07 19:59:46,985 [WARNING] Skipping batch due to non-finite loss at step=4180 (loss=nan, epoch=5).
2026-01-07 19:59:50,194 [INFO] Step 4190/19460 | Loss: 1.2027 | Acc: 0.6875 | LR: 1.37e-05
2026-01-07 19:59:53,902 [INFO] Step 4200/19460 | Loss: 1.1236 | Acc: 0.6250 | LR: 1.37e-05
2026-01-07 20:00:02,568 [INFO] [EVAL] Step 4200 | Val Loss: 0.3992 | Val Acc: 0.8906
2026-01-07 20:00:02,596 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:00:02,598 [INFO] New best validation accuracy: 0.8906
2026-01-07 20:00:05,769 [INFO] Step 4210/19460 | Loss: 0.8203 | Acc: 0.7500 | LR: 1.37e-05
2026-01-07 20:00:08,930 [INFO] Step 4220/19460 | Loss: 0.9784 | Acc: 0.7500 | LR: 1.37e-05
2026-01-07 20:00:12,160 [INFO] Step 4230/19460 | Loss: 1.0231 | Acc: 0.6875 | LR: 1.37e-05
2026-01-07 20:00:15,243 [INFO] Step 4240/19460 | Loss: 0.6854 | Acc: 0.9375 | LR: 1.37e-05
2026-01-07 20:00:18,381 [INFO] Step 4250/19460 | Loss: 0.9265 | Acc: 0.6875 | LR: 1.37e-05
2026-01-07 20:00:21,460 [INFO] Step 4260/19460 | Loss: 0.9191 | Acc: 0.8125 | LR: 1.37e-05
2026-01-07 20:00:24,586 [INFO] Step 4270/19460 | Loss: 1.1870 | Acc: 0.7500 | LR: 1.37e-05
2026-01-07 20:00:27,856 [INFO] Step 4280/19460 | Loss: 0.8176 | Acc: 0.9375 | LR: 1.37e-05
2026-01-07 20:00:31,022 [INFO] Step 4290/19460 | Loss: 0.6549 | Acc: 0.8750 | LR: 1.37e-05
2026-01-07 20:00:34,812 [INFO] Step 4300/19460 | Loss: 0.5951 | Acc: 0.8750 | LR: 1.37e-05
2026-01-07 20:00:43,465 [INFO] [EVAL] Step 4300 | Val Loss: 0.4140 | Val Acc: 0.8862
2026-01-07 20:00:46,760 [INFO] Step 4310/19460 | Loss: 0.9237 | Acc: 0.8125 | LR: 1.37e-05
2026-01-07 20:00:49,840 [INFO] Step 4320/19460 | Loss: 1.0868 | Acc: 0.5625 | LR: 1.36e-05
2026-01-07 20:00:53,086 [INFO] Step 4330/19460 | Loss: 0.7169 | Acc: 0.9375 | LR: 1.36e-05
2026-01-07 20:00:56,221 [INFO] Step 4340/19460 | Loss: 0.8171 | Acc: 0.8125 | LR: 1.36e-05
2026-01-07 20:00:59,405 [INFO] Step 4350/19460 | Loss: 0.9569 | Acc: 0.8125 | LR: 1.36e-05
2026-01-07 20:01:02,527 [INFO] Step 4360/19460 | Loss: 0.6564 | Acc: 0.9375 | LR: 1.36e-05
2026-01-07 20:01:05,772 [INFO] Step 4370/19460 | Loss: 0.7937 | Acc: 0.8125 | LR: 1.36e-05
2026-01-07 20:01:08,853 [INFO] Step 4380/19460 | Loss: 0.8690 | Acc: 0.8125 | LR: 1.36e-05
2026-01-07 20:01:12,215 [INFO] Step 4390/19460 | Loss: 1.0997 | Acc: 0.6875 | LR: 1.36e-05
2026-01-07 20:01:15,933 [INFO] Step 4400/19460 | Loss: 0.8827 | Acc: 0.8750 | LR: 1.36e-05
2026-01-07 20:01:24,563 [INFO] [EVAL] Step 4400 | Val Loss: 0.4124 | Val Acc: 0.8881
2026-01-07 20:01:27,818 [INFO] Step 4410/19460 | Loss: 0.8244 | Acc: 0.8125 | LR: 1.36e-05
2026-01-07 20:01:30,908 [INFO] Step 4420/19460 | Loss: 0.5959 | Acc: 0.8750 | LR: 1.36e-05
2026-01-07 20:01:34,172 [INFO] Step 4430/19460 | Loss: 0.9498 | Acc: 0.6875 | LR: 1.36e-05
2026-01-07 20:01:37,413 [INFO] Step 4440/19460 | Loss: 0.7157 | Acc: 0.8125 | LR: 1.36e-05
2026-01-07 20:01:40,540 [INFO] Step 4450/19460 | Loss: 0.7179 | Acc: 0.8750 | LR: 1.36e-05
2026-01-07 20:01:43,630 [INFO] Step 4460/19460 | Loss: 0.8963 | Acc: 0.8125 | LR: 1.35e-05
2026-01-07 20:01:46,752 [INFO] Step 4470/19460 | Loss: 0.9702 | Acc: 0.8125 | LR: 1.35e-05
2026-01-07 20:01:49,991 [INFO] Step 4480/19460 | Loss: 0.8324 | Acc: 0.8750 | LR: 1.35e-05
2026-01-07 20:01:53,079 [INFO] Step 4490/19460 | Loss: 1.0311 | Acc: 0.5625 | LR: 1.35e-05
2026-01-07 20:01:56,871 [INFO] Step 4500/19460 | Loss: 0.6582 | Acc: 0.8750 | LR: 1.35e-05
2026-01-07 20:02:05,561 [INFO] [EVAL] Step 4500 | Val Loss: 0.3885 | Val Acc: 0.9000
2026-01-07 20:02:05,584 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:02:05,586 [INFO] New best validation accuracy: 0.9000
2026-01-07 20:02:05,614 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_4500.npz
2026-01-07 20:02:08,751 [INFO] Step 4510/19460 | Loss: 0.8222 | Acc: 0.8125 | LR: 1.35e-05
2026-01-07 20:02:11,878 [INFO] Step 4520/19460 | Loss: 0.8367 | Acc: 0.8125 | LR: 1.35e-05
2026-01-07 20:02:14,969 [INFO] Step 4530/19460 | Loss: 0.6554 | Acc: 0.8750 | LR: 1.35e-05
2026-01-07 20:02:18,224 [INFO] Step 4540/19460 | Loss: 0.9552 | Acc: 0.7500 | LR: 1.35e-05
2026-01-07 20:02:21,413 [INFO] Step 4550/19460 | Loss: 1.1731 | Acc: 0.6250 | LR: 1.35e-05
2026-01-07 20:02:24,716 [INFO] Step 4560/19460 | Loss: 0.9143 | Acc: 0.8125 | LR: 1.35e-05
2026-01-07 20:02:27,996 [INFO] Step 4570/19460 | Loss: 0.9041 | Acc: 0.8125 | LR: 1.35e-05
2026-01-07 20:02:31,108 [INFO] Step 4580/19460 | Loss: 0.8408 | Acc: 0.7500 | LR: 1.35e-05
2026-01-07 20:02:34,305 [INFO] Step 4590/19460 | Loss: 0.8678 | Acc: 0.7500 | LR: 1.35e-05
2026-01-07 20:02:38,143 [INFO] Step 4600/19460 | Loss: 0.5986 | Acc: 0.9375 | LR: 1.34e-05
2026-01-07 20:02:46,821 [INFO] [EVAL] Step 4600 | Val Loss: 0.3908 | Val Acc: 0.8856
2026-01-07 20:02:49,993 [INFO] Step 4610/19460 | Loss: 1.1107 | Acc: 0.6875 | LR: 1.34e-05
2026-01-07 20:02:53,131 [INFO] Step 4620/19460 | Loss: 0.7601 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:02:56,294 [INFO] Step 4630/19460 | Loss: 1.0227 | Acc: 0.6875 | LR: 1.34e-05
2026-01-07 20:02:59,705 [INFO] Step 4640/19460 | Loss: 0.9574 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:03:00,126 [WARNING] Skipping batch due to non-finite loss at step=4641 (loss=nan, epoch=5).
2026-01-07 20:03:03,049 [INFO] Step 4650/19460 | Loss: 0.8081 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:03:06,190 [INFO] Step 4660/19460 | Loss: 0.8191 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:03:09,293 [INFO] Step 4670/19460 | Loss: 0.7962 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:03:12,402 [INFO] Step 4680/19460 | Loss: 0.7001 | Acc: 0.8750 | LR: 1.34e-05
2026-01-07 20:03:15,529 [INFO] Step 4690/19460 | Loss: 1.0102 | Acc: 0.6875 | LR: 1.34e-05
2026-01-07 20:03:19,326 [INFO] Step 4700/19460 | Loss: 0.6378 | Acc: 0.9375 | LR: 1.34e-05
2026-01-07 20:03:27,974 [INFO] [EVAL] Step 4700 | Val Loss: 0.3865 | Val Acc: 0.8906
2026-01-07 20:03:31,184 [INFO] Step 4710/19460 | Loss: 0.9499 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:03:34,510 [INFO] Step 4720/19460 | Loss: 0.7162 | Acc: 0.8125 | LR: 1.34e-05
2026-01-07 20:03:37,668 [INFO] Step 4730/19460 | Loss: 0.8603 | Acc: 0.7500 | LR: 1.34e-05
2026-01-07 20:03:40,812 [INFO] Step 4740/19460 | Loss: 0.8648 | Acc: 0.8125 | LR: 1.33e-05
2026-01-07 20:03:43,991 [INFO] Step 4750/19460 | Loss: 0.5328 | Acc: 1.0000 | LR: 1.33e-05
2026-01-07 20:03:47,075 [INFO] Step 4760/19460 | Loss: 0.8242 | Acc: 0.8125 | LR: 1.33e-05
2026-01-07 20:03:50,498 [INFO] Step 4770/19460 | Loss: 0.8742 | Acc: 0.8125 | LR: 1.33e-05
2026-01-07 20:03:53,691 [INFO] Step 4780/19460 | Loss: 0.9663 | Acc: 0.7500 | LR: 1.33e-05
2026-01-07 20:03:56,696 [INFO] Step 4790/19460 | Loss: 0.8238 | Acc: 0.8750 | LR: 1.33e-05
2026-01-07 20:04:00,305 [INFO] Step 4800/19460 | Loss: 0.8938 | Acc: 0.7500 | LR: 1.33e-05
2026-01-07 20:04:09,007 [INFO] [EVAL] Step 4800 | Val Loss: 0.3971 | Val Acc: 0.8919
2026-01-07 20:04:12,203 [INFO] Step 4810/19460 | Loss: 0.6618 | Acc: 0.9375 | LR: 1.33e-05
2026-01-07 20:04:15,270 [INFO] Step 4820/19460 | Loss: 0.5661 | Acc: 1.0000 | LR: 1.33e-05
2026-01-07 20:04:18,053 [INFO] Epoch 5 complete | Avg Loss: 0.8231 | Avg Acc: 0.8096 | Updates: 968 | Micro-batches: 971 | Skipped: 3 (loss=3, logits=0, grads=0)
2026-01-07 20:04:18,053 [INFO] Epoch 6/20
2026-01-07 20:04:18,392 [INFO] Step 4830/19460 | Loss: 1.2496 | Acc: 0.6250 | LR: 1.33e-05
2026-01-07 20:04:21,442 [INFO] Step 4840/19460 | Loss: 1.2003 | Acc: 0.6250 | LR: 1.33e-05
2026-01-07 20:04:24,422 [INFO] Step 4850/19460 | Loss: 0.7423 | Acc: 0.8125 | LR: 1.33e-05
2026-01-07 20:04:27,632 [INFO] Step 4860/19460 | Loss: 0.8292 | Acc: 0.7500 | LR: 1.33e-05
2026-01-07 20:04:30,824 [INFO] Step 4870/19460 | Loss: 0.8458 | Acc: 0.8125 | LR: 1.32e-05
2026-01-07 20:04:34,119 [INFO] Step 4880/19460 | Loss: 0.8155 | Acc: 0.7500 | LR: 1.32e-05
2026-01-07 20:04:37,240 [INFO] Step 4890/19460 | Loss: 0.5137 | Acc: 1.0000 | LR: 1.32e-05
2026-01-07 20:04:37,418 [WARNING] Skipping batch due to non-finite loss at step=4890 (loss=nan, epoch=6).
2026-01-07 20:04:41,126 [INFO] Step 4900/19460 | Loss: 0.6939 | Acc: 0.8125 | LR: 1.32e-05
2026-01-07 20:04:49,806 [INFO] [EVAL] Step 4900 | Val Loss: 0.3842 | Val Acc: 0.8944
2026-01-07 20:04:51,863 [WARNING] Skipping batch due to non-finite loss at step=4906 (loss=nan, epoch=6).
2026-01-07 20:04:53,070 [INFO] Step 4910/19460 | Loss: 0.7517 | Acc: 0.7500 | LR: 1.32e-05
2026-01-07 20:04:56,363 [INFO] Step 4920/19460 | Loss: 0.6646 | Acc: 0.8750 | LR: 1.32e-05
2026-01-07 20:04:59,643 [INFO] Step 4930/19460 | Loss: 0.8092 | Acc: 0.8750 | LR: 1.32e-05
2026-01-07 20:05:02,863 [INFO] Step 4940/19460 | Loss: 0.7697 | Acc: 0.8750 | LR: 1.32e-05
2026-01-07 20:05:06,236 [INFO] Step 4950/19460 | Loss: 0.5079 | Acc: 1.0000 | LR: 1.32e-05
2026-01-07 20:05:09,580 [INFO] Step 4960/19460 | Loss: 0.7788 | Acc: 0.7500 | LR: 1.32e-05
2026-01-07 20:05:12,575 [INFO] Step 4970/19460 | Loss: 0.8755 | Acc: 0.7500 | LR: 1.32e-05
2026-01-07 20:05:15,645 [INFO] Step 4980/19460 | Loss: 0.7985 | Acc: 0.7500 | LR: 1.32e-05
2026-01-07 20:05:18,547 [INFO] Step 4990/19460 | Loss: 0.8305 | Acc: 0.8750 | LR: 1.32e-05
2026-01-07 20:05:22,148 [INFO] Step 5000/19460 | Loss: 0.7469 | Acc: 0.8125 | LR: 1.31e-05
2026-01-07 20:05:30,579 [INFO] [EVAL] Step 5000 | Val Loss: 0.3871 | Val Acc: 0.8894
2026-01-07 20:05:30,613 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_5000.npz
2026-01-07 20:05:33,688 [INFO] Step 5010/19460 | Loss: 0.7573 | Acc: 0.9375 | LR: 1.31e-05
2026-01-07 20:05:36,803 [INFO] Step 5020/19460 | Loss: 0.7927 | Acc: 0.8125 | LR: 1.31e-05
2026-01-07 20:05:39,897 [INFO] Step 5030/19460 | Loss: 0.9691 | Acc: 0.7500 | LR: 1.31e-05
2026-01-07 20:05:42,837 [INFO] Step 5040/19460 | Loss: 0.6227 | Acc: 0.8750 | LR: 1.31e-05
2026-01-07 20:05:45,773 [INFO] Step 5050/19460 | Loss: 1.0435 | Acc: 0.5625 | LR: 1.31e-05
2026-01-07 20:05:48,854 [INFO] Step 5060/19460 | Loss: 0.6976 | Acc: 0.8750 | LR: 1.31e-05
2026-01-07 20:05:51,833 [INFO] Step 5070/19460 | Loss: 0.8188 | Acc: 0.8125 | LR: 1.31e-05
2026-01-07 20:05:54,979 [INFO] Step 5080/19460 | Loss: 1.0170 | Acc: 0.8125 | LR: 1.31e-05
2026-01-07 20:05:57,851 [INFO] Step 5090/19460 | Loss: 0.6678 | Acc: 0.8750 | LR: 1.31e-05
2026-01-07 20:06:01,381 [INFO] Step 5100/19460 | Loss: 0.6945 | Acc: 0.8125 | LR: 1.31e-05
2026-01-07 20:06:09,898 [INFO] [EVAL] Step 5100 | Val Loss: 0.3921 | Val Acc: 0.8919
2026-01-07 20:06:11,065 [WARNING] Skipping batch due to non-finite loss at step=5103 (loss=nan, epoch=6).
2026-01-07 20:06:13,205 [INFO] Step 5110/19460 | Loss: 0.7303 | Acc: 0.8750 | LR: 1.31e-05
2026-01-07 20:06:16,157 [WARNING] Skipping batch due to non-finite loss at step=5119 (loss=nan, epoch=6).
2026-01-07 20:06:16,462 [INFO] Step 5120/19460 | Loss: 0.7380 | Acc: 0.8125 | LR: 1.30e-05
2026-01-07 20:06:19,613 [INFO] Step 5130/19460 | Loss: 1.3551 | Acc: 0.6250 | LR: 1.30e-05
2026-01-07 20:06:22,757 [INFO] Step 5140/19460 | Loss: 0.9567 | Acc: 0.6875 | LR: 1.30e-05
2026-01-07 20:06:25,902 [INFO] Step 5150/19460 | Loss: 0.7832 | Acc: 0.8125 | LR: 1.30e-05
2026-01-07 20:06:29,232 [INFO] Step 5160/19460 | Loss: 0.6971 | Acc: 0.8750 | LR: 1.30e-05
2026-01-07 20:06:32,292 [INFO] Step 5170/19460 | Loss: 0.6434 | Acc: 0.9375 | LR: 1.30e-05
2026-01-07 20:06:35,509 [INFO] Step 5180/19460 | Loss: 1.0604 | Acc: 0.7500 | LR: 1.30e-05
2026-01-07 20:06:38,892 [INFO] Step 5190/19460 | Loss: 0.5606 | Acc: 0.9375 | LR: 1.30e-05
2026-01-07 20:06:39,089 [WARNING] Skipping batch due to non-finite loss at step=5190 (loss=nan, epoch=6).
2026-01-07 20:06:42,751 [INFO] Step 5200/19460 | Loss: 1.0494 | Acc: 0.6250 | LR: 1.30e-05
2026-01-07 20:06:51,483 [INFO] [EVAL] Step 5200 | Val Loss: 0.3892 | Val Acc: 0.8900
2026-01-07 20:06:54,839 [INFO] Step 5210/19460 | Loss: 0.9608 | Acc: 0.8125 | LR: 1.30e-05
2026-01-07 20:06:58,136 [INFO] Step 5220/19460 | Loss: 0.9430 | Acc: 0.7500 | LR: 1.30e-05
2026-01-07 20:07:01,545 [INFO] Step 5230/19460 | Loss: 0.8938 | Acc: 0.7500 | LR: 1.30e-05
2026-01-07 20:07:04,851 [INFO] Step 5240/19460 | Loss: 0.6601 | Acc: 0.9375 | LR: 1.30e-05
2026-01-07 20:07:08,117 [INFO] Step 5250/19460 | Loss: 0.6203 | Acc: 0.9375 | LR: 1.29e-05
2026-01-07 20:07:11,146 [INFO] Step 5260/19460 | Loss: 0.6947 | Acc: 0.8125 | LR: 1.29e-05
2026-01-07 20:07:14,326 [INFO] Step 5270/19460 | Loss: 0.7049 | Acc: 0.8750 | LR: 1.29e-05
2026-01-07 20:07:17,659 [INFO] Step 5280/19460 | Loss: 0.6069 | Acc: 0.9375 | LR: 1.29e-05
2026-01-07 20:07:20,778 [INFO] Step 5290/19460 | Loss: 0.6725 | Acc: 0.9375 | LR: 1.29e-05
2026-01-07 20:07:24,319 [INFO] Step 5300/19460 | Loss: 0.6076 | Acc: 0.9375 | LR: 1.29e-05
2026-01-07 20:07:32,943 [INFO] [EVAL] Step 5300 | Val Loss: 0.3902 | Val Acc: 0.8856
2026-01-07 20:07:36,243 [INFO] Step 5310/19460 | Loss: 0.9698 | Acc: 0.6875 | LR: 1.29e-05
2026-01-07 20:07:38,368 [WARNING] Skipping batch due to non-finite loss at step=5316 (loss=nan, epoch=6).
2026-01-07 20:07:39,643 [INFO] Step 5320/19460 | Loss: 0.8443 | Acc: 0.8125 | LR: 1.29e-05
2026-01-07 20:07:42,841 [INFO] Step 5330/19460 | Loss: 0.7524 | Acc: 0.8750 | LR: 1.29e-05
2026-01-07 20:07:45,909 [INFO] Step 5340/19460 | Loss: 0.8187 | Acc: 0.8125 | LR: 1.29e-05
2026-01-07 20:07:49,053 [INFO] Step 5350/19460 | Loss: 0.7123 | Acc: 0.8750 | LR: 1.29e-05
2026-01-07 20:07:52,317 [INFO] Step 5360/19460 | Loss: 0.7551 | Acc: 0.8125 | LR: 1.29e-05
2026-01-07 20:07:55,615 [INFO] Step 5370/19460 | Loss: 0.5067 | Acc: 0.9375 | LR: 1.28e-05
2026-01-07 20:07:58,960 [INFO] Step 5380/19460 | Loss: 0.8820 | Acc: 0.7500 | LR: 1.28e-05
2026-01-07 20:08:02,165 [INFO] Step 5390/19460 | Loss: 0.7257 | Acc: 0.8750 | LR: 1.28e-05
2026-01-07 20:08:05,800 [INFO] Step 5400/19460 | Loss: 0.8128 | Acc: 0.8125 | LR: 1.28e-05
2026-01-07 20:08:14,524 [INFO] [EVAL] Step 5400 | Val Loss: 0.3947 | Val Acc: 0.8850
2026-01-07 20:08:17,615 [INFO] Step 5410/19460 | Loss: 0.9728 | Acc: 0.5625 | LR: 1.28e-05
2026-01-07 20:08:20,841 [INFO] Step 5420/19460 | Loss: 0.8483 | Acc: 0.6875 | LR: 1.28e-05
2026-01-07 20:08:24,007 [INFO] Step 5430/19460 | Loss: 1.3607 | Acc: 0.5625 | LR: 1.28e-05
2026-01-07 20:08:27,042 [INFO] Step 5440/19460 | Loss: 0.9035 | Acc: 0.7500 | LR: 1.28e-05
2026-01-07 20:08:30,389 [INFO] Step 5450/19460 | Loss: 1.0265 | Acc: 0.7500 | LR: 1.28e-05
2026-01-07 20:08:33,627 [INFO] Step 5460/19460 | Loss: 0.8776 | Acc: 0.8125 | LR: 1.28e-05
2026-01-07 20:08:36,762 [INFO] Step 5470/19460 | Loss: 1.0089 | Acc: 0.8125 | LR: 1.28e-05
2026-01-07 20:08:39,912 [INFO] Step 5480/19460 | Loss: 0.7465 | Acc: 0.8125 | LR: 1.27e-05
2026-01-07 20:08:43,130 [INFO] Step 5490/19460 | Loss: 0.8196 | Acc: 0.8125 | LR: 1.27e-05
2026-01-07 20:08:46,751 [INFO] Step 5500/19460 | Loss: 0.7693 | Acc: 0.8750 | LR: 1.27e-05
2026-01-07 20:08:55,441 [INFO] [EVAL] Step 5500 | Val Loss: 0.3903 | Val Acc: 0.8875
2026-01-07 20:08:55,472 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_5500.npz
2026-01-07 20:08:58,614 [INFO] Step 5510/19460 | Loss: 0.8129 | Acc: 0.7500 | LR: 1.27e-05
2026-01-07 20:09:01,857 [INFO] Step 5520/19460 | Loss: 0.7938 | Acc: 0.8125 | LR: 1.27e-05
2026-01-07 20:09:05,057 [INFO] Step 5530/19460 | Loss: 1.1009 | Acc: 0.6875 | LR: 1.27e-05
2026-01-07 20:09:08,464 [INFO] Step 5540/19460 | Loss: 0.9723 | Acc: 0.7500 | LR: 1.27e-05
2026-01-07 20:09:11,663 [INFO] Step 5550/19460 | Loss: 1.1865 | Acc: 0.6875 | LR: 1.27e-05
2026-01-07 20:09:14,759 [INFO] Step 5560/19460 | Loss: 1.1111 | Acc: 0.6875 | LR: 1.27e-05
2026-01-07 20:09:17,905 [INFO] Step 5570/19460 | Loss: 0.6999 | Acc: 0.8750 | LR: 1.27e-05
2026-01-07 20:09:21,012 [INFO] Step 5580/19460 | Loss: 0.8434 | Acc: 0.8125 | LR: 1.27e-05
2026-01-07 20:09:24,199 [INFO] Step 5590/19460 | Loss: 0.8515 | Acc: 0.8125 | LR: 1.27e-05
2026-01-07 20:09:27,881 [INFO] Step 5600/19460 | Loss: 0.7929 | Acc: 0.8750 | LR: 1.26e-05
2026-01-07 20:09:36,547 [INFO] [EVAL] Step 5600 | Val Loss: 0.3832 | Val Acc: 0.8994
2026-01-07 20:09:39,774 [INFO] Step 5610/19460 | Loss: 0.9340 | Acc: 0.8125 | LR: 1.26e-05
2026-01-07 20:09:43,033 [INFO] Step 5620/19460 | Loss: 0.6594 | Acc: 0.8750 | LR: 1.26e-05
2026-01-07 20:09:46,334 [INFO] Step 5630/19460 | Loss: 0.8984 | Acc: 0.7500 | LR: 1.26e-05
2026-01-07 20:09:49,568 [INFO] Step 5640/19460 | Loss: 0.6410 | Acc: 0.8750 | LR: 1.26e-05
2026-01-07 20:09:52,701 [INFO] Step 5650/19460 | Loss: 1.1500 | Acc: 0.6250 | LR: 1.26e-05
2026-01-07 20:09:55,838 [INFO] Step 5660/19460 | Loss: 0.8158 | Acc: 0.7500 | LR: 1.26e-05
2026-01-07 20:09:58,981 [INFO] Step 5670/19460 | Loss: 0.5336 | Acc: 0.9375 | LR: 1.26e-05
2026-01-07 20:10:02,074 [INFO] Step 5680/19460 | Loss: 0.6860 | Acc: 0.8750 | LR: 1.26e-05
2026-01-07 20:10:05,251 [INFO] Step 5690/19460 | Loss: 0.9685 | Acc: 0.8750 | LR: 1.26e-05
2026-01-07 20:10:08,924 [INFO] Step 5700/19460 | Loss: 0.5304 | Acc: 0.9375 | LR: 1.26e-05
2026-01-07 20:10:17,588 [INFO] [EVAL] Step 5700 | Val Loss: 0.3726 | Val Acc: 0.9019
2026-01-07 20:10:17,619 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:10:17,621 [INFO] New best validation accuracy: 0.9019
2026-01-07 20:10:20,678 [INFO] Step 5710/19460 | Loss: 0.7217 | Acc: 0.8750 | LR: 1.26e-05
2026-01-07 20:10:23,959 [INFO] Step 5720/19460 | Loss: 1.0399 | Acc: 0.6875 | LR: 1.25e-05
2026-01-07 20:10:27,069 [INFO] Step 5730/19460 | Loss: 0.9174 | Acc: 0.7500 | LR: 1.25e-05
2026-01-07 20:10:30,344 [INFO] Step 5740/19460 | Loss: 1.0760 | Acc: 0.8125 | LR: 1.25e-05
2026-01-07 20:10:33,542 [INFO] Step 5750/19460 | Loss: 0.7666 | Acc: 0.8750 | LR: 1.25e-05
2026-01-07 20:10:36,592 [INFO] Step 5760/19460 | Loss: 0.7081 | Acc: 0.8750 | LR: 1.25e-05
2026-01-07 20:10:39,735 [INFO] Step 5770/19460 | Loss: 0.5380 | Acc: 0.9375 | LR: 1.25e-05
2026-01-07 20:10:43,016 [INFO] Step 5780/19460 | Loss: 0.9702 | Acc: 0.6250 | LR: 1.25e-05
2026-01-07 20:10:46,279 [INFO] Step 5790/19460 | Loss: 0.7813 | Acc: 0.8750 | LR: 1.25e-05
2026-01-07 20:10:47,378 [WARNING] Skipping batch due to non-finite loss at step=5793 (loss=nan, epoch=6).
2026-01-07 20:10:47,378 [INFO] Epoch 6 complete | Avg Loss: 0.8040 | Avg Acc: 0.8203 | Updates: 964 | Micro-batches: 971 | Skipped: 7 (loss=7, logits=0, grads=0)
2026-01-07 20:10:47,378 [INFO] Epoch 7/20
2026-01-07 20:10:50,108 [INFO] Step 5800/19460 | Loss: 0.7045 | Acc: 0.9375 | LR: 1.25e-05
2026-01-07 20:10:58,811 [INFO] [EVAL] Step 5800 | Val Loss: 0.3784 | Val Acc: 0.8975
2026-01-07 20:11:02,113 [INFO] Step 5810/19460 | Loss: 0.6607 | Acc: 0.8750 | LR: 1.25e-05
2026-01-07 20:11:05,252 [INFO] Step 5820/19460 | Loss: 0.6956 | Acc: 0.9375 | LR: 1.25e-05
2026-01-07 20:11:08,514 [INFO] Step 5830/19460 | Loss: 0.4773 | Acc: 1.0000 | LR: 1.24e-05
2026-01-07 20:11:11,665 [INFO] Step 5840/19460 | Loss: 0.6825 | Acc: 0.9375 | LR: 1.24e-05
2026-01-07 20:11:15,002 [INFO] Step 5850/19460 | Loss: 0.9699 | Acc: 0.7500 | LR: 1.24e-05
2026-01-07 20:11:18,203 [INFO] Step 5860/19460 | Loss: 0.8500 | Acc: 0.7500 | LR: 1.24e-05
2026-01-07 20:11:21,377 [INFO] Step 5870/19460 | Loss: 0.9535 | Acc: 0.7500 | LR: 1.24e-05
2026-01-07 20:11:24,829 [INFO] Step 5880/19460 | Loss: 0.5410 | Acc: 1.0000 | LR: 1.24e-05
2026-01-07 20:11:27,898 [INFO] Step 5890/19460 | Loss: 0.9341 | Acc: 0.7500 | LR: 1.24e-05
2026-01-07 20:11:31,730 [INFO] Step 5900/19460 | Loss: 1.2010 | Acc: 0.6250 | LR: 1.24e-05
2026-01-07 20:11:40,419 [INFO] [EVAL] Step 5900 | Val Loss: 0.3770 | Val Acc: 0.8931
2026-01-07 20:11:43,657 [INFO] Step 5910/19460 | Loss: 0.8530 | Acc: 0.8125 | LR: 1.24e-05
2026-01-07 20:11:46,884 [INFO] Step 5920/19460 | Loss: 0.9830 | Acc: 0.6875 | LR: 1.24e-05
2026-01-07 20:11:49,954 [INFO] Step 5930/19460 | Loss: 1.0050 | Acc: 0.6875 | LR: 1.24e-05
2026-01-07 20:11:53,088 [INFO] Step 5940/19460 | Loss: 0.6383 | Acc: 0.8125 | LR: 1.23e-05
2026-01-07 20:11:56,107 [INFO] Step 5950/19460 | Loss: 0.8077 | Acc: 0.8125 | LR: 1.23e-05
2026-01-07 20:11:59,152 [INFO] Step 5960/19460 | Loss: 0.5704 | Acc: 0.9375 | LR: 1.23e-05
2026-01-07 20:12:02,225 [INFO] Step 5970/19460 | Loss: 1.1565 | Acc: 0.6875 | LR: 1.23e-05
2026-01-07 20:12:05,386 [INFO] Step 5980/19460 | Loss: 0.6117 | Acc: 0.8750 | LR: 1.23e-05
2026-01-07 20:12:08,718 [INFO] Step 5990/19460 | Loss: 0.7308 | Acc: 0.8750 | LR: 1.23e-05
2026-01-07 20:12:12,342 [INFO] Step 6000/19460 | Loss: 0.8899 | Acc: 0.8125 | LR: 1.23e-05
2026-01-07 20:12:21,038 [INFO] [EVAL] Step 6000 | Val Loss: 0.4187 | Val Acc: 0.8812
2026-01-07 20:12:21,066 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_6000.npz
2026-01-07 20:12:24,337 [INFO] Step 6010/19460 | Loss: 0.5713 | Acc: 0.9375 | LR: 1.23e-05
2026-01-07 20:12:27,605 [INFO] Step 6020/19460 | Loss: 0.7848 | Acc: 0.8750 | LR: 1.23e-05
2026-01-07 20:12:30,762 [INFO] Step 6030/19460 | Loss: 0.8142 | Acc: 0.8125 | LR: 1.23e-05
2026-01-07 20:12:34,006 [INFO] Step 6040/19460 | Loss: 0.7021 | Acc: 0.8750 | LR: 1.23e-05
2026-01-07 20:12:37,197 [INFO] Step 6050/19460 | Loss: 0.9398 | Acc: 0.7500 | LR: 1.22e-05
2026-01-07 20:12:40,458 [INFO] Step 6060/19460 | Loss: 0.8331 | Acc: 0.8125 | LR: 1.22e-05
2026-01-07 20:12:43,775 [INFO] Step 6070/19460 | Loss: 0.8156 | Acc: 0.8125 | LR: 1.22e-05
2026-01-07 20:12:47,123 [INFO] Step 6080/19460 | Loss: 0.5589 | Acc: 0.9375 | LR: 1.22e-05
2026-01-07 20:12:50,326 [INFO] Step 6090/19460 | Loss: 0.5701 | Acc: 0.9375 | LR: 1.22e-05
2026-01-07 20:12:53,996 [INFO] Step 6100/19460 | Loss: 0.6637 | Acc: 0.8750 | LR: 1.22e-05
2026-01-07 20:13:02,743 [INFO] [EVAL] Step 6100 | Val Loss: 0.3608 | Val Acc: 0.9012
2026-01-07 20:13:06,040 [INFO] Step 6110/19460 | Loss: 0.6262 | Acc: 0.9375 | LR: 1.22e-05
2026-01-07 20:13:09,285 [INFO] Step 6120/19460 | Loss: 0.7179 | Acc: 0.8750 | LR: 1.22e-05
2026-01-07 20:13:12,488 [INFO] Step 6130/19460 | Loss: 0.8694 | Acc: 0.7500 | LR: 1.22e-05
2026-01-07 20:13:15,862 [INFO] Step 6140/19460 | Loss: 0.7014 | Acc: 0.8125 | LR: 1.22e-05
2026-01-07 20:13:18,995 [INFO] Step 6150/19460 | Loss: 0.5755 | Acc: 0.9375 | LR: 1.22e-05
2026-01-07 20:13:22,161 [INFO] Step 6160/19460 | Loss: 0.6984 | Acc: 0.8750 | LR: 1.21e-05
2026-01-07 20:13:25,379 [INFO] Step 6170/19460 | Loss: 1.0282 | Acc: 0.8125 | LR: 1.21e-05
2026-01-07 20:13:28,478 [INFO] Step 6180/19460 | Loss: 1.0102 | Acc: 0.6250 | LR: 1.21e-05
2026-01-07 20:13:31,664 [INFO] Step 6190/19460 | Loss: 0.9078 | Acc: 0.7500 | LR: 1.21e-05
2026-01-07 20:13:35,521 [INFO] Step 6200/19460 | Loss: 0.7302 | Acc: 0.8125 | LR: 1.21e-05
2026-01-07 20:13:44,227 [INFO] [EVAL] Step 6200 | Val Loss: 0.3747 | Val Acc: 0.9062
2026-01-07 20:13:44,265 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:13:44,267 [INFO] New best validation accuracy: 0.9062
2026-01-07 20:13:47,568 [INFO] Step 6210/19460 | Loss: 0.8328 | Acc: 0.8125 | LR: 1.21e-05
2026-01-07 20:13:50,821 [INFO] Step 6220/19460 | Loss: 0.7359 | Acc: 0.7500 | LR: 1.21e-05
2026-01-07 20:13:54,112 [INFO] Step 6230/19460 | Loss: 0.7434 | Acc: 0.8750 | LR: 1.21e-05
2026-01-07 20:13:57,245 [INFO] Step 6240/19460 | Loss: 0.7863 | Acc: 0.8750 | LR: 1.21e-05
2026-01-07 20:14:00,322 [INFO] Step 6250/19460 | Loss: 0.7899 | Acc: 0.8125 | LR: 1.21e-05
2026-01-07 20:14:03,324 [INFO] Step 6260/19460 | Loss: 0.6125 | Acc: 0.8750 | LR: 1.20e-05
2026-01-07 20:14:06,439 [INFO] Step 6270/19460 | Loss: 0.8805 | Acc: 0.8125 | LR: 1.20e-05
2026-01-07 20:14:09,668 [INFO] Step 6280/19460 | Loss: 0.8266 | Acc: 0.8125 | LR: 1.20e-05
2026-01-07 20:14:12,654 [INFO] Step 6290/19460 | Loss: 1.0507 | Acc: 0.6875 | LR: 1.20e-05
2026-01-07 20:14:16,276 [INFO] Step 6300/19460 | Loss: 0.9668 | Acc: 0.7500 | LR: 1.20e-05
2026-01-07 20:14:24,974 [INFO] [EVAL] Step 6300 | Val Loss: 0.3940 | Val Acc: 0.8919
2026-01-07 20:14:28,232 [INFO] Step 6310/19460 | Loss: 0.8983 | Acc: 0.8125 | LR: 1.20e-05
2026-01-07 20:14:31,596 [INFO] Step 6320/19460 | Loss: 0.7922 | Acc: 0.8750 | LR: 1.20e-05
2026-01-07 20:14:34,736 [INFO] Step 6330/19460 | Loss: 0.5617 | Acc: 0.9375 | LR: 1.20e-05
2026-01-07 20:14:37,896 [INFO] Step 6340/19460 | Loss: 0.8052 | Acc: 0.8125 | LR: 1.20e-05
2026-01-07 20:14:41,277 [INFO] Step 6350/19460 | Loss: 0.8222 | Acc: 0.7500 | LR: 1.20e-05
2026-01-07 20:14:44,485 [INFO] Step 6360/19460 | Loss: 0.8228 | Acc: 0.8125 | LR: 1.20e-05
2026-01-07 20:14:47,683 [INFO] Step 6370/19460 | Loss: 0.8033 | Acc: 0.7500 | LR: 1.19e-05
2026-01-07 20:14:50,836 [INFO] Step 6380/19460 | Loss: 0.7469 | Acc: 0.8125 | LR: 1.19e-05
2026-01-07 20:14:54,128 [INFO] Step 6390/19460 | Loss: 0.7489 | Acc: 0.8750 | LR: 1.19e-05
2026-01-07 20:14:57,895 [INFO] Step 6400/19460 | Loss: 1.2233 | Acc: 0.6250 | LR: 1.19e-05
2026-01-07 20:15:06,592 [INFO] [EVAL] Step 6400 | Val Loss: 0.3890 | Val Acc: 0.8956
2026-01-07 20:15:09,826 [INFO] Step 6410/19460 | Loss: 1.0347 | Acc: 0.7500 | LR: 1.19e-05
2026-01-07 20:15:13,101 [INFO] Step 6420/19460 | Loss: 0.6234 | Acc: 0.8750 | LR: 1.19e-05
2026-01-07 20:15:16,183 [INFO] Step 6430/19460 | Loss: 0.7785 | Acc: 0.9375 | LR: 1.19e-05
2026-01-07 20:15:19,349 [INFO] Step 6440/19460 | Loss: 0.6809 | Acc: 0.8750 | LR: 1.19e-05
2026-01-07 20:15:22,584 [INFO] Step 6450/19460 | Loss: 1.1058 | Acc: 0.5625 | LR: 1.19e-05
2026-01-07 20:15:25,742 [INFO] Step 6460/19460 | Loss: 0.7111 | Acc: 0.8750 | LR: 1.19e-05
2026-01-07 20:15:29,120 [INFO] Step 6470/19460 | Loss: 1.0991 | Acc: 0.6250 | LR: 1.18e-05
2026-01-07 20:15:32,363 [INFO] Step 6480/19460 | Loss: 0.6881 | Acc: 0.8750 | LR: 1.18e-05
2026-01-07 20:15:35,610 [INFO] Step 6490/19460 | Loss: 0.7978 | Acc: 0.8750 | LR: 1.18e-05
2026-01-07 20:15:39,416 [INFO] Step 6500/19460 | Loss: 0.8491 | Acc: 0.8125 | LR: 1.18e-05
2026-01-07 20:15:48,130 [INFO] [EVAL] Step 6500 | Val Loss: 0.3675 | Val Acc: 0.9044
2026-01-07 20:15:48,158 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_6500.npz
2026-01-07 20:15:51,409 [INFO] Step 6510/19460 | Loss: 0.6570 | Acc: 0.9375 | LR: 1.18e-05
2026-01-07 20:15:54,612 [INFO] Step 6520/19460 | Loss: 0.9142 | Acc: 0.7500 | LR: 1.18e-05
2026-01-07 20:15:57,859 [INFO] Step 6530/19460 | Loss: 0.7099 | Acc: 0.8750 | LR: 1.18e-05
2026-01-07 20:16:01,008 [INFO] Step 6540/19460 | Loss: 0.8928 | Acc: 0.6875 | LR: 1.18e-05
2026-01-07 20:16:04,127 [INFO] Step 6550/19460 | Loss: 0.6934 | Acc: 0.8750 | LR: 1.18e-05
2026-01-07 20:16:07,272 [INFO] Step 6560/19460 | Loss: 0.9104 | Acc: 0.7500 | LR: 1.18e-05
2026-01-07 20:16:10,430 [INFO] Step 6570/19460 | Loss: 0.8353 | Acc: 0.8125 | LR: 1.17e-05
2026-01-07 20:16:13,671 [INFO] Step 6580/19460 | Loss: 0.9136 | Acc: 0.8125 | LR: 1.17e-05
2026-01-07 20:16:15,721 [WARNING] Skipping batch due to non-finite loss at step=6586 (loss=nan, epoch=7).
2026-01-07 20:16:16,978 [INFO] Step 6590/19460 | Loss: 1.2558 | Acc: 0.6250 | LR: 1.17e-05
2026-01-07 20:16:20,639 [INFO] Step 6600/19460 | Loss: 0.8103 | Acc: 0.7500 | LR: 1.17e-05
2026-01-07 20:16:29,393 [INFO] [EVAL] Step 6600 | Val Loss: 0.3783 | Val Acc: 0.8912
2026-01-07 20:16:32,628 [INFO] Step 6610/19460 | Loss: 0.6355 | Acc: 0.9375 | LR: 1.17e-05
2026-01-07 20:16:35,794 [INFO] Step 6620/19460 | Loss: 0.8898 | Acc: 0.8125 | LR: 1.17e-05
2026-01-07 20:16:39,020 [INFO] Step 6630/19460 | Loss: 0.5115 | Acc: 1.0000 | LR: 1.17e-05
2026-01-07 20:16:42,262 [INFO] Step 6640/19460 | Loss: 0.6482 | Acc: 0.8750 | LR: 1.17e-05
2026-01-07 20:16:45,561 [INFO] Step 6650/19460 | Loss: 0.6101 | Acc: 0.8750 | LR: 1.17e-05
2026-01-07 20:16:48,938 [INFO] Step 6660/19460 | Loss: 0.8277 | Acc: 0.7500 | LR: 1.17e-05
2026-01-07 20:16:52,287 [INFO] Step 6670/19460 | Loss: 0.5935 | Acc: 0.9375 | LR: 1.17e-05
2026-01-07 20:16:55,479 [INFO] Step 6680/19460 | Loss: 0.8011 | Acc: 0.8125 | LR: 1.16e-05
2026-01-07 20:16:58,782 [INFO] Step 6690/19460 | Loss: 0.8484 | Acc: 0.8750 | LR: 1.16e-05
2026-01-07 20:17:02,443 [INFO] Step 6700/19460 | Loss: 0.7099 | Acc: 0.8750 | LR: 1.16e-05
2026-01-07 20:17:11,141 [INFO] [EVAL] Step 6700 | Val Loss: 0.3614 | Val Acc: 0.9044
2026-01-07 20:17:14,336 [INFO] Step 6710/19460 | Loss: 1.1285 | Acc: 0.6875 | LR: 1.16e-05
2026-01-07 20:17:14,487 [WARNING] Skipping batch due to non-finite loss at step=6710 (loss=nan, epoch=7).
2026-01-07 20:17:17,799 [INFO] Step 6720/19460 | Loss: 0.8572 | Acc: 0.8750 | LR: 1.16e-05
2026-01-07 20:17:21,075 [INFO] Step 6730/19460 | Loss: 0.7280 | Acc: 0.9375 | LR: 1.16e-05
2026-01-07 20:17:24,301 [INFO] Step 6740/19460 | Loss: 0.6601 | Acc: 0.8750 | LR: 1.16e-05
2026-01-07 20:17:25,705 [WARNING] Skipping batch due to non-finite loss at step=6744 (loss=nan, epoch=7).
2026-01-07 20:17:27,628 [INFO] Step 6750/19460 | Loss: 0.7155 | Acc: 0.8125 | LR: 1.16e-05
2026-01-07 20:17:30,981 [INFO] Step 6760/19460 | Loss: 0.6683 | Acc: 0.8750 | LR: 1.16e-05
2026-01-07 20:17:31,258 [INFO] Epoch 7 complete | Avg Loss: 0.7917 | Avg Acc: 0.8255 | Updates: 968 | Micro-batches: 971 | Skipped: 3 (loss=3, logits=0, grads=0)
2026-01-07 20:17:31,258 [INFO] Epoch 8/20
2026-01-07 20:17:34,228 [INFO] Step 6770/19460 | Loss: 0.7056 | Acc: 0.8125 | LR: 1.16e-05
2026-01-07 20:17:37,368 [INFO] Step 6780/19460 | Loss: 0.9906 | Acc: 0.7500 | LR: 1.15e-05
2026-01-07 20:17:40,548 [INFO] Step 6790/19460 | Loss: 0.8546 | Acc: 0.8125 | LR: 1.15e-05
2026-01-07 20:17:44,229 [INFO] Step 6800/19460 | Loss: 0.6968 | Acc: 0.8750 | LR: 1.15e-05
2026-01-07 20:17:52,901 [INFO] [EVAL] Step 6800 | Val Loss: 0.3636 | Val Acc: 0.9025
2026-01-07 20:17:56,115 [INFO] Step 6810/19460 | Loss: 0.6200 | Acc: 0.8750 | LR: 1.15e-05
2026-01-07 20:17:59,457 [INFO] Step 6820/19460 | Loss: 0.7838 | Acc: 0.7500 | LR: 1.15e-05
2026-01-07 20:18:02,719 [INFO] Step 6830/19460 | Loss: 0.7138 | Acc: 0.8125 | LR: 1.15e-05
2026-01-07 20:18:05,895 [INFO] Step 6840/19460 | Loss: 0.9015 | Acc: 0.8750 | LR: 1.15e-05
2026-01-07 20:18:09,119 [INFO] Step 6850/19460 | Loss: 0.6806 | Acc: 0.8750 | LR: 1.15e-05
2026-01-07 20:18:12,353 [INFO] Step 6860/19460 | Loss: 0.6156 | Acc: 0.9375 | LR: 1.15e-05
2026-01-07 20:18:15,555 [INFO] Step 6870/19460 | Loss: 0.7841 | Acc: 0.8125 | LR: 1.15e-05
2026-01-07 20:18:18,731 [INFO] Step 6880/19460 | Loss: 0.9478 | Acc: 0.8125 | LR: 1.14e-05
2026-01-07 20:18:22,010 [INFO] Step 6890/19460 | Loss: 0.5638 | Acc: 0.9375 | LR: 1.14e-05
2026-01-07 20:18:25,526 [INFO] Step 6900/19460 | Loss: 0.8561 | Acc: 0.8125 | LR: 1.14e-05
2026-01-07 20:18:34,161 [INFO] [EVAL] Step 6900 | Val Loss: 0.3891 | Val Acc: 0.8862
2026-01-07 20:18:37,420 [INFO] Step 6910/19460 | Loss: 1.1023 | Acc: 0.6875 | LR: 1.14e-05
2026-01-07 20:18:40,647 [INFO] Step 6920/19460 | Loss: 0.7590 | Acc: 0.7500 | LR: 1.14e-05
2026-01-07 20:18:43,812 [INFO] Step 6930/19460 | Loss: 0.9473 | Acc: 0.8125 | LR: 1.14e-05
2026-01-07 20:18:47,019 [INFO] Step 6940/19460 | Loss: 0.4964 | Acc: 1.0000 | LR: 1.14e-05
2026-01-07 20:18:50,089 [INFO] Step 6950/19460 | Loss: 0.7297 | Acc: 0.8750 | LR: 1.14e-05
2026-01-07 20:18:53,292 [INFO] Step 6960/19460 | Loss: 0.8634 | Acc: 0.8750 | LR: 1.14e-05
2026-01-07 20:18:56,585 [INFO] Step 6970/19460 | Loss: 0.4611 | Acc: 1.0000 | LR: 1.13e-05
2026-01-07 20:18:59,784 [INFO] Step 6980/19460 | Loss: 1.2366 | Acc: 0.5000 | LR: 1.13e-05
2026-01-07 20:19:03,035 [INFO] Step 6990/19460 | Loss: 0.4422 | Acc: 1.0000 | LR: 1.13e-05
2026-01-07 20:19:06,718 [INFO] Step 7000/19460 | Loss: 0.6409 | Acc: 0.9375 | LR: 1.13e-05
2026-01-07 20:19:15,457 [INFO] [EVAL] Step 7000 | Val Loss: 0.3578 | Val Acc: 0.9050
2026-01-07 20:19:15,485 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_7000.npz
2026-01-07 20:19:18,726 [INFO] Step 7010/19460 | Loss: 0.5744 | Acc: 0.9375 | LR: 1.13e-05
2026-01-07 20:19:21,896 [INFO] Step 7020/19460 | Loss: 0.7702 | Acc: 0.8125 | LR: 1.13e-05
2026-01-07 20:19:25,237 [INFO] Step 7030/19460 | Loss: 0.5449 | Acc: 0.9375 | LR: 1.13e-05
2026-01-07 20:19:28,563 [INFO] Step 7040/19460 | Loss: 0.9530 | Acc: 0.8125 | LR: 1.13e-05
2026-01-07 20:19:31,920 [INFO] Step 7050/19460 | Loss: 0.8375 | Acc: 0.8125 | LR: 1.13e-05
2026-01-07 20:19:35,033 [INFO] Step 7060/19460 | Loss: 1.3065 | Acc: 0.6250 | LR: 1.13e-05
2026-01-07 20:19:38,133 [INFO] Step 7070/19460 | Loss: 1.1898 | Acc: 0.6250 | LR: 1.12e-05
2026-01-07 20:19:41,229 [INFO] Step 7080/19460 | Loss: 0.8053 | Acc: 0.8125 | LR: 1.12e-05
2026-01-07 20:19:44,479 [INFO] Step 7090/19460 | Loss: 0.9196 | Acc: 0.8125 | LR: 1.12e-05
2026-01-07 20:19:48,200 [INFO] Step 7100/19460 | Loss: 0.6629 | Acc: 0.9375 | LR: 1.12e-05
2026-01-07 20:19:56,929 [INFO] [EVAL] Step 7100 | Val Loss: 0.3672 | Val Acc: 0.9006
2026-01-07 20:20:00,095 [INFO] Step 7110/19460 | Loss: 0.8280 | Acc: 0.8125 | LR: 1.12e-05
2026-01-07 20:20:03,267 [INFO] Step 7120/19460 | Loss: 0.8833 | Acc: 0.6875 | LR: 1.12e-05
2026-01-07 20:20:06,593 [INFO] Step 7130/19460 | Loss: 0.8742 | Acc: 0.8125 | LR: 1.12e-05
2026-01-07 20:20:09,752 [INFO] Step 7140/19460 | Loss: 1.0575 | Acc: 0.7500 | LR: 1.12e-05
2026-01-07 20:20:12,896 [INFO] Step 7150/19460 | Loss: 0.7060 | Acc: 0.8125 | LR: 1.12e-05
2026-01-07 20:20:16,131 [INFO] Step 7160/19460 | Loss: 0.5826 | Acc: 0.8750 | LR: 1.12e-05
2026-01-07 20:20:19,397 [INFO] Step 7170/19460 | Loss: 0.9841 | Acc: 0.6875 | LR: 1.11e-05
2026-01-07 20:20:22,651 [INFO] Step 7180/19460 | Loss: 0.5788 | Acc: 0.9375 | LR: 1.11e-05
2026-01-07 20:20:25,910 [INFO] Step 7190/19460 | Loss: 0.7307 | Acc: 0.8750 | LR: 1.11e-05
2026-01-07 20:20:29,559 [INFO] Step 7200/19460 | Loss: 0.6219 | Acc: 0.9375 | LR: 1.11e-05
2026-01-07 20:20:38,245 [INFO] [EVAL] Step 7200 | Val Loss: 0.3458 | Val Acc: 0.9050
2026-01-07 20:20:41,601 [INFO] Step 7210/19460 | Loss: 0.5712 | Acc: 0.9375 | LR: 1.11e-05
2026-01-07 20:20:44,854 [INFO] Step 7220/19460 | Loss: 0.7119 | Acc: 0.8125 | LR: 1.11e-05
2026-01-07 20:20:48,072 [INFO] Step 7230/19460 | Loss: 0.7616 | Acc: 0.8125 | LR: 1.11e-05
2026-01-07 20:20:51,311 [INFO] Step 7240/19460 | Loss: 0.6404 | Acc: 0.9375 | LR: 1.11e-05
2026-01-07 20:20:54,617 [INFO] Step 7250/19460 | Loss: 0.9787 | Acc: 0.8125 | LR: 1.11e-05
2026-01-07 20:20:58,037 [INFO] Step 7260/19460 | Loss: 0.7671 | Acc: 0.8125 | LR: 1.11e-05
2026-01-07 20:21:01,262 [INFO] Step 7270/19460 | Loss: 0.7944 | Acc: 0.8125 | LR: 1.10e-05
2026-01-07 20:21:04,444 [INFO] Step 7280/19460 | Loss: 1.0938 | Acc: 0.6875 | LR: 1.10e-05
2026-01-07 20:21:07,707 [INFO] Step 7290/19460 | Loss: 0.7961 | Acc: 0.8125 | LR: 1.10e-05
2026-01-07 20:21:11,469 [INFO] Step 7300/19460 | Loss: 1.0447 | Acc: 0.7500 | LR: 1.10e-05
2026-01-07 20:21:20,145 [INFO] [EVAL] Step 7300 | Val Loss: 0.3777 | Val Acc: 0.8938
2026-01-07 20:21:23,407 [INFO] Step 7310/19460 | Loss: 1.0203 | Acc: 0.6875 | LR: 1.10e-05
2026-01-07 20:21:26,514 [INFO] Step 7320/19460 | Loss: 0.5967 | Acc: 0.9375 | LR: 1.10e-05
2026-01-07 20:21:29,736 [INFO] Step 7330/19460 | Loss: 0.9262 | Acc: 0.8125 | LR: 1.10e-05
2026-01-07 20:21:33,087 [INFO] Step 7340/19460 | Loss: 1.2018 | Acc: 0.6875 | LR: 1.10e-05
2026-01-07 20:21:36,324 [INFO] Step 7350/19460 | Loss: 0.8088 | Acc: 0.7500 | LR: 1.10e-05
2026-01-07 20:21:39,320 [INFO] Step 7360/19460 | Loss: 0.7177 | Acc: 0.8750 | LR: 1.09e-05
2026-01-07 20:21:42,463 [INFO] Step 7370/19460 | Loss: 0.7011 | Acc: 0.8750 | LR: 1.09e-05
2026-01-07 20:21:45,605 [INFO] Step 7380/19460 | Loss: 0.8703 | Acc: 0.7500 | LR: 1.09e-05
2026-01-07 20:21:48,689 [INFO] Step 7390/19460 | Loss: 0.7293 | Acc: 0.8750 | LR: 1.09e-05
2026-01-07 20:21:52,407 [INFO] Step 7400/19460 | Loss: 0.9618 | Acc: 0.7500 | LR: 1.09e-05
2026-01-07 20:22:01,129 [INFO] [EVAL] Step 7400 | Val Loss: 0.3522 | Val Acc: 0.9087
2026-01-07 20:22:01,163 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:22:01,164 [INFO] New best validation accuracy: 0.9087
2026-01-07 20:22:04,567 [INFO] Step 7410/19460 | Loss: 0.4816 | Acc: 1.0000 | LR: 1.09e-05
2026-01-07 20:22:07,851 [INFO] Step 7420/19460 | Loss: 1.3552 | Acc: 0.5625 | LR: 1.09e-05
2026-01-07 20:22:11,044 [INFO] Step 7430/19460 | Loss: 1.0709 | Acc: 0.7500 | LR: 1.09e-05
2026-01-07 20:22:14,289 [INFO] Step 7440/19460 | Loss: 0.9324 | Acc: 0.8750 | LR: 1.09e-05
2026-01-07 20:22:17,384 [INFO] Step 7450/19460 | Loss: 0.9086 | Acc: 0.7500 | LR: 1.09e-05
2026-01-07 20:22:17,874 [WARNING] Skipping batch due to non-finite loss at step=7451 (loss=nan, epoch=8).
2026-01-07 20:22:20,803 [INFO] Step 7460/19460 | Loss: 0.9540 | Acc: 0.8125 | LR: 1.08e-05
2026-01-07 20:22:24,206 [INFO] Step 7470/19460 | Loss: 0.9885 | Acc: 0.7500 | LR: 1.08e-05
2026-01-07 20:22:27,403 [INFO] Step 7480/19460 | Loss: 0.6949 | Acc: 0.8750 | LR: 1.08e-05
2026-01-07 20:22:30,630 [INFO] Step 7490/19460 | Loss: 0.5491 | Acc: 1.0000 | LR: 1.08e-05
2026-01-07 20:22:34,311 [INFO] Step 7500/19460 | Loss: 0.6709 | Acc: 0.8750 | LR: 1.08e-05
2026-01-07 20:22:43,042 [INFO] [EVAL] Step 7500 | Val Loss: 0.3633 | Val Acc: 0.9025
2026-01-07 20:22:43,069 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_7500.npz
2026-01-07 20:22:46,261 [INFO] Step 7510/19460 | Loss: 0.7088 | Acc: 0.8750 | LR: 1.08e-05
2026-01-07 20:22:49,342 [INFO] Step 7520/19460 | Loss: 0.7521 | Acc: 0.8750 | LR: 1.08e-05
2026-01-07 20:22:52,722 [INFO] Step 7530/19460 | Loss: 0.8807 | Acc: 0.7500 | LR: 1.08e-05
2026-01-07 20:22:55,873 [INFO] Step 7540/19460 | Loss: 0.9597 | Acc: 0.7500 | LR: 1.08e-05
2026-01-07 20:22:59,218 [INFO] Step 7550/19460 | Loss: 0.7447 | Acc: 0.8750 | LR: 1.07e-05
2026-01-07 20:23:00,944 [WARNING] Skipping batch due to non-finite loss at step=7555 (loss=nan, epoch=8).
2026-01-07 20:23:02,536 [INFO] Step 7560/19460 | Loss: 0.9096 | Acc: 0.7500 | LR: 1.07e-05
2026-01-07 20:23:05,760 [INFO] Step 7570/19460 | Loss: 1.0487 | Acc: 0.6875 | LR: 1.07e-05
2026-01-07 20:23:09,059 [INFO] Step 7580/19460 | Loss: 0.8634 | Acc: 0.7500 | LR: 1.07e-05
2026-01-07 20:23:12,168 [INFO] Step 7590/19460 | Loss: 0.8372 | Acc: 0.8125 | LR: 1.07e-05
2026-01-07 20:23:16,075 [INFO] Step 7600/19460 | Loss: 0.7880 | Acc: 0.7500 | LR: 1.07e-05
2026-01-07 20:23:24,762 [INFO] [EVAL] Step 7600 | Val Loss: 0.3586 | Val Acc: 0.9131
2026-01-07 20:23:24,795 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:23:24,796 [INFO] New best validation accuracy: 0.9131
2026-01-07 20:23:27,998 [INFO] Step 7610/19460 | Loss: 0.6399 | Acc: 0.9375 | LR: 1.07e-05
2026-01-07 20:23:31,239 [INFO] Step 7620/19460 | Loss: 0.8362 | Acc: 0.7500 | LR: 1.07e-05
2026-01-07 20:23:34,500 [INFO] Step 7630/19460 | Loss: 0.7117 | Acc: 0.8125 | LR: 1.07e-05
2026-01-07 20:23:37,636 [INFO] Step 7640/19460 | Loss: 0.6573 | Acc: 0.9375 | LR: 1.06e-05
2026-01-07 20:23:40,989 [INFO] Step 7650/19460 | Loss: 0.6778 | Acc: 0.8125 | LR: 1.06e-05
2026-01-07 20:23:44,361 [INFO] Step 7660/19460 | Loss: 0.9638 | Acc: 0.6875 | LR: 1.06e-05
2026-01-07 20:23:47,784 [INFO] Step 7670/19460 | Loss: 0.6367 | Acc: 0.9375 | LR: 1.06e-05
2026-01-07 20:23:50,985 [INFO] Step 7680/19460 | Loss: 0.6275 | Acc: 0.8750 | LR: 1.06e-05
2026-01-07 20:23:54,372 [INFO] Step 7690/19460 | Loss: 0.8620 | Acc: 0.7500 | LR: 1.06e-05
2026-01-07 20:23:58,177 [INFO] Step 7700/19460 | Loss: 0.8420 | Acc: 0.8125 | LR: 1.06e-05
2026-01-07 20:24:06,875 [INFO] [EVAL] Step 7700 | Val Loss: 0.3815 | Val Acc: 0.8994
2026-01-07 20:24:10,119 [INFO] Step 7710/19460 | Loss: 0.7509 | Acc: 0.8750 | LR: 1.06e-05
2026-01-07 20:24:13,457 [INFO] Step 7720/19460 | Loss: 0.7059 | Acc: 0.8125 | LR: 1.06e-05
2026-01-07 20:24:16,693 [INFO] Step 7730/19460 | Loss: 0.5731 | Acc: 0.8750 | LR: 1.06e-05
2026-01-07 20:24:16,693 [INFO] Epoch 8 complete | Avg Loss: 0.7726 | Avg Acc: 0.8368 | Updates: 969 | Micro-batches: 971 | Skipped: 2 (loss=2, logits=0, grads=0)
2026-01-07 20:24:16,693 [INFO] Epoch 9/20
2026-01-07 20:24:19,871 [INFO] Step 7740/19460 | Loss: 0.6201 | Acc: 0.9375 | LR: 1.05e-05
2026-01-07 20:24:22,963 [INFO] Step 7750/19460 | Loss: 0.6164 | Acc: 0.9375 | LR: 1.05e-05
2026-01-07 20:24:26,213 [INFO] Step 7760/19460 | Loss: 0.6551 | Acc: 0.8750 | LR: 1.05e-05
2026-01-07 20:24:29,488 [INFO] Step 7770/19460 | Loss: 0.9526 | Acc: 0.7500 | LR: 1.05e-05
2026-01-07 20:24:32,789 [INFO] Step 7780/19460 | Loss: 0.7750 | Acc: 0.8125 | LR: 1.05e-05
2026-01-07 20:24:36,071 [INFO] Step 7790/19460 | Loss: 0.5001 | Acc: 1.0000 | LR: 1.05e-05
2026-01-07 20:24:39,916 [INFO] Step 7800/19460 | Loss: 0.6225 | Acc: 0.9375 | LR: 1.05e-05
2026-01-07 20:24:48,647 [INFO] [EVAL] Step 7800 | Val Loss: 0.3660 | Val Acc: 0.8988
2026-01-07 20:24:51,934 [INFO] Step 7810/19460 | Loss: 0.5471 | Acc: 0.9375 | LR: 1.05e-05
2026-01-07 20:24:55,113 [INFO] Step 7820/19460 | Loss: 0.6304 | Acc: 0.9375 | LR: 1.05e-05
2026-01-07 20:24:58,288 [INFO] Step 7830/19460 | Loss: 0.7929 | Acc: 0.8125 | LR: 1.04e-05
2026-01-07 20:25:01,477 [INFO] Step 7840/19460 | Loss: 0.8337 | Acc: 0.8750 | LR: 1.04e-05
2026-01-07 20:25:04,673 [INFO] Step 7850/19460 | Loss: 1.1637 | Acc: 0.5625 | LR: 1.04e-05
2026-01-07 20:25:07,999 [INFO] Step 7860/19460 | Loss: 0.7496 | Acc: 0.8750 | LR: 1.04e-05
2026-01-07 20:25:11,211 [INFO] Step 7870/19460 | Loss: 0.6956 | Acc: 0.8750 | LR: 1.04e-05
2026-01-07 20:25:14,592 [INFO] Step 7880/19460 | Loss: 0.6192 | Acc: 0.9375 | LR: 1.04e-05
2026-01-07 20:25:17,664 [INFO] Step 7890/19460 | Loss: 0.8222 | Acc: 0.8125 | LR: 1.04e-05
2026-01-07 20:25:21,343 [INFO] Step 7900/19460 | Loss: 0.5459 | Acc: 0.8750 | LR: 1.04e-05
2026-01-07 20:25:30,043 [INFO] [EVAL] Step 7900 | Val Loss: 0.3593 | Val Acc: 0.9038
2026-01-07 20:25:33,236 [INFO] Step 7910/19460 | Loss: 0.6466 | Acc: 0.9375 | LR: 1.04e-05
2026-01-07 20:25:36,520 [INFO] Step 7920/19460 | Loss: 0.7925 | Acc: 0.8125 | LR: 1.03e-05
2026-01-07 20:25:39,783 [INFO] Step 7930/19460 | Loss: 0.9047 | Acc: 0.7500 | LR: 1.03e-05
2026-01-07 20:25:43,188 [INFO] Step 7940/19460 | Loss: 0.5364 | Acc: 1.0000 | LR: 1.03e-05
2026-01-07 20:25:46,471 [INFO] Step 7950/19460 | Loss: 0.6251 | Acc: 0.9375 | LR: 1.03e-05
2026-01-07 20:25:49,581 [INFO] Step 7960/19460 | Loss: 0.9479 | Acc: 0.8750 | LR: 1.03e-05
2026-01-07 20:25:52,834 [INFO] Step 7970/19460 | Loss: 0.7416 | Acc: 0.8750 | LR: 1.03e-05
2026-01-07 20:25:56,049 [INFO] Step 7980/19460 | Loss: 1.0138 | Acc: 0.6250 | LR: 1.03e-05
2026-01-07 20:25:59,306 [INFO] Step 7990/19460 | Loss: 0.8514 | Acc: 0.6875 | LR: 1.03e-05
2026-01-07 20:26:03,084 [INFO] Step 8000/19460 | Loss: 0.6565 | Acc: 0.8750 | LR: 1.03e-05
2026-01-07 20:26:11,782 [INFO] [EVAL] Step 8000 | Val Loss: 0.3596 | Val Acc: 0.9062
2026-01-07 20:26:11,810 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_8000.npz
2026-01-07 20:26:15,049 [INFO] Step 8010/19460 | Loss: 0.6223 | Acc: 0.8750 | LR: 1.02e-05
2026-01-07 20:26:18,146 [INFO] Step 8020/19460 | Loss: 0.5822 | Acc: 0.9375 | LR: 1.02e-05
2026-01-07 20:26:21,458 [INFO] Step 8030/19460 | Loss: 0.5994 | Acc: 0.8750 | LR: 1.02e-05
2026-01-07 20:26:24,640 [INFO] Step 8040/19460 | Loss: 0.8184 | Acc: 0.8125 | LR: 1.02e-05
2026-01-07 20:26:27,797 [INFO] Step 8050/19460 | Loss: 0.7437 | Acc: 0.8125 | LR: 1.02e-05
2026-01-07 20:26:30,879 [INFO] Step 8060/19460 | Loss: 0.4875 | Acc: 1.0000 | LR: 1.02e-05
2026-01-07 20:26:33,958 [INFO] Step 8070/19460 | Loss: 0.7636 | Acc: 0.8125 | LR: 1.02e-05
2026-01-07 20:26:37,105 [INFO] Step 8080/19460 | Loss: 0.7201 | Acc: 0.8750 | LR: 1.02e-05
2026-01-07 20:26:40,318 [INFO] Step 8090/19460 | Loss: 0.4807 | Acc: 1.0000 | LR: 1.02e-05
2026-01-07 20:26:44,147 [INFO] Step 8100/19460 | Loss: 0.7644 | Acc: 0.8750 | LR: 1.01e-05
2026-01-07 20:26:52,808 [INFO] [EVAL] Step 8100 | Val Loss: 0.3646 | Val Acc: 0.9044
2026-01-07 20:26:56,160 [INFO] Step 8110/19460 | Loss: 0.8553 | Acc: 0.7500 | LR: 1.01e-05
2026-01-07 20:26:59,248 [INFO] Step 8120/19460 | Loss: 0.6699 | Acc: 0.8125 | LR: 1.01e-05
2026-01-07 20:27:02,379 [INFO] Step 8130/19460 | Loss: 0.8905 | Acc: 0.8125 | LR: 1.01e-05
2026-01-07 20:27:05,594 [INFO] Step 8140/19460 | Loss: 0.9840 | Acc: 0.8125 | LR: 1.01e-05
2026-01-07 20:27:08,895 [INFO] Step 8150/19460 | Loss: 0.8610 | Acc: 0.8125 | LR: 1.01e-05
2026-01-07 20:27:12,207 [INFO] Step 8160/19460 | Loss: 1.0660 | Acc: 0.8125 | LR: 1.01e-05
2026-01-07 20:27:12,368 [WARNING] Skipping batch due to non-finite loss at step=8160 (loss=nan, epoch=9).
2026-01-07 20:27:15,527 [INFO] Step 8170/19460 | Loss: 0.8787 | Acc: 0.8125 | LR: 1.01e-05
2026-01-07 20:27:16,016 [WARNING] Skipping batch due to non-finite loss at step=8171 (loss=nan, epoch=9).
2026-01-07 20:27:18,981 [INFO] Step 8180/19460 | Loss: 0.5891 | Acc: 0.9375 | LR: 1.01e-05
2026-01-07 20:27:22,275 [INFO] Step 8190/19460 | Loss: 0.6655 | Acc: 0.8125 | LR: 1.00e-05
2026-01-07 20:27:26,002 [INFO] Step 8200/19460 | Loss: 0.8805 | Acc: 0.8125 | LR: 1.00e-05
2026-01-07 20:27:34,664 [INFO] [EVAL] Step 8200 | Val Loss: 0.3527 | Val Acc: 0.9087
2026-01-07 20:27:37,656 [INFO] Step 8210/19460 | Loss: 0.9757 | Acc: 0.8750 | LR: 1.00e-05
2026-01-07 20:27:40,634 [INFO] Step 8220/19460 | Loss: 0.9063 | Acc: 0.8125 | LR: 1.00e-05
2026-01-07 20:27:43,508 [INFO] Step 8230/19460 | Loss: 0.8178 | Acc: 0.8125 | LR: 1.00e-05
2026-01-07 20:27:46,629 [INFO] Step 8240/19460 | Loss: 0.7854 | Acc: 0.8750 | LR: 9.99e-06
2026-01-07 20:27:49,691 [INFO] Step 8250/19460 | Loss: 0.6299 | Acc: 0.9375 | LR: 9.98e-06
2026-01-07 20:27:52,763 [INFO] Step 8260/19460 | Loss: 0.5270 | Acc: 1.0000 | LR: 9.97e-06
2026-01-07 20:27:55,710 [INFO] Step 8270/19460 | Loss: 0.8419 | Acc: 0.8750 | LR: 9.96e-06
2026-01-07 20:27:56,442 [WARNING] Skipping batch due to non-finite loss at step=8272 (loss=nan, epoch=9).
2026-01-07 20:27:59,012 [INFO] Step 8280/19460 | Loss: 0.5678 | Acc: 1.0000 | LR: 9.95e-06
2026-01-07 20:28:02,151 [INFO] Step 8290/19460 | Loss: 0.8875 | Acc: 0.8125 | LR: 9.94e-06
2026-01-07 20:28:05,739 [INFO] Step 8300/19460 | Loss: 0.4676 | Acc: 1.0000 | LR: 9.92e-06
2026-01-07 20:28:14,380 [INFO] [EVAL] Step 8300 | Val Loss: 0.3590 | Val Acc: 0.8969
2026-01-07 20:28:17,552 [INFO] Step 8310/19460 | Loss: 0.4862 | Acc: 1.0000 | LR: 9.91e-06
2026-01-07 20:28:20,883 [INFO] Step 8320/19460 | Loss: 0.9982 | Acc: 0.6250 | LR: 9.90e-06
2026-01-07 20:28:24,085 [INFO] Step 8330/19460 | Loss: 0.6664 | Acc: 0.9375 | LR: 9.89e-06
2026-01-07 20:28:27,263 [INFO] Step 8340/19460 | Loss: 0.9973 | Acc: 0.7500 | LR: 9.88e-06
2026-01-07 20:28:30,571 [INFO] Step 8350/19460 | Loss: 1.0188 | Acc: 0.6875 | LR: 9.87e-06
2026-01-07 20:28:33,628 [INFO] Step 8360/19460 | Loss: 0.8606 | Acc: 0.6250 | LR: 9.86e-06
2026-01-07 20:28:36,873 [INFO] Step 8370/19460 | Loss: 0.9822 | Acc: 0.7500 | LR: 9.85e-06
2026-01-07 20:28:40,001 [INFO] Step 8380/19460 | Loss: 0.7108 | Acc: 0.8750 | LR: 9.84e-06
2026-01-07 20:28:43,364 [INFO] Step 8390/19460 | Loss: 0.9401 | Acc: 0.8125 | LR: 9.82e-06
2026-01-07 20:28:47,044 [INFO] Step 8400/19460 | Loss: 0.5746 | Acc: 1.0000 | LR: 9.81e-06
2026-01-07 20:28:55,697 [INFO] [EVAL] Step 8400 | Val Loss: 0.3566 | Val Acc: 0.9031
2026-01-07 20:28:58,881 [INFO] Step 8410/19460 | Loss: 0.7296 | Acc: 0.8750 | LR: 9.80e-06
2026-01-07 20:29:01,983 [INFO] Step 8420/19460 | Loss: 0.6859 | Acc: 0.8750 | LR: 9.79e-06
2026-01-07 20:29:05,132 [INFO] Step 8430/19460 | Loss: 0.8416 | Acc: 0.7500 | LR: 9.78e-06
2026-01-07 20:29:08,497 [INFO] Step 8440/19460 | Loss: 0.8783 | Acc: 0.8125 | LR: 9.77e-06
2026-01-07 20:29:11,592 [INFO] Step 8450/19460 | Loss: 1.2891 | Acc: 0.5625 | LR: 9.76e-06
2026-01-07 20:29:13,413 [WARNING] Skipping batch due to non-finite loss at step=8455 (loss=nan, epoch=9).
2026-01-07 20:29:15,028 [INFO] Step 8460/19460 | Loss: 0.8643 | Acc: 0.8125 | LR: 9.75e-06
2026-01-07 20:29:18,286 [INFO] Step 8470/19460 | Loss: 0.8431 | Acc: 0.8125 | LR: 9.73e-06
2026-01-07 20:29:21,424 [INFO] Step 8480/19460 | Loss: 0.7096 | Acc: 0.8750 | LR: 9.72e-06
2026-01-07 20:29:24,798 [INFO] Step 8490/19460 | Loss: 0.9700 | Acc: 0.7500 | LR: 9.71e-06
2026-01-07 20:29:28,518 [INFO] Step 8500/19460 | Loss: 1.0239 | Acc: 0.6250 | LR: 9.70e-06
2026-01-07 20:29:37,276 [INFO] [EVAL] Step 8500 | Val Loss: 0.3632 | Val Acc: 0.9006
2026-01-07 20:29:37,311 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_8500.npz
2026-01-07 20:29:40,533 [INFO] Step 8510/19460 | Loss: 0.5839 | Acc: 0.8750 | LR: 9.69e-06
2026-01-07 20:29:43,817 [INFO] Step 8520/19460 | Loss: 0.8064 | Acc: 0.8750 | LR: 9.68e-06
2026-01-07 20:29:47,073 [INFO] Step 8530/19460 | Loss: 0.6335 | Acc: 0.9375 | LR: 9.67e-06
2026-01-07 20:29:50,430 [INFO] Step 8540/19460 | Loss: 0.6179 | Acc: 0.9375 | LR: 9.66e-06
2026-01-07 20:29:53,672 [INFO] Step 8550/19460 | Loss: 1.0645 | Acc: 0.6875 | LR: 9.64e-06
2026-01-07 20:29:56,926 [INFO] Step 8560/19460 | Loss: 0.8420 | Acc: 0.8750 | LR: 9.63e-06
2026-01-07 20:30:00,359 [INFO] Step 8570/19460 | Loss: 0.7945 | Acc: 0.8750 | LR: 9.62e-06
2026-01-07 20:30:03,527 [INFO] Step 8580/19460 | Loss: 0.9092 | Acc: 0.7500 | LR: 9.61e-06
2026-01-07 20:30:06,834 [INFO] Step 8590/19460 | Loss: 0.8530 | Acc: 0.8125 | LR: 9.60e-06
2026-01-07 20:30:07,303 [WARNING] Skipping batch due to non-finite loss at step=8591 (loss=nan, epoch=9).
2026-01-07 20:30:10,910 [INFO] Step 8600/19460 | Loss: 0.7974 | Acc: 0.8125 | LR: 9.59e-06
2026-01-07 20:30:19,574 [INFO] [EVAL] Step 8600 | Val Loss: 0.3420 | Val Acc: 0.9075
2026-01-07 20:30:22,771 [INFO] Step 8610/19460 | Loss: 0.8753 | Acc: 0.8125 | LR: 9.58e-06
2026-01-07 20:30:25,897 [INFO] Step 8620/19460 | Loss: 0.7463 | Acc: 0.8125 | LR: 9.57e-06
2026-01-07 20:30:29,230 [INFO] Step 8630/19460 | Loss: 0.8486 | Acc: 0.7500 | LR: 9.55e-06
2026-01-07 20:30:32,443 [INFO] Step 8640/19460 | Loss: 0.5986 | Acc: 0.9375 | LR: 9.54e-06
2026-01-07 20:30:35,726 [INFO] Step 8650/19460 | Loss: 0.7299 | Acc: 0.8125 | LR: 9.53e-06
2026-01-07 20:30:38,904 [INFO] Step 8660/19460 | Loss: 0.7593 | Acc: 0.8750 | LR: 9.52e-06
2026-01-07 20:30:42,137 [INFO] Step 8670/19460 | Loss: 0.6620 | Acc: 0.9375 | LR: 9.51e-06
2026-01-07 20:30:45,360 [INFO] Step 8680/19460 | Loss: 0.5939 | Acc: 0.8750 | LR: 9.50e-06
2026-01-07 20:30:48,705 [INFO] Step 8690/19460 | Loss: 0.7774 | Acc: 0.7500 | LR: 9.49e-06
2026-01-07 20:30:50,586 [INFO] Epoch 9 complete | Avg Loss: 0.7669 | Avg Acc: 0.8373 | Updates: 966 | Micro-batches: 971 | Skipped: 5 (loss=5, logits=0, grads=0)
2026-01-07 20:30:50,586 [INFO] Epoch 10/20
2026-01-07 20:30:52,401 [INFO] Step 8700/19460 | Loss: 0.6941 | Acc: 0.8750 | LR: 9.47e-06
2026-01-07 20:31:01,131 [INFO] [EVAL] Step 8700 | Val Loss: 0.3561 | Val Acc: 0.9062
2026-01-07 20:31:04,368 [INFO] Step 8710/19460 | Loss: 0.9314 | Acc: 0.8125 | LR: 9.46e-06
2026-01-07 20:31:07,440 [INFO] Step 8720/19460 | Loss: 1.2449 | Acc: 0.6875 | LR: 9.45e-06
2026-01-07 20:31:10,682 [INFO] Step 8730/19460 | Loss: 0.8691 | Acc: 0.8125 | LR: 9.44e-06
2026-01-07 20:31:13,881 [INFO] Step 8740/19460 | Loss: 0.7617 | Acc: 0.8125 | LR: 9.43e-06
2026-01-07 20:31:17,283 [INFO] Step 8750/19460 | Loss: 1.1054 | Acc: 0.6875 | LR: 9.42e-06
2026-01-07 20:31:20,613 [INFO] Step 8760/19460 | Loss: 0.7019 | Acc: 0.8125 | LR: 9.41e-06
2026-01-07 20:31:23,843 [INFO] Step 8770/19460 | Loss: 0.5909 | Acc: 0.8750 | LR: 9.40e-06
2026-01-07 20:31:27,212 [INFO] Step 8780/19460 | Loss: 0.7461 | Acc: 0.8750 | LR: 9.38e-06
2026-01-07 20:31:30,460 [INFO] Step 8790/19460 | Loss: 0.6475 | Acc: 0.8750 | LR: 9.37e-06
2026-01-07 20:31:34,401 [INFO] Step 8800/19460 | Loss: 0.5494 | Acc: 0.9375 | LR: 9.36e-06
2026-01-07 20:31:43,100 [INFO] [EVAL] Step 8800 | Val Loss: 0.3419 | Val Acc: 0.9094
2026-01-07 20:31:46,411 [INFO] Step 8810/19460 | Loss: 0.8494 | Acc: 0.8125 | LR: 9.35e-06
2026-01-07 20:31:49,716 [INFO] Step 8820/19460 | Loss: 1.1089 | Acc: 0.6875 | LR: 9.34e-06
2026-01-07 20:31:53,030 [INFO] Step 8830/19460 | Loss: 1.1149 | Acc: 0.6875 | LR: 9.33e-06
2026-01-07 20:31:56,467 [INFO] Step 8840/19460 | Loss: 0.9681 | Acc: 0.7500 | LR: 9.32e-06
2026-01-07 20:31:59,634 [INFO] Step 8850/19460 | Loss: 0.6106 | Acc: 0.9375 | LR: 9.30e-06
2026-01-07 20:32:02,981 [INFO] Step 8860/19460 | Loss: 0.7184 | Acc: 0.8750 | LR: 9.29e-06
2026-01-07 20:32:06,236 [INFO] Step 8870/19460 | Loss: 0.7821 | Acc: 0.8125 | LR: 9.28e-06
2026-01-07 20:32:09,542 [INFO] Step 8880/19460 | Loss: 0.4808 | Acc: 1.0000 | LR: 9.27e-06
2026-01-07 20:32:12,790 [INFO] Step 8890/19460 | Loss: 0.6619 | Acc: 0.8125 | LR: 9.26e-06
2026-01-07 20:32:16,638 [INFO] Step 8900/19460 | Loss: 0.7213 | Acc: 0.8750 | LR: 9.25e-06
2026-01-07 20:32:25,286 [INFO] [EVAL] Step 8900 | Val Loss: 0.3451 | Val Acc: 0.9069
2026-01-07 20:32:28,427 [INFO] Step 8910/19460 | Loss: 0.9767 | Acc: 0.7500 | LR: 9.24e-06
2026-01-07 20:32:31,683 [INFO] Step 8920/19460 | Loss: 0.8358 | Acc: 0.7500 | LR: 9.22e-06
2026-01-07 20:32:34,911 [INFO] Step 8930/19460 | Loss: 0.7169 | Acc: 0.8125 | LR: 9.21e-06
2026-01-07 20:32:38,145 [INFO] Step 8940/19460 | Loss: 0.9114 | Acc: 0.7500 | LR: 9.20e-06
2026-01-07 20:32:41,453 [INFO] Step 8950/19460 | Loss: 0.4410 | Acc: 1.0000 | LR: 9.19e-06
2026-01-07 20:32:44,496 [INFO] Step 8960/19460 | Loss: 0.5393 | Acc: 0.9375 | LR: 9.18e-06
2026-01-07 20:32:47,612 [INFO] Step 8970/19460 | Loss: 0.6252 | Acc: 0.8750 | LR: 9.17e-06
2026-01-07 20:32:50,760 [INFO] Step 8980/19460 | Loss: 0.8354 | Acc: 0.8125 | LR: 9.16e-06
2026-01-07 20:32:54,027 [INFO] Step 8990/19460 | Loss: 0.7323 | Acc: 0.8125 | LR: 9.14e-06
2026-01-07 20:32:57,657 [INFO] Step 9000/19460 | Loss: 0.5733 | Acc: 1.0000 | LR: 9.13e-06
2026-01-07 20:33:06,361 [INFO] [EVAL] Step 9000 | Val Loss: 0.3559 | Val Acc: 0.9069
2026-01-07 20:33:06,396 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_9000.npz
2026-01-07 20:33:09,700 [INFO] Step 9010/19460 | Loss: 1.3647 | Acc: 0.6250 | LR: 9.12e-06
2026-01-07 20:33:12,959 [INFO] Step 9020/19460 | Loss: 0.5074 | Acc: 1.0000 | LR: 9.11e-06
2026-01-07 20:33:16,313 [INFO] Step 9030/19460 | Loss: 0.5101 | Acc: 0.9375 | LR: 9.10e-06
2026-01-07 20:33:19,496 [INFO] Step 9040/19460 | Loss: 0.7491 | Acc: 0.8750 | LR: 9.09e-06
2026-01-07 20:33:22,716 [INFO] Step 9050/19460 | Loss: 0.8299 | Acc: 0.8125 | LR: 9.08e-06
2026-01-07 20:33:26,011 [INFO] Step 9060/19460 | Loss: 0.6925 | Acc: 0.8750 | LR: 9.06e-06
2026-01-07 20:33:29,151 [INFO] Step 9070/19460 | Loss: 0.8279 | Acc: 0.8750 | LR: 9.05e-06
2026-01-07 20:33:32,347 [INFO] Step 9080/19460 | Loss: 0.6352 | Acc: 0.9375 | LR: 9.04e-06
2026-01-07 20:33:35,525 [INFO] Step 9090/19460 | Loss: 0.7820 | Acc: 0.8750 | LR: 9.03e-06
2026-01-07 20:33:39,474 [INFO] Step 9100/19460 | Loss: 0.9211 | Acc: 0.8125 | LR: 9.02e-06
2026-01-07 20:33:48,227 [INFO] [EVAL] Step 9100 | Val Loss: 0.3537 | Val Acc: 0.9050
2026-01-07 20:33:51,534 [INFO] Step 9110/19460 | Loss: 1.0550 | Acc: 0.6875 | LR: 9.01e-06
2026-01-07 20:33:54,888 [INFO] Step 9120/19460 | Loss: 0.9306 | Acc: 0.8125 | LR: 9.00e-06
2026-01-07 20:33:58,243 [INFO] Step 9130/19460 | Loss: 0.6467 | Acc: 0.9375 | LR: 8.98e-06
2026-01-07 20:34:01,472 [INFO] Step 9140/19460 | Loss: 0.8019 | Acc: 0.8125 | LR: 8.97e-06
2026-01-07 20:34:04,713 [INFO] Step 9150/19460 | Loss: 0.5940 | Acc: 0.9375 | LR: 8.96e-06
2026-01-07 20:34:08,054 [INFO] Step 9160/19460 | Loss: 0.5556 | Acc: 0.9375 | LR: 8.95e-06
2026-01-07 20:34:11,223 [INFO] Step 9170/19460 | Loss: 0.8831 | Acc: 0.7500 | LR: 8.94e-06
2026-01-07 20:34:14,410 [INFO] Step 9180/19460 | Loss: 0.5927 | Acc: 0.8750 | LR: 8.93e-06
2026-01-07 20:34:17,709 [INFO] Step 9190/19460 | Loss: 0.7020 | Acc: 0.8750 | LR: 8.91e-06
2026-01-07 20:34:21,387 [INFO] Step 9200/19460 | Loss: 0.4930 | Acc: 1.0000 | LR: 8.90e-06
2026-01-07 20:34:30,121 [INFO] [EVAL] Step 9200 | Val Loss: 0.3455 | Val Acc: 0.9100
2026-01-07 20:34:33,417 [INFO] Step 9210/19460 | Loss: 0.5674 | Acc: 0.8750 | LR: 8.89e-06
2026-01-07 20:34:36,829 [INFO] Step 9220/19460 | Loss: 0.9157 | Acc: 0.7500 | LR: 8.88e-06
2026-01-07 20:34:40,089 [INFO] Step 9230/19460 | Loss: 0.6661 | Acc: 0.8750 | LR: 8.87e-06
2026-01-07 20:34:43,263 [INFO] Step 9240/19460 | Loss: 0.4715 | Acc: 1.0000 | LR: 8.86e-06
2026-01-07 20:34:46,455 [INFO] Step 9250/19460 | Loss: 0.7835 | Acc: 0.8750 | LR: 8.85e-06
2026-01-07 20:34:49,559 [INFO] Step 9260/19460 | Loss: 0.7218 | Acc: 0.8750 | LR: 8.83e-06
2026-01-07 20:34:52,897 [INFO] Step 9270/19460 | Loss: 0.7804 | Acc: 0.8750 | LR: 8.82e-06
2026-01-07 20:34:56,260 [INFO] Step 9280/19460 | Loss: 0.4801 | Acc: 1.0000 | LR: 8.81e-06
2026-01-07 20:34:59,612 [INFO] Step 9290/19460 | Loss: 1.0266 | Acc: 0.7500 | LR: 8.80e-06
2026-01-07 20:35:03,346 [INFO] Step 9300/19460 | Loss: 1.0149 | Acc: 0.7500 | LR: 8.79e-06
2026-01-07 20:35:12,035 [INFO] [EVAL] Step 9300 | Val Loss: 0.3427 | Val Acc: 0.9050
2026-01-07 20:35:12,216 [WARNING] Skipping batch due to non-finite loss at step=9300 (loss=nan, epoch=10).
2026-01-07 20:35:15,563 [INFO] Step 9310/19460 | Loss: 0.5357 | Acc: 0.9375 | LR: 8.78e-06
2026-01-07 20:35:18,822 [INFO] Step 9320/19460 | Loss: 0.8218 | Acc: 0.7500 | LR: 8.77e-06
2026-01-07 20:35:22,009 [INFO] Step 9330/19460 | Loss: 0.6468 | Acc: 0.8750 | LR: 8.75e-06
2026-01-07 20:35:25,163 [INFO] Step 9340/19460 | Loss: 0.8093 | Acc: 0.8750 | LR: 8.74e-06
2026-01-07 20:35:28,301 [INFO] Step 9350/19460 | Loss: 0.8248 | Acc: 0.8125 | LR: 8.73e-06
2026-01-07 20:35:29,104 [WARNING] Skipping batch due to non-finite loss at step=9352 (loss=nan, epoch=10).
2026-01-07 20:35:31,646 [INFO] Step 9360/19460 | Loss: 0.6373 | Acc: 0.9375 | LR: 8.72e-06
2026-01-07 20:35:34,840 [INFO] Step 9370/19460 | Loss: 1.0551 | Acc: 0.7500 | LR: 8.71e-06
2026-01-07 20:35:38,140 [INFO] Step 9380/19460 | Loss: 0.8255 | Acc: 0.7500 | LR: 8.70e-06
2026-01-07 20:35:41,293 [INFO] Step 9390/19460 | Loss: 0.6486 | Acc: 0.8750 | LR: 8.68e-06
2026-01-07 20:35:45,078 [INFO] Step 9400/19460 | Loss: 0.8322 | Acc: 0.8750 | LR: 8.67e-06
2026-01-07 20:35:53,818 [INFO] [EVAL] Step 9400 | Val Loss: 0.3384 | Val Acc: 0.9163
2026-01-07 20:35:53,858 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:35:53,860 [INFO] New best validation accuracy: 0.9163
2026-01-07 20:35:57,113 [INFO] Step 9410/19460 | Loss: 0.9515 | Acc: 0.7500 | LR: 8.66e-06
2026-01-07 20:36:00,235 [INFO] Step 9420/19460 | Loss: 0.6525 | Acc: 0.8125 | LR: 8.65e-06
2026-01-07 20:36:03,441 [INFO] Step 9430/19460 | Loss: 1.1057 | Acc: 0.7500 | LR: 8.64e-06
2026-01-07 20:36:06,718 [INFO] Step 9440/19460 | Loss: 0.8113 | Acc: 0.8750 | LR: 8.63e-06
2026-01-07 20:36:09,965 [INFO] Step 9450/19460 | Loss: 1.2306 | Acc: 0.6250 | LR: 8.62e-06
2026-01-07 20:36:12,034 [WARNING] Skipping batch due to non-finite loss at step=9456 (loss=nan, epoch=10).
2026-01-07 20:36:13,341 [INFO] Step 9460/19460 | Loss: 0.7363 | Acc: 0.8750 | LR: 8.60e-06
2026-01-07 20:36:16,604 [INFO] Step 9470/19460 | Loss: 0.5486 | Acc: 0.9375 | LR: 8.59e-06
2026-01-07 20:36:20,002 [INFO] Step 9480/19460 | Loss: 0.6528 | Acc: 0.8750 | LR: 8.58e-06
2026-01-07 20:36:23,277 [INFO] Step 9490/19460 | Loss: 0.6974 | Acc: 0.8750 | LR: 8.57e-06
2026-01-07 20:36:26,831 [INFO] Step 9500/19460 | Loss: 0.5189 | Acc: 0.9375 | LR: 8.56e-06
2026-01-07 20:36:35,510 [INFO] [EVAL] Step 9500 | Val Loss: 0.3560 | Val Acc: 0.9069
2026-01-07 20:36:35,539 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_9500.npz
2026-01-07 20:36:38,800 [INFO] Step 9510/19460 | Loss: 0.8545 | Acc: 0.8125 | LR: 8.55e-06
2026-01-07 20:36:42,104 [INFO] Step 9520/19460 | Loss: 0.7569 | Acc: 0.7500 | LR: 8.53e-06
2026-01-07 20:36:45,306 [INFO] Step 9530/19460 | Loss: 0.7756 | Acc: 0.8125 | LR: 8.52e-06
2026-01-07 20:36:48,578 [INFO] Step 9540/19460 | Loss: 0.4458 | Acc: 1.0000 | LR: 8.51e-06
2026-01-07 20:36:51,607 [INFO] Step 9550/19460 | Loss: 0.7211 | Acc: 0.8750 | LR: 8.50e-06
2026-01-07 20:36:54,780 [INFO] Step 9560/19460 | Loss: 0.6093 | Acc: 0.9375 | LR: 8.49e-06
2026-01-07 20:36:58,085 [INFO] Step 9570/19460 | Loss: 0.4764 | Acc: 1.0000 | LR: 8.48e-06
2026-01-07 20:37:01,504 [INFO] Step 9580/19460 | Loss: 0.5445 | Acc: 0.9375 | LR: 8.46e-06
2026-01-07 20:37:04,902 [INFO] Step 9590/19460 | Loss: 0.8848 | Acc: 0.7500 | LR: 8.45e-06
2026-01-07 20:37:08,546 [INFO] Step 9600/19460 | Loss: 0.5964 | Acc: 0.8750 | LR: 8.44e-06
2026-01-07 20:37:17,242 [INFO] [EVAL] Step 9600 | Val Loss: 0.3394 | Val Acc: 0.9106
2026-01-07 20:37:20,416 [INFO] Step 9610/19460 | Loss: 0.6544 | Acc: 0.8750 | LR: 8.43e-06
2026-01-07 20:37:23,714 [INFO] Step 9620/19460 | Loss: 0.7859 | Acc: 0.8750 | LR: 8.42e-06
2026-01-07 20:37:27,069 [INFO] Step 9630/19460 | Loss: 0.7627 | Acc: 0.8750 | LR: 8.41e-06
2026-01-07 20:37:30,279 [INFO] Step 9640/19460 | Loss: 0.6871 | Acc: 0.8750 | LR: 8.40e-06
2026-01-07 20:37:33,634 [INFO] Step 9650/19460 | Loss: 0.7259 | Acc: 0.8750 | LR: 8.38e-06
2026-01-07 20:37:36,973 [INFO] Step 9660/19460 | Loss: 0.9118 | Acc: 0.6875 | LR: 8.37e-06
2026-01-07 20:37:38,298 [INFO] Epoch 10 complete | Avg Loss: 0.7535 | Avg Acc: 0.8464 | Updates: 968 | Micro-batches: 971 | Skipped: 3 (loss=3, logits=0, grads=0)
2026-01-07 20:37:38,298 [INFO] Epoch 11/20
2026-01-07 20:37:40,336 [INFO] Step 9670/19460 | Loss: 0.7149 | Acc: 0.8750 | LR: 8.36e-06
2026-01-07 20:37:43,614 [INFO] Step 9680/19460 | Loss: 0.7756 | Acc: 0.8125 | LR: 8.35e-06
2026-01-07 20:37:47,027 [INFO] Step 9690/19460 | Loss: 0.6853 | Acc: 0.8750 | LR: 8.34e-06
2026-01-07 20:37:50,701 [INFO] Step 9700/19460 | Loss: 0.8429 | Acc: 0.6875 | LR: 8.33e-06
2026-01-07 20:37:59,393 [INFO] [EVAL] Step 9700 | Val Loss: 0.3568 | Val Acc: 0.9056
2026-01-07 20:38:02,655 [INFO] Step 9710/19460 | Loss: 0.7179 | Acc: 0.8125 | LR: 8.31e-06
2026-01-07 20:38:05,879 [INFO] Step 9720/19460 | Loss: 0.6863 | Acc: 0.8125 | LR: 8.30e-06
2026-01-07 20:38:09,081 [INFO] Step 9730/19460 | Loss: 0.9651 | Acc: 0.7500 | LR: 8.29e-06
2026-01-07 20:38:12,220 [INFO] Step 9740/19460 | Loss: 0.6323 | Acc: 0.8750 | LR: 8.28e-06
2026-01-07 20:38:15,337 [INFO] Step 9750/19460 | Loss: 0.8353 | Acc: 0.7500 | LR: 8.27e-06
2026-01-07 20:38:18,549 [INFO] Step 9760/19460 | Loss: 0.9448 | Acc: 0.7500 | LR: 8.26e-06
2026-01-07 20:38:21,854 [INFO] Step 9770/19460 | Loss: 0.8265 | Acc: 0.7500 | LR: 8.24e-06
2026-01-07 20:38:25,165 [INFO] Step 9780/19460 | Loss: 0.5732 | Acc: 0.9375 | LR: 8.23e-06
2026-01-07 20:38:28,359 [INFO] Step 9790/19460 | Loss: 0.6556 | Acc: 0.9375 | LR: 8.22e-06
2026-01-07 20:38:32,154 [INFO] Step 9800/19460 | Loss: 0.9331 | Acc: 0.7500 | LR: 8.21e-06
2026-01-07 20:38:40,809 [INFO] [EVAL] Step 9800 | Val Loss: 0.3411 | Val Acc: 0.9062
2026-01-07 20:38:44,040 [INFO] Step 9810/19460 | Loss: 0.5880 | Acc: 0.9375 | LR: 8.20e-06
2026-01-07 20:38:45,185 [WARNING] Skipping batch due to non-finite loss at step=9813 (loss=nan, epoch=11).
2026-01-07 20:38:47,449 [INFO] Step 9820/19460 | Loss: 0.8246 | Acc: 0.8125 | LR: 8.19e-06
2026-01-07 20:38:50,676 [INFO] Step 9830/19460 | Loss: 0.6965 | Acc: 0.8125 | LR: 8.18e-06
2026-01-07 20:38:53,924 [INFO] Step 9840/19460 | Loss: 1.0273 | Acc: 0.7500 | LR: 8.16e-06
2026-01-07 20:38:57,250 [INFO] Step 9850/19460 | Loss: 0.6534 | Acc: 0.9375 | LR: 8.15e-06
2026-01-07 20:39:00,457 [INFO] Step 9860/19460 | Loss: 0.6500 | Acc: 0.8750 | LR: 8.14e-06
2026-01-07 20:39:03,735 [INFO] Step 9870/19460 | Loss: 0.7027 | Acc: 0.8750 | LR: 8.13e-06
2026-01-07 20:39:06,909 [INFO] Step 9880/19460 | Loss: 0.6313 | Acc: 0.8750 | LR: 8.12e-06
2026-01-07 20:39:10,151 [INFO] Step 9890/19460 | Loss: 0.8125 | Acc: 0.8125 | LR: 8.11e-06
2026-01-07 20:39:13,750 [INFO] Step 9900/19460 | Loss: 0.7827 | Acc: 0.8750 | LR: 8.09e-06
2026-01-07 20:39:22,430 [INFO] [EVAL] Step 9900 | Val Loss: 0.3487 | Val Acc: 0.9031
2026-01-07 20:39:25,851 [INFO] Step 9910/19460 | Loss: 0.6993 | Acc: 0.9375 | LR: 8.08e-06
2026-01-07 20:39:29,054 [INFO] Step 9920/19460 | Loss: 0.5478 | Acc: 0.9375 | LR: 8.07e-06
2026-01-07 20:39:32,364 [INFO] Step 9930/19460 | Loss: 1.0830 | Acc: 0.7500 | LR: 8.06e-06
2026-01-07 20:39:35,697 [INFO] Step 9940/19460 | Loss: 0.8259 | Acc: 0.7500 | LR: 8.05e-06
2026-01-07 20:39:38,870 [INFO] Step 9950/19460 | Loss: 1.0473 | Acc: 0.7500 | LR: 8.04e-06
2026-01-07 20:39:42,154 [INFO] Step 9960/19460 | Loss: 0.6321 | Acc: 0.9375 | LR: 8.02e-06
2026-01-07 20:39:45,287 [INFO] Step 9970/19460 | Loss: 1.2059 | Acc: 0.6875 | LR: 8.01e-06
2026-01-07 20:39:48,482 [INFO] Step 9980/19460 | Loss: 0.7382 | Acc: 0.8125 | LR: 8.00e-06
2026-01-07 20:39:51,836 [INFO] Step 9990/19460 | Loss: 0.6900 | Acc: 0.8750 | LR: 7.99e-06
2026-01-07 20:39:55,697 [INFO] Step 10000/19460 | Loss: 0.4605 | Acc: 1.0000 | LR: 7.98e-06
2026-01-07 20:40:04,439 [INFO] [EVAL] Step 10000 | Val Loss: 0.3387 | Val Acc: 0.9119
2026-01-07 20:40:04,475 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_10000.npz
2026-01-07 20:40:07,742 [INFO] Step 10010/19460 | Loss: 0.4716 | Acc: 1.0000 | LR: 7.97e-06
2026-01-07 20:40:10,989 [INFO] Step 10020/19460 | Loss: 0.6638 | Acc: 0.8750 | LR: 7.95e-06
2026-01-07 20:40:14,264 [INFO] Step 10030/19460 | Loss: 0.6949 | Acc: 0.8750 | LR: 7.94e-06
2026-01-07 20:40:17,395 [INFO] Step 10040/19460 | Loss: 0.8579 | Acc: 0.8125 | LR: 7.93e-06
2026-01-07 20:40:20,500 [INFO] Step 10050/19460 | Loss: 0.6120 | Acc: 0.9375 | LR: 7.92e-06
2026-01-07 20:40:23,872 [INFO] Step 10060/19460 | Loss: 0.4910 | Acc: 0.9375 | LR: 7.91e-06
2026-01-07 20:40:27,021 [INFO] Step 10070/19460 | Loss: 0.8158 | Acc: 0.8125 | LR: 7.90e-06
2026-01-07 20:40:30,348 [INFO] Step 10080/19460 | Loss: 0.6073 | Acc: 0.9375 | LR: 7.89e-06
2026-01-07 20:40:33,051 [WARNING] Skipping batch due to non-finite loss at step=10088 (loss=nan, epoch=11).
2026-01-07 20:40:33,708 [INFO] Step 10090/19460 | Loss: 0.5808 | Acc: 0.9375 | LR: 7.87e-06
2026-01-07 20:40:37,447 [INFO] Step 10100/19460 | Loss: 0.5840 | Acc: 0.8750 | LR: 7.86e-06
2026-01-07 20:40:46,150 [INFO] [EVAL] Step 10100 | Val Loss: 0.3303 | Val Acc: 0.9181
2026-01-07 20:40:46,180 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 20:40:46,181 [INFO] New best validation accuracy: 0.9181
2026-01-07 20:40:49,432 [INFO] Step 10110/19460 | Loss: 0.5521 | Acc: 1.0000 | LR: 7.85e-06
2026-01-07 20:40:52,567 [INFO] Step 10120/19460 | Loss: 0.8311 | Acc: 0.8125 | LR: 7.84e-06
2026-01-07 20:40:55,851 [INFO] Step 10130/19460 | Loss: 0.6257 | Acc: 0.8750 | LR: 7.83e-06
2026-01-07 20:40:59,134 [INFO] Step 10140/19460 | Loss: 0.8683 | Acc: 0.8125 | LR: 7.82e-06
2026-01-07 20:41:02,301 [INFO] Step 10150/19460 | Loss: 0.6533 | Acc: 0.9375 | LR: 7.80e-06
2026-01-07 20:41:05,561 [INFO] Step 10160/19460 | Loss: 1.1757 | Acc: 0.6875 | LR: 7.79e-06
2026-01-07 20:41:08,906 [INFO] Step 10170/19460 | Loss: 0.5611 | Acc: 0.9375 | LR: 7.78e-06
2026-01-07 20:41:12,104 [INFO] Step 10180/19460 | Loss: 0.6448 | Acc: 0.8750 | LR: 7.77e-06
2026-01-07 20:41:15,414 [INFO] Step 10190/19460 | Loss: 0.8168 | Acc: 0.7500 | LR: 7.76e-06
2026-01-07 20:41:19,251 [INFO] Step 10200/19460 | Loss: 0.5190 | Acc: 0.9375 | LR: 7.75e-06
2026-01-07 20:41:28,011 [INFO] [EVAL] Step 10200 | Val Loss: 0.3339 | Val Acc: 0.9125
2026-01-07 20:41:31,323 [INFO] Step 10210/19460 | Loss: 0.6071 | Acc: 0.9375 | LR: 7.73e-06
2026-01-07 20:41:34,522 [INFO] Step 10220/19460 | Loss: 0.4562 | Acc: 1.0000 | LR: 7.72e-06
2026-01-07 20:41:37,802 [INFO] Step 10230/19460 | Loss: 1.0051 | Acc: 0.7500 | LR: 7.71e-06
2026-01-07 20:41:41,071 [INFO] Step 10240/19460 | Loss: 1.1171 | Acc: 0.6875 | LR: 7.70e-06
2026-01-07 20:41:44,352 [INFO] Step 10250/19460 | Loss: 0.5784 | Acc: 0.9375 | LR: 7.69e-06
2026-01-07 20:41:47,657 [INFO] Step 10260/19460 | Loss: 0.9554 | Acc: 0.7500 | LR: 7.68e-06
2026-01-07 20:41:51,018 [INFO] Step 10270/19460 | Loss: 0.7842 | Acc: 0.8125 | LR: 7.66e-06
2026-01-07 20:41:54,369 [INFO] Step 10280/19460 | Loss: 0.9839 | Acc: 0.6250 | LR: 7.65e-06
2026-01-07 20:41:57,509 [INFO] Step 10290/19460 | Loss: 0.9927 | Acc: 0.7500 | LR: 7.64e-06
2026-01-07 20:42:01,370 [INFO] Step 10300/19460 | Loss: 0.4835 | Acc: 1.0000 | LR: 7.63e-06
2026-01-07 20:42:10,090 [INFO] [EVAL] Step 10300 | Val Loss: 0.3519 | Val Acc: 0.9106
2026-01-07 20:42:13,462 [INFO] Step 10310/19460 | Loss: 0.8587 | Acc: 0.7500 | LR: 7.62e-06
2026-01-07 20:42:16,718 [INFO] Step 10320/19460 | Loss: 0.8448 | Acc: 0.8125 | LR: 7.61e-06
2026-01-07 20:42:20,034 [INFO] Step 10330/19460 | Loss: 0.5669 | Acc: 0.9375 | LR: 7.60e-06
2026-01-07 20:42:23,467 [INFO] Step 10340/19460 | Loss: 0.7136 | Acc: 0.8125 | LR: 7.58e-06
2026-01-07 20:42:26,707 [INFO] Step 10350/19460 | Loss: 0.5607 | Acc: 0.9375 | LR: 7.57e-06
2026-01-07 20:42:29,831 [INFO] Step 10360/19460 | Loss: 0.6489 | Acc: 0.8750 | LR: 7.56e-06
2026-01-07 20:42:33,106 [INFO] Step 10370/19460 | Loss: 0.6601 | Acc: 0.8750 | LR: 7.55e-06
2026-01-07 20:42:36,337 [INFO] Step 10380/19460 | Loss: 0.6161 | Acc: 0.8750 | LR: 7.54e-06
2026-01-07 20:42:39,617 [INFO] Step 10390/19460 | Loss: 0.7337 | Acc: 0.8750 | LR: 7.53e-06
2026-01-07 20:42:43,451 [INFO] Step 10400/19460 | Loss: 0.5661 | Acc: 0.9375 | LR: 7.51e-06
2026-01-07 20:42:52,180 [INFO] [EVAL] Step 10400 | Val Loss: 0.3377 | Val Acc: 0.9100
2026-01-07 20:42:55,417 [INFO] Step 10410/19460 | Loss: 0.6706 | Acc: 0.8750 | LR: 7.50e-06
2026-01-07 20:42:58,641 [INFO] Step 10420/19460 | Loss: 0.6557 | Acc: 0.8750 | LR: 7.49e-06
2026-01-07 20:43:01,739 [INFO] Step 10430/19460 | Loss: 0.7236 | Acc: 0.8750 | LR: 7.48e-06
2026-01-07 20:43:05,042 [INFO] Step 10440/19460 | Loss: 0.6697 | Acc: 0.8750 | LR: 7.47e-06
2026-01-07 20:43:08,315 [INFO] Step 10450/19460 | Loss: 0.5993 | Acc: 0.9375 | LR: 7.46e-06
2026-01-07 20:43:11,498 [INFO] Step 10460/19460 | Loss: 0.5965 | Acc: 0.9375 | LR: 7.45e-06
2026-01-07 20:43:14,783 [INFO] Step 10470/19460 | Loss: 1.0848 | Acc: 0.6875 | LR: 7.43e-06
2026-01-07 20:43:18,121 [INFO] Step 10480/19460 | Loss: 0.6184 | Acc: 0.9375 | LR: 7.42e-06
2026-01-07 20:43:21,304 [INFO] Step 10490/19460 | Loss: 0.8124 | Acc: 0.8125 | LR: 7.41e-06
2026-01-07 20:43:25,157 [INFO] Step 10500/19460 | Loss: 0.5860 | Acc: 0.8750 | LR: 7.40e-06
2026-01-07 20:43:33,904 [INFO] [EVAL] Step 10500 | Val Loss: 0.3547 | Val Acc: 0.9087
2026-01-07 20:43:33,929 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_10500.npz
2026-01-07 20:43:37,151 [INFO] Step 10510/19460 | Loss: 0.8705 | Acc: 0.8125 | LR: 7.39e-06
2026-01-07 20:43:40,465 [INFO] Step 10520/19460 | Loss: 0.8443 | Acc: 0.7500 | LR: 7.38e-06
2026-01-07 20:43:43,816 [INFO] Step 10530/19460 | Loss: 0.5126 | Acc: 0.9375 | LR: 7.36e-06
2026-01-07 20:43:47,101 [INFO] Step 10540/19460 | Loss: 0.7291 | Acc: 0.7500 | LR: 7.35e-06
2026-01-07 20:43:50,386 [INFO] Step 10550/19460 | Loss: 0.6498 | Acc: 0.8750 | LR: 7.34e-06
2026-01-07 20:43:53,677 [INFO] Step 10560/19460 | Loss: 0.6100 | Acc: 0.9375 | LR: 7.33e-06
2026-01-07 20:43:56,800 [INFO] Step 10570/19460 | Loss: 0.6920 | Acc: 0.9375 | LR: 7.32e-06
2026-01-07 20:44:00,081 [INFO] Step 10580/19460 | Loss: 0.6132 | Acc: 1.0000 | LR: 7.31e-06
2026-01-07 20:44:03,284 [INFO] Step 10590/19460 | Loss: 1.1583 | Acc: 0.6250 | LR: 7.29e-06
2026-01-07 20:44:07,245 [INFO] Step 10600/19460 | Loss: 0.5575 | Acc: 0.9375 | LR: 7.28e-06
2026-01-07 20:44:15,961 [INFO] [EVAL] Step 10600 | Val Loss: 0.3379 | Val Acc: 0.9106
2026-01-07 20:44:19,298 [INFO] Step 10610/19460 | Loss: 0.7698 | Acc: 0.8125 | LR: 7.27e-06
2026-01-07 20:44:22,557 [INFO] Step 10620/19460 | Loss: 0.8709 | Acc: 0.7500 | LR: 7.26e-06
2026-01-07 20:44:25,740 [INFO] Step 10630/19460 | Loss: 0.4588 | Acc: 1.0000 | LR: 7.25e-06
2026-01-07 20:44:26,761 [INFO] Epoch 11 complete | Avg Loss: 0.7440 | Avg Acc: 0.8481 | Updates: 969 | Micro-batches: 971 | Skipped: 2 (loss=2, logits=0, grads=0)
2026-01-07 20:44:26,761 [INFO] Epoch 12/20
2026-01-07 20:44:29,140 [INFO] Step 10640/19460 | Loss: 0.7816 | Acc: 0.8750 | LR: 7.24e-06
2026-01-07 20:44:32,466 [INFO] Step 10650/19460 | Loss: 0.9724 | Acc: 0.7500 | LR: 7.23e-06
2026-01-07 20:44:35,652 [INFO] Step 10660/19460 | Loss: 0.8971 | Acc: 0.7500 | LR: 7.21e-06
2026-01-07 20:44:39,092 [INFO] Step 10670/19460 | Loss: 0.6246 | Acc: 0.8750 | LR: 7.20e-06
2026-01-07 20:44:42,370 [INFO] Step 10680/19460 | Loss: 1.1563 | Acc: 0.6250 | LR: 7.19e-06
2026-01-07 20:44:45,684 [INFO] Step 10690/19460 | Loss: 0.8730 | Acc: 0.7500 | LR: 7.18e-06
2026-01-07 20:44:49,383 [INFO] Step 10700/19460 | Loss: 0.6663 | Acc: 0.8750 | LR: 7.17e-06
2026-01-07 20:44:58,124 [INFO] [EVAL] Step 10700 | Val Loss: 0.3452 | Val Acc: 0.9137
2026-01-07 20:45:01,380 [INFO] Step 10710/19460 | Loss: 1.1405 | Acc: 0.6875 | LR: 7.16e-06
2026-01-07 20:45:04,633 [INFO] Step 10720/19460 | Loss: 0.6380 | Acc: 0.9375 | LR: 7.14e-06
2026-01-07 20:45:07,987 [INFO] Step 10730/19460 | Loss: 0.6841 | Acc: 0.8750 | LR: 7.13e-06
2026-01-07 20:45:11,079 [INFO] Step 10740/19460 | Loss: 0.9785 | Acc: 0.6250 | LR: 7.12e-06
2026-01-07 20:45:14,179 [INFO] Step 10750/19460 | Loss: 0.6015 | Acc: 0.9375 | LR: 7.11e-06
2026-01-07 20:45:17,340 [INFO] Step 10760/19460 | Loss: 0.6656 | Acc: 0.8750 | LR: 7.10e-06
2026-01-07 20:45:20,717 [INFO] Step 10770/19460 | Loss: 1.0469 | Acc: 0.8125 | LR: 7.09e-06
2026-01-07 20:45:24,009 [INFO] Step 10780/19460 | Loss: 0.9583 | Acc: 0.7500 | LR: 7.08e-06
2026-01-07 20:45:27,270 [INFO] Step 10790/19460 | Loss: 0.5220 | Acc: 0.9375 | LR: 7.06e-06
2026-01-07 20:45:30,977 [INFO] Step 10800/19460 | Loss: 0.8313 | Acc: 0.7500 | LR: 7.05e-06
2026-01-07 20:45:39,731 [INFO] [EVAL] Step 10800 | Val Loss: 0.3377 | Val Acc: 0.9131
2026-01-07 20:45:42,925 [INFO] Step 10810/19460 | Loss: 0.6892 | Acc: 0.8125 | LR: 7.04e-06
2026-01-07 20:45:46,165 [INFO] Step 10820/19460 | Loss: 0.5790 | Acc: 0.9375 | LR: 7.03e-06
2026-01-07 20:45:49,538 [INFO] Step 10830/19460 | Loss: 0.7985 | Acc: 0.8750 | LR: 7.02e-06
2026-01-07 20:45:52,720 [INFO] Step 10840/19460 | Loss: 0.5817 | Acc: 0.9375 | LR: 7.01e-06
2026-01-07 20:45:55,936 [INFO] Step 10850/19460 | Loss: 0.4990 | Acc: 1.0000 | LR: 7.00e-06
2026-01-07 20:45:59,164 [INFO] Step 10860/19460 | Loss: 0.9889 | Acc: 0.8125 | LR: 6.98e-06
2026-01-07 20:46:02,381 [INFO] Step 10870/19460 | Loss: 0.4823 | Acc: 1.0000 | LR: 6.97e-06
2026-01-07 20:46:05,540 [INFO] Step 10880/19460 | Loss: 1.0257 | Acc: 0.6875 | LR: 6.96e-06
2026-01-07 20:46:08,736 [INFO] Step 10890/19460 | Loss: 0.7938 | Acc: 0.8125 | LR: 6.95e-06
2026-01-07 20:46:12,544 [INFO] Step 10900/19460 | Loss: 0.7524 | Acc: 0.8750 | LR: 6.94e-06
2026-01-07 20:46:21,228 [INFO] [EVAL] Step 10900 | Val Loss: 0.3335 | Val Acc: 0.9113
2026-01-07 20:46:24,566 [INFO] Step 10910/19460 | Loss: 0.5736 | Acc: 0.9375 | LR: 6.93e-06
2026-01-07 20:46:27,762 [INFO] Step 10920/19460 | Loss: 0.5962 | Acc: 0.8750 | LR: 6.92e-06
2026-01-07 20:46:30,992 [INFO] Step 10930/19460 | Loss: 0.6703 | Acc: 0.8750 | LR: 6.90e-06
2026-01-07 20:46:34,463 [INFO] Step 10940/19460 | Loss: 0.8600 | Acc: 0.8125 | LR: 6.89e-06
2026-01-07 20:46:37,809 [INFO] Step 10950/19460 | Loss: 1.0564 | Acc: 0.6875 | LR: 6.88e-06
2026-01-07 20:46:41,102 [INFO] Step 10960/19460 | Loss: 0.7204 | Acc: 0.8125 | LR: 6.87e-06
2026-01-07 20:46:44,479 [INFO] Step 10970/19460 | Loss: 0.6019 | Acc: 0.9375 | LR: 6.86e-06
2026-01-07 20:46:47,803 [INFO] Step 10980/19460 | Loss: 0.6171 | Acc: 0.8750 | LR: 6.85e-06
2026-01-07 20:46:51,200 [INFO] Step 10990/19460 | Loss: 0.6986 | Acc: 0.9375 | LR: 6.84e-06
2026-01-07 20:46:54,989 [INFO] Step 11000/19460 | Loss: 0.6164 | Acc: 0.8750 | LR: 6.82e-06
2026-01-07 20:47:03,709 [INFO] [EVAL] Step 11000 | Val Loss: 0.3380 | Val Acc: 0.9094
2026-01-07 20:47:03,744 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_11000.npz
2026-01-07 20:47:06,942 [INFO] Step 11010/19460 | Loss: 1.0721 | Acc: 0.6875 | LR: 6.81e-06
2026-01-07 20:47:10,245 [INFO] Step 11020/19460 | Loss: 0.6282 | Acc: 0.9375 | LR: 6.80e-06
2026-01-07 20:47:13,400 [INFO] Step 11030/19460 | Loss: 0.6210 | Acc: 0.8750 | LR: 6.79e-06
2026-01-07 20:47:14,549 [WARNING] Skipping batch due to non-finite loss at step=11033 (loss=nan, epoch=12).
2026-01-07 20:47:16,790 [INFO] Step 11040/19460 | Loss: 0.7064 | Acc: 0.8750 | LR: 6.78e-06
2026-01-07 20:47:20,160 [INFO] Step 11050/19460 | Loss: 0.8039 | Acc: 0.8750 | LR: 6.77e-06
2026-01-07 20:47:23,332 [INFO] Step 11060/19460 | Loss: 0.9471 | Acc: 0.7500 | LR: 6.76e-06
2026-01-07 20:47:26,686 [INFO] Step 11070/19460 | Loss: 0.9384 | Acc: 0.7500 | LR: 6.74e-06
2026-01-07 20:47:29,853 [INFO] Step 11080/19460 | Loss: 0.7248 | Acc: 0.8125 | LR: 6.73e-06
2026-01-07 20:47:33,090 [INFO] Step 11090/19460 | Loss: 0.5847 | Acc: 0.9375 | LR: 6.72e-06
2026-01-07 20:47:37,017 [INFO] Step 11100/19460 | Loss: 0.7991 | Acc: 0.8125 | LR: 6.71e-06
2026-01-07 20:47:45,804 [INFO] [EVAL] Step 11100 | Val Loss: 0.3461 | Val Acc: 0.9081
2026-01-07 20:47:49,208 [INFO] Step 11110/19460 | Loss: 0.5012 | Acc: 1.0000 | LR: 6.70e-06
2026-01-07 20:47:52,473 [INFO] Step 11120/19460 | Loss: 0.7172 | Acc: 0.8750 | LR: 6.69e-06
2026-01-07 20:47:55,662 [INFO] Step 11130/19460 | Loss: 0.6724 | Acc: 0.9375 | LR: 6.68e-06
2026-01-07 20:47:58,741 [INFO] Step 11140/19460 | Loss: 1.1651 | Acc: 0.6875 | LR: 6.66e-06
2026-01-07 20:48:01,883 [INFO] Step 11150/19460 | Loss: 0.8394 | Acc: 0.8125 | LR: 6.65e-06
2026-01-07 20:48:05,047 [INFO] Step 11160/19460 | Loss: 0.7827 | Acc: 0.8125 | LR: 6.64e-06
2026-01-07 20:48:08,410 [INFO] Step 11170/19460 | Loss: 0.7066 | Acc: 0.8750 | LR: 6.63e-06
2026-01-07 20:48:11,693 [INFO] Step 11180/19460 | Loss: 1.0738 | Acc: 0.7500 | LR: 6.62e-06
2026-01-07 20:48:14,946 [INFO] Step 11190/19460 | Loss: 0.6300 | Acc: 0.8750 | LR: 6.61e-06
2026-01-07 20:48:18,815 [INFO] Step 11200/19460 | Loss: 0.4734 | Acc: 1.0000 | LR: 6.60e-06
2026-01-07 20:48:27,606 [INFO] [EVAL] Step 11200 | Val Loss: 0.3371 | Val Acc: 0.9075
2026-01-07 20:48:30,950 [INFO] Step 11210/19460 | Loss: 0.8830 | Acc: 0.7500 | LR: 6.58e-06
2026-01-07 20:48:34,137 [INFO] Step 11220/19460 | Loss: 0.5647 | Acc: 0.9375 | LR: 6.57e-06
2026-01-07 20:48:37,346 [INFO] Step 11230/19460 | Loss: 0.7398 | Acc: 0.8750 | LR: 6.56e-06
2026-01-07 20:48:40,470 [INFO] Step 11240/19460 | Loss: 0.8221 | Acc: 0.8125 | LR: 6.55e-06
2026-01-07 20:48:43,673 [INFO] Step 11250/19460 | Loss: 0.6608 | Acc: 0.9375 | LR: 6.54e-06
2026-01-07 20:48:47,058 [INFO] Step 11260/19460 | Loss: 0.6600 | Acc: 0.8750 | LR: 6.53e-06
2026-01-07 20:48:50,249 [INFO] Step 11270/19460 | Loss: 0.8250 | Acc: 0.7500 | LR: 6.52e-06
2026-01-07 20:48:53,615 [INFO] Step 11280/19460 | Loss: 0.7090 | Acc: 0.8750 | LR: 6.50e-06
2026-01-07 20:48:57,026 [INFO] Step 11290/19460 | Loss: 0.6462 | Acc: 0.8750 | LR: 6.49e-06
2026-01-07 20:49:00,796 [INFO] Step 11300/19460 | Loss: 0.7411 | Acc: 0.8750 | LR: 6.48e-06
2026-01-07 20:49:09,500 [INFO] [EVAL] Step 11300 | Val Loss: 0.3434 | Val Acc: 0.9081
2026-01-07 20:49:12,742 [INFO] Step 11310/19460 | Loss: 0.8467 | Acc: 0.8125 | LR: 6.47e-06
2026-01-07 20:49:16,036 [INFO] Step 11320/19460 | Loss: 0.6393 | Acc: 0.8750 | LR: 6.46e-06
2026-01-07 20:49:19,342 [INFO] Step 11330/19460 | Loss: 0.5588 | Acc: 0.9375 | LR: 6.45e-06
2026-01-07 20:49:22,667 [INFO] Step 11340/19460 | Loss: 0.8826 | Acc: 0.7500 | LR: 6.44e-06
2026-01-07 20:49:25,899 [INFO] Step 11350/19460 | Loss: 0.6883 | Acc: 0.8125 | LR: 6.43e-06
2026-01-07 20:49:29,130 [INFO] Step 11360/19460 | Loss: 0.6489 | Acc: 0.9375 | LR: 6.41e-06
2026-01-07 20:49:32,455 [INFO] Step 11370/19460 | Loss: 0.6362 | Acc: 0.8750 | LR: 6.40e-06
2026-01-07 20:49:35,618 [INFO] Step 11380/19460 | Loss: 1.0241 | Acc: 0.7500 | LR: 6.39e-06
2026-01-07 20:49:38,921 [INFO] Step 11390/19460 | Loss: 1.2407 | Acc: 0.6875 | LR: 6.38e-06
2026-01-07 20:49:42,760 [INFO] Step 11400/19460 | Loss: 0.5253 | Acc: 1.0000 | LR: 6.37e-06
2026-01-07 20:49:51,464 [INFO] [EVAL] Step 11400 | Val Loss: 0.3570 | Val Acc: 0.9031
2026-01-07 20:49:54,869 [INFO] Step 11410/19460 | Loss: 0.7020 | Acc: 0.8750 | LR: 6.36e-06
2026-01-07 20:49:58,067 [INFO] Step 11420/19460 | Loss: 0.7158 | Acc: 0.8125 | LR: 6.35e-06
2026-01-07 20:50:01,417 [INFO] Step 11430/19460 | Loss: 0.7229 | Acc: 0.8125 | LR: 6.34e-06
2026-01-07 20:50:04,807 [INFO] Step 11440/19460 | Loss: 1.2418 | Acc: 0.6250 | LR: 6.32e-06
2026-01-07 20:50:08,043 [INFO] Step 11450/19460 | Loss: 0.6668 | Acc: 0.9375 | LR: 6.31e-06
2026-01-07 20:50:11,292 [INFO] Step 11460/19460 | Loss: 0.6893 | Acc: 0.8750 | LR: 6.30e-06
2026-01-07 20:50:14,548 [INFO] Step 11470/19460 | Loss: 0.6266 | Acc: 0.8125 | LR: 6.29e-06
2026-01-07 20:50:17,837 [INFO] Step 11480/19460 | Loss: 0.6072 | Acc: 0.8750 | LR: 6.28e-06
2026-01-07 20:50:21,171 [INFO] Step 11490/19460 | Loss: 0.7374 | Acc: 0.8125 | LR: 6.27e-06
2026-01-07 20:50:24,957 [INFO] Step 11500/19460 | Loss: 0.6209 | Acc: 0.9375 | LR: 6.26e-06
2026-01-07 20:50:33,668 [INFO] [EVAL] Step 11500 | Val Loss: 0.3349 | Val Acc: 0.9094
2026-01-07 20:50:33,699 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_11500.npz
2026-01-07 20:50:37,023 [INFO] Step 11510/19460 | Loss: 0.5980 | Acc: 0.9375 | LR: 6.25e-06
2026-01-07 20:50:40,241 [INFO] Step 11520/19460 | Loss: 0.5946 | Acc: 0.9375 | LR: 6.23e-06
2026-01-07 20:50:43,474 [INFO] Step 11530/19460 | Loss: 0.5888 | Acc: 0.9375 | LR: 6.22e-06
2026-01-07 20:50:46,760 [INFO] Step 11540/19460 | Loss: 0.4500 | Acc: 1.0000 | LR: 6.21e-06
2026-01-07 20:50:50,026 [INFO] Step 11550/19460 | Loss: 0.7460 | Acc: 0.8125 | LR: 6.20e-06
2026-01-07 20:50:53,104 [INFO] Step 11560/19460 | Loss: 0.9167 | Acc: 0.7500 | LR: 6.19e-06
2026-01-07 20:50:56,525 [INFO] Step 11570/19460 | Loss: 0.5247 | Acc: 0.9375 | LR: 6.18e-06
2026-01-07 20:50:59,855 [INFO] Step 11580/19460 | Loss: 0.8591 | Acc: 0.7500 | LR: 6.17e-06
2026-01-07 20:51:03,237 [INFO] Step 11590/19460 | Loss: 0.8797 | Acc: 0.7500 | LR: 6.16e-06
2026-01-07 20:51:07,024 [INFO] Step 11600/19460 | Loss: 0.7496 | Acc: 0.8125 | LR: 6.14e-06
2026-01-07 20:51:15,854 [INFO] [EVAL] Step 11600 | Val Loss: 0.3391 | Val Acc: 0.9119
2026-01-07 20:51:16,827 [INFO] Epoch 12 complete | Avg Loss: 0.7374 | Avg Acc: 0.8515 | Updates: 970 | Micro-batches: 971 | Skipped: 1 (loss=1, logits=0, grads=0)
2026-01-07 20:51:16,827 [INFO] Epoch 13/20
2026-01-07 20:51:19,110 [INFO] Step 11610/19460 | Loss: 0.8546 | Acc: 0.8125 | LR: 6.13e-06
2026-01-07 20:51:22,353 [INFO] Step 11620/19460 | Loss: 0.9025 | Acc: 0.7500 | LR: 6.12e-06
2026-01-07 20:51:25,640 [INFO] Step 11630/19460 | Loss: 0.6388 | Acc: 0.9375 | LR: 6.11e-06
2026-01-07 20:51:29,011 [INFO] Step 11640/19460 | Loss: 0.7109 | Acc: 0.8750 | LR: 6.10e-06
2026-01-07 20:51:32,270 [INFO] Step 11650/19460 | Loss: 0.8924 | Acc: 0.8125 | LR: 6.09e-06
2026-01-07 20:51:35,489 [INFO] Step 11660/19460 | Loss: 0.7315 | Acc: 0.8125 | LR: 6.08e-06
2026-01-07 20:51:38,691 [INFO] Step 11670/19460 | Loss: 0.7844 | Acc: 0.8750 | LR: 6.07e-06
2026-01-07 20:51:41,886 [INFO] Step 11680/19460 | Loss: 0.7721 | Acc: 0.7500 | LR: 6.06e-06
2026-01-07 20:51:45,204 [INFO] Step 11690/19460 | Loss: 0.7864 | Acc: 0.8750 | LR: 6.04e-06
2026-01-07 20:51:49,041 [INFO] Step 11700/19460 | Loss: 0.5373 | Acc: 0.9375 | LR: 6.03e-06
2026-01-07 20:51:57,770 [INFO] [EVAL] Step 11700 | Val Loss: 0.3321 | Val Acc: 0.9113
2026-01-07 20:52:01,123 [INFO] Step 11710/19460 | Loss: 0.8136 | Acc: 0.8125 | LR: 6.02e-06
2026-01-07 20:52:04,551 [INFO] Step 11720/19460 | Loss: 0.5007 | Acc: 1.0000 | LR: 6.01e-06
2026-01-07 20:52:07,828 [INFO] Step 11730/19460 | Loss: 0.5447 | Acc: 1.0000 | LR: 6.00e-06
2026-01-07 20:52:11,032 [INFO] Step 11740/19460 | Loss: 1.0442 | Acc: 0.6875 | LR: 5.99e-06
2026-01-07 20:52:14,224 [INFO] Step 11750/19460 | Loss: 0.7945 | Acc: 0.7500 | LR: 5.98e-06
2026-01-07 20:52:17,523 [INFO] Step 11760/19460 | Loss: 0.5142 | Acc: 1.0000 | LR: 5.97e-06
2026-01-07 20:52:20,892 [INFO] Step 11770/19460 | Loss: 0.6302 | Acc: 0.8750 | LR: 5.96e-06
2026-01-07 20:52:24,194 [INFO] Step 11780/19460 | Loss: 0.7771 | Acc: 0.8125 | LR: 5.94e-06
2026-01-07 20:52:27,494 [INFO] Step 11790/19460 | Loss: 0.8254 | Acc: 0.7500 | LR: 5.93e-06
2026-01-07 20:52:31,370 [INFO] Step 11800/19460 | Loss: 0.4746 | Acc: 1.0000 | LR: 5.92e-06
2026-01-07 20:52:40,120 [INFO] [EVAL] Step 11800 | Val Loss: 0.3495 | Val Acc: 0.9031
2026-01-07 20:52:43,429 [INFO] Step 11810/19460 | Loss: 0.6180 | Acc: 0.9375 | LR: 5.91e-06
2026-01-07 20:52:46,645 [INFO] Step 11820/19460 | Loss: 0.8199 | Acc: 0.8125 | LR: 5.90e-06
2026-01-07 20:52:50,049 [INFO] Step 11830/19460 | Loss: 0.7395 | Acc: 0.8750 | LR: 5.89e-06
2026-01-07 20:52:53,338 [INFO] Step 11840/19460 | Loss: 0.5854 | Acc: 0.9375 | LR: 5.88e-06
2026-01-07 20:52:56,651 [INFO] Step 11850/19460 | Loss: 1.1473 | Acc: 0.7500 | LR: 5.87e-06
2026-01-07 20:52:59,998 [INFO] Step 11860/19460 | Loss: 0.7765 | Acc: 0.8750 | LR: 5.86e-06
2026-01-07 20:53:03,254 [INFO] Step 11870/19460 | Loss: 0.5337 | Acc: 0.8750 | LR: 5.84e-06
2026-01-07 20:53:06,435 [INFO] Step 11880/19460 | Loss: 0.9441 | Acc: 0.7500 | LR: 5.83e-06
2026-01-07 20:53:09,703 [INFO] Step 11890/19460 | Loss: 0.6043 | Acc: 0.9375 | LR: 5.82e-06
2026-01-07 20:53:13,434 [INFO] Step 11900/19460 | Loss: 0.5799 | Acc: 0.8750 | LR: 5.81e-06
2026-01-07 20:53:22,145 [INFO] [EVAL] Step 11900 | Val Loss: 0.3329 | Val Acc: 0.9100
2026-01-07 20:53:25,348 [INFO] Step 11910/19460 | Loss: 0.7146 | Acc: 0.8750 | LR: 5.80e-06
2026-01-07 20:53:28,428 [WARNING] Skipping batch due to non-finite loss at step=11919 (loss=nan, epoch=13).
2026-01-07 20:53:28,711 [INFO] Step 11920/19460 | Loss: 0.5614 | Acc: 0.9375 | LR: 5.79e-06
2026-01-07 20:53:31,892 [INFO] Step 11930/19460 | Loss: 0.7090 | Acc: 0.8125 | LR: 5.78e-06
2026-01-07 20:53:35,079 [INFO] Step 11940/19460 | Loss: 0.5923 | Acc: 0.9375 | LR: 5.77e-06
2026-01-07 20:53:38,354 [INFO] Step 11950/19460 | Loss: 0.7377 | Acc: 0.8125 | LR: 5.76e-06
2026-01-07 20:53:41,631 [INFO] Step 11960/19460 | Loss: 0.5621 | Acc: 0.9375 | LR: 5.75e-06
2026-01-07 20:53:44,922 [INFO] Step 11970/19460 | Loss: 0.5655 | Acc: 0.8750 | LR: 5.73e-06
2026-01-07 20:53:48,187 [INFO] Step 11980/19460 | Loss: 0.6761 | Acc: 0.8125 | LR: 5.72e-06
2026-01-07 20:53:51,579 [INFO] Step 11990/19460 | Loss: 0.7524 | Acc: 0.8125 | LR: 5.71e-06
2026-01-07 20:53:55,394 [INFO] Step 12000/19460 | Loss: 1.0157 | Acc: 0.6875 | LR: 5.70e-06
2026-01-07 20:54:04,074 [INFO] [EVAL] Step 12000 | Val Loss: 0.3255 | Val Acc: 0.9137
2026-01-07 20:54:04,103 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_12000.npz
2026-01-07 20:54:07,366 [INFO] Step 12010/19460 | Loss: 0.7426 | Acc: 0.8750 | LR: 5.69e-06
2026-01-07 20:54:10,510 [WARNING] Skipping batch due to non-finite loss at step=12019 (loss=nan, epoch=13).
2026-01-07 20:54:10,899 [INFO] Step 12020/19460 | Loss: 0.5958 | Acc: 0.8750 | LR: 5.68e-06
2026-01-07 20:54:14,198 [INFO] Step 12030/19460 | Loss: 0.6873 | Acc: 0.8125 | LR: 5.67e-06
2026-01-07 20:54:17,343 [INFO] Step 12040/19460 | Loss: 0.7322 | Acc: 0.8750 | LR: 5.66e-06
2026-01-07 20:54:20,614 [INFO] Step 12050/19460 | Loss: 1.1929 | Acc: 0.5625 | LR: 5.65e-06
2026-01-07 20:54:23,964 [INFO] Step 12060/19460 | Loss: 0.5191 | Acc: 0.9375 | LR: 5.64e-06
2026-01-07 20:54:27,181 [INFO] Step 12070/19460 | Loss: 0.5901 | Acc: 0.9375 | LR: 5.63e-06
2026-01-07 20:54:30,459 [INFO] Step 12080/19460 | Loss: 0.6394 | Acc: 0.9375 | LR: 5.61e-06
2026-01-07 20:54:33,799 [INFO] Step 12090/19460 | Loss: 0.9391 | Acc: 0.8125 | LR: 5.60e-06
2026-01-07 20:54:37,709 [INFO] Step 12100/19460 | Loss: 0.6107 | Acc: 0.9375 | LR: 5.59e-06
2026-01-07 20:54:46,454 [INFO] [EVAL] Step 12100 | Val Loss: 0.3263 | Val Acc: 0.9150
2026-01-07 20:54:49,589 [INFO] Step 12110/19460 | Loss: 0.7451 | Acc: 0.8125 | LR: 5.58e-06
2026-01-07 20:54:52,854 [INFO] Step 12120/19460 | Loss: 0.8660 | Acc: 0.7500 | LR: 5.57e-06
2026-01-07 20:54:56,102 [INFO] Step 12130/19460 | Loss: 0.6566 | Acc: 0.9375 | LR: 5.56e-06
2026-01-07 20:54:59,371 [INFO] Step 12140/19460 | Loss: 0.7313 | Acc: 0.8125 | LR: 5.55e-06
2026-01-07 20:55:02,538 [INFO] Step 12150/19460 | Loss: 0.8039 | Acc: 0.8750 | LR: 5.54e-06
2026-01-07 20:55:05,793 [INFO] Step 12160/19460 | Loss: 0.7553 | Acc: 0.8125 | LR: 5.53e-06
2026-01-07 20:55:09,190 [INFO] Step 12170/19460 | Loss: 1.1410 | Acc: 0.6250 | LR: 5.52e-06
2026-01-07 20:55:12,406 [INFO] Step 12180/19460 | Loss: 0.5500 | Acc: 0.9375 | LR: 5.51e-06
2026-01-07 20:55:15,573 [INFO] Step 12190/19460 | Loss: 0.6526 | Acc: 0.9375 | LR: 5.49e-06
2026-01-07 20:55:19,409 [INFO] Step 12200/19460 | Loss: 0.5722 | Acc: 0.9375 | LR: 5.48e-06
2026-01-07 20:55:28,200 [INFO] [EVAL] Step 12200 | Val Loss: 0.3261 | Val Acc: 0.9150
2026-01-07 20:55:31,545 [INFO] Step 12210/19460 | Loss: 0.9648 | Acc: 0.7500 | LR: 5.47e-06
2026-01-07 20:55:34,872 [INFO] Step 12220/19460 | Loss: 0.4439 | Acc: 1.0000 | LR: 5.46e-06
2026-01-07 20:55:38,056 [INFO] Step 12230/19460 | Loss: 0.8406 | Acc: 0.6250 | LR: 5.45e-06
2026-01-07 20:55:41,355 [INFO] Step 12240/19460 | Loss: 0.6289 | Acc: 0.8750 | LR: 5.44e-06
2026-01-07 20:55:44,767 [INFO] Step 12250/19460 | Loss: 0.6336 | Acc: 0.9375 | LR: 5.43e-06
2026-01-07 20:55:48,043 [INFO] Step 12260/19460 | Loss: 0.5858 | Acc: 0.9375 | LR: 5.42e-06
2026-01-07 20:55:51,278 [INFO] Step 12270/19460 | Loss: 0.4415 | Acc: 1.0000 | LR: 5.41e-06
2026-01-07 20:55:54,556 [INFO] Step 12280/19460 | Loss: 0.7974 | Acc: 0.8125 | LR: 5.40e-06
2026-01-07 20:55:57,737 [INFO] Step 12290/19460 | Loss: 0.4938 | Acc: 1.0000 | LR: 5.39e-06
2026-01-07 20:56:01,529 [INFO] Step 12300/19460 | Loss: 0.6640 | Acc: 0.9375 | LR: 5.38e-06
2026-01-07 20:56:10,219 [INFO] [EVAL] Step 12300 | Val Loss: 0.3299 | Val Acc: 0.9119
2026-01-07 20:56:13,408 [INFO] Step 12310/19460 | Loss: 0.8447 | Acc: 0.7500 | LR: 5.37e-06
2026-01-07 20:56:16,679 [INFO] Step 12320/19460 | Loss: 0.8447 | Acc: 0.7500 | LR: 5.35e-06
2026-01-07 20:56:19,934 [INFO] Step 12330/19460 | Loss: 0.5025 | Acc: 1.0000 | LR: 5.34e-06
2026-01-07 20:56:23,119 [INFO] Step 12340/19460 | Loss: 0.9263 | Acc: 0.8750 | LR: 5.33e-06
2026-01-07 20:56:26,569 [INFO] Step 12350/19460 | Loss: 0.7173 | Acc: 0.8125 | LR: 5.32e-06
2026-01-07 20:56:29,827 [INFO] Step 12360/19460 | Loss: 0.7981 | Acc: 0.8750 | LR: 5.31e-06
2026-01-07 20:56:33,041 [INFO] Step 12370/19460 | Loss: 1.1142 | Acc: 0.6250 | LR: 5.30e-06
2026-01-07 20:56:36,431 [INFO] Step 12380/19460 | Loss: 0.8550 | Acc: 0.8125 | LR: 5.29e-06
2026-01-07 20:56:39,609 [INFO] Step 12390/19460 | Loss: 1.0170 | Acc: 0.6875 | LR: 5.28e-06
2026-01-07 20:56:43,343 [INFO] Step 12400/19460 | Loss: 0.7813 | Acc: 0.8750 | LR: 5.27e-06
2026-01-07 20:56:52,082 [INFO] [EVAL] Step 12400 | Val Loss: 0.3281 | Val Acc: 0.9131
2026-01-07 20:56:55,369 [INFO] Step 12410/19460 | Loss: 0.4995 | Acc: 1.0000 | LR: 5.26e-06
2026-01-07 20:56:58,616 [INFO] Step 12420/19460 | Loss: 0.6784 | Acc: 0.9375 | LR: 5.25e-06
2026-01-07 20:57:02,102 [INFO] Step 12430/19460 | Loss: 0.8443 | Acc: 0.8750 | LR: 5.24e-06
2026-01-07 20:57:05,467 [INFO] Step 12440/19460 | Loss: 1.0448 | Acc: 0.7500 | LR: 5.23e-06
2026-01-07 20:57:08,577 [INFO] Step 12450/19460 | Loss: 0.7312 | Acc: 0.8750 | LR: 5.22e-06
2026-01-07 20:57:11,890 [INFO] Step 12460/19460 | Loss: 0.7315 | Acc: 0.8750 | LR: 5.20e-06
2026-01-07 20:57:15,115 [INFO] Step 12470/19460 | Loss: 0.6259 | Acc: 0.8750 | LR: 5.19e-06
2026-01-07 20:57:18,345 [INFO] Step 12480/19460 | Loss: 1.0425 | Acc: 0.6250 | LR: 5.18e-06
2026-01-07 20:57:21,885 [INFO] Step 12490/19460 | Loss: 0.7262 | Acc: 0.8750 | LR: 5.17e-06
2026-01-07 20:57:25,788 [INFO] Step 12500/19460 | Loss: 0.6337 | Acc: 0.8750 | LR: 5.16e-06
2026-01-07 20:57:34,544 [INFO] [EVAL] Step 12500 | Val Loss: 0.3279 | Val Acc: 0.9106
2026-01-07 20:57:34,582 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_12500.npz
2026-01-07 20:57:37,807 [INFO] Step 12510/19460 | Loss: 0.7756 | Acc: 0.8750 | LR: 5.15e-06
2026-01-07 20:57:41,013 [INFO] Step 12520/19460 | Loss: 0.6047 | Acc: 0.9375 | LR: 5.14e-06
2026-01-07 20:57:44,418 [INFO] Step 12530/19460 | Loss: 0.8434 | Acc: 0.8125 | LR: 5.13e-06
2026-01-07 20:57:47,711 [INFO] Step 12540/19460 | Loss: 0.9693 | Acc: 0.7500 | LR: 5.12e-06
2026-01-07 20:57:51,081 [INFO] Step 12550/19460 | Loss: 0.6280 | Acc: 0.9375 | LR: 5.11e-06
2026-01-07 20:57:54,364 [INFO] Step 12560/19460 | Loss: 0.6824 | Acc: 0.8750 | LR: 5.10e-06
2026-01-07 20:57:57,807 [INFO] Step 12570/19460 | Loss: 0.6689 | Acc: 1.0000 | LR: 5.09e-06
2026-01-07 20:57:58,535 [INFO] Epoch 13 complete | Avg Loss: 0.7347 | Avg Acc: 0.8520 | Updates: 969 | Micro-batches: 971 | Skipped: 2 (loss=2, logits=0, grads=0)
2026-01-07 20:57:58,535 [INFO] Epoch 14/20
2026-01-07 20:58:01,240 [INFO] Step 12580/19460 | Loss: 0.5279 | Acc: 0.9375 | LR: 5.08e-06
2026-01-07 20:58:04,527 [INFO] Step 12590/19460 | Loss: 0.5697 | Acc: 0.9375 | LR: 5.07e-06
2026-01-07 20:58:08,347 [INFO] Step 12600/19460 | Loss: 0.8713 | Acc: 0.8125 | LR: 5.06e-06
2026-01-07 20:58:17,133 [INFO] [EVAL] Step 12600 | Val Loss: 0.3244 | Val Acc: 0.9113
2026-01-07 20:58:20,481 [INFO] Step 12610/19460 | Loss: 0.6575 | Acc: 0.8750 | LR: 5.05e-06
2026-01-07 20:58:23,667 [INFO] Step 12620/19460 | Loss: 0.5257 | Acc: 1.0000 | LR: 5.04e-06
2026-01-07 20:58:27,005 [INFO] Step 12630/19460 | Loss: 0.5876 | Acc: 0.9375 | LR: 5.03e-06
2026-01-07 20:58:30,168 [INFO] Step 12640/19460 | Loss: 0.6918 | Acc: 0.8125 | LR: 5.01e-06
2026-01-07 20:58:33,307 [INFO] Step 12650/19460 | Loss: 0.4624 | Acc: 1.0000 | LR: 5.00e-06
2026-01-07 20:58:36,678 [INFO] Step 12660/19460 | Loss: 0.7457 | Acc: 0.8750 | LR: 4.99e-06
2026-01-07 20:58:39,895 [INFO] Step 12670/19460 | Loss: 0.8341 | Acc: 0.7500 | LR: 4.98e-06
2026-01-07 20:58:43,346 [INFO] Step 12680/19460 | Loss: 0.5762 | Acc: 0.8750 | LR: 4.97e-06
2026-01-07 20:58:46,438 [INFO] Step 12690/19460 | Loss: 0.9304 | Acc: 0.6250 | LR: 4.96e-06
2026-01-07 20:58:50,394 [INFO] Step 12700/19460 | Loss: 0.7587 | Acc: 0.8750 | LR: 4.95e-06
2026-01-07 20:58:59,216 [INFO] [EVAL] Step 12700 | Val Loss: 0.3232 | Val Acc: 0.9156
2026-01-07 20:59:02,542 [INFO] Step 12710/19460 | Loss: 0.5475 | Acc: 0.8750 | LR: 4.94e-06
2026-01-07 20:59:05,775 [INFO] Step 12720/19460 | Loss: 0.7129 | Acc: 0.8750 | LR: 4.93e-06
2026-01-07 20:59:09,324 [INFO] Step 12730/19460 | Loss: 0.8448 | Acc: 0.8125 | LR: 4.92e-06
2026-01-07 20:59:12,560 [INFO] Step 12740/19460 | Loss: 0.8770 | Acc: 0.8750 | LR: 4.91e-06
2026-01-07 20:59:13,373 [WARNING] Skipping batch due to non-finite loss at step=12742 (loss=nan, epoch=14).
2026-01-07 20:59:16,044 [INFO] Step 12750/19460 | Loss: 0.6446 | Acc: 0.9375 | LR: 4.90e-06
2026-01-07 20:59:19,361 [INFO] Step 12760/19460 | Loss: 0.7586 | Acc: 0.8125 | LR: 4.89e-06
2026-01-07 20:59:22,701 [INFO] Step 12770/19460 | Loss: 0.8663 | Acc: 0.7500 | LR: 4.88e-06
2026-01-07 20:59:26,046 [INFO] Step 12780/19460 | Loss: 0.6497 | Acc: 0.8125 | LR: 4.87e-06
2026-01-07 20:59:29,236 [INFO] Step 12790/19460 | Loss: 0.4627 | Acc: 1.0000 | LR: 4.86e-06
2026-01-07 20:59:33,085 [INFO] Step 12800/19460 | Loss: 0.5061 | Acc: 1.0000 | LR: 4.85e-06
2026-01-07 20:59:41,876 [INFO] [EVAL] Step 12800 | Val Loss: 0.3338 | Val Acc: 0.9113
2026-01-07 20:59:45,265 [INFO] Step 12810/19460 | Loss: 0.6611 | Acc: 0.8750 | LR: 4.84e-06
2026-01-07 20:59:48,486 [INFO] Step 12820/19460 | Loss: 0.6615 | Acc: 0.8125 | LR: 4.83e-06
2026-01-07 20:59:51,747 [INFO] Step 12830/19460 | Loss: 1.0663 | Acc: 0.8125 | LR: 4.82e-06
2026-01-07 20:59:54,960 [INFO] Step 12840/19460 | Loss: 0.8758 | Acc: 0.7500 | LR: 4.81e-06
2026-01-07 20:59:58,304 [INFO] Step 12850/19460 | Loss: 1.0589 | Acc: 0.6875 | LR: 4.80e-06
2026-01-07 21:00:01,740 [INFO] Step 12860/19460 | Loss: 0.8995 | Acc: 0.8125 | LR: 4.79e-06
2026-01-07 21:00:04,902 [INFO] Step 12870/19460 | Loss: 0.6073 | Acc: 0.8750 | LR: 4.78e-06
2026-01-07 21:00:08,230 [INFO] Step 12880/19460 | Loss: 0.7813 | Acc: 0.7500 | LR: 4.77e-06
2026-01-07 21:00:11,695 [INFO] Step 12890/19460 | Loss: 0.7429 | Acc: 0.8750 | LR: 4.76e-06
2026-01-07 21:00:15,633 [INFO] Step 12900/19460 | Loss: 0.8239 | Acc: 0.7500 | LR: 4.74e-06
2026-01-07 21:00:24,380 [INFO] [EVAL] Step 12900 | Val Loss: 0.3201 | Val Acc: 0.9181
2026-01-07 21:00:27,626 [INFO] Step 12910/19460 | Loss: 0.6118 | Acc: 0.8750 | LR: 4.73e-06
2026-01-07 21:00:30,856 [INFO] Step 12920/19460 | Loss: 0.6949 | Acc: 0.8125 | LR: 4.72e-06
2026-01-07 21:00:31,042 [WARNING] Skipping batch due to non-finite loss at step=12920 (loss=nan, epoch=14).
2026-01-07 21:00:34,399 [INFO] Step 12930/19460 | Loss: 0.7265 | Acc: 0.9375 | LR: 4.71e-06
2026-01-07 21:00:37,639 [INFO] Step 12940/19460 | Loss: 0.7590 | Acc: 0.8750 | LR: 4.70e-06
2026-01-07 21:00:40,879 [INFO] Step 12950/19460 | Loss: 0.4842 | Acc: 0.9375 | LR: 4.69e-06
2026-01-07 21:00:44,034 [INFO] Step 12960/19460 | Loss: 0.7341 | Acc: 0.8750 | LR: 4.68e-06
2026-01-07 21:00:47,257 [INFO] Step 12970/19460 | Loss: 0.9421 | Acc: 0.8125 | LR: 4.67e-06
2026-01-07 21:00:50,436 [INFO] Step 12980/19460 | Loss: 0.8628 | Acc: 0.7500 | LR: 4.66e-06
2026-01-07 21:00:53,729 [INFO] Step 12990/19460 | Loss: 0.7446 | Acc: 0.8750 | LR: 4.65e-06
2026-01-07 21:00:57,471 [INFO] Step 13000/19460 | Loss: 0.6987 | Acc: 0.8750 | LR: 4.64e-06
2026-01-07 21:01:06,219 [INFO] [EVAL] Step 13000 | Val Loss: 0.3195 | Val Acc: 0.9137
2026-01-07 21:01:06,253 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_13000.npz
2026-01-07 21:01:09,484 [INFO] Step 13010/19460 | Loss: 0.8180 | Acc: 0.8125 | LR: 4.63e-06
2026-01-07 21:01:12,852 [INFO] Step 13020/19460 | Loss: 0.6492 | Acc: 0.9375 | LR: 4.62e-06
2026-01-07 21:01:16,138 [INFO] Step 13030/19460 | Loss: 0.4979 | Acc: 1.0000 | LR: 4.61e-06
2026-01-07 21:01:19,421 [INFO] Step 13040/19460 | Loss: 1.2067 | Acc: 0.6875 | LR: 4.60e-06
2026-01-07 21:01:22,686 [INFO] Step 13050/19460 | Loss: 0.5968 | Acc: 0.9375 | LR: 4.59e-06
2026-01-07 21:01:26,061 [INFO] Step 13060/19460 | Loss: 0.7426 | Acc: 0.8125 | LR: 4.58e-06
2026-01-07 21:01:29,420 [INFO] Step 13070/19460 | Loss: 0.7698 | Acc: 0.8750 | LR: 4.57e-06
2026-01-07 21:01:32,787 [INFO] Step 13080/19460 | Loss: 0.8768 | Acc: 0.8125 | LR: 4.56e-06
2026-01-07 21:01:36,156 [INFO] Step 13090/19460 | Loss: 0.8703 | Acc: 0.7500 | LR: 4.55e-06
2026-01-07 21:01:40,036 [INFO] Step 13100/19460 | Loss: 1.0392 | Acc: 0.7500 | LR: 4.54e-06
2026-01-07 21:01:48,830 [INFO] [EVAL] Step 13100 | Val Loss: 0.3218 | Val Acc: 0.9187
2026-01-07 21:01:48,867 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 21:01:48,868 [INFO] New best validation accuracy: 0.9187
2026-01-07 21:01:52,212 [INFO] Step 13110/19460 | Loss: 0.4885 | Acc: 0.9375 | LR: 4.53e-06
2026-01-07 21:01:55,698 [INFO] Step 13120/19460 | Loss: 0.8213 | Acc: 0.7500 | LR: 4.52e-06
2026-01-07 21:01:58,862 [INFO] Step 13130/19460 | Loss: 0.7558 | Acc: 0.8750 | LR: 4.51e-06
2026-01-07 21:02:02,239 [INFO] Step 13140/19460 | Loss: 0.5468 | Acc: 1.0000 | LR: 4.50e-06
2026-01-07 21:02:05,530 [INFO] Step 13150/19460 | Loss: 0.6627 | Acc: 0.9375 | LR: 4.49e-06
2026-01-07 21:02:08,694 [INFO] Step 13160/19460 | Loss: 0.6407 | Acc: 0.8750 | LR: 4.48e-06
2026-01-07 21:02:11,934 [INFO] Step 13170/19460 | Loss: 0.7022 | Acc: 0.9375 | LR: 4.47e-06
2026-01-07 21:02:14,955 [INFO] Step 13180/19460 | Loss: 0.5715 | Acc: 0.9375 | LR: 4.46e-06
2026-01-07 21:02:17,996 [INFO] Step 13190/19460 | Loss: 0.6117 | Acc: 0.9375 | LR: 4.45e-06
2026-01-07 21:02:21,768 [INFO] Step 13200/19460 | Loss: 0.5648 | Acc: 0.9375 | LR: 4.44e-06
2026-01-07 21:02:30,468 [INFO] [EVAL] Step 13200 | Val Loss: 0.3254 | Val Acc: 0.9175
2026-01-07 21:02:33,717 [INFO] Step 13210/19460 | Loss: 0.8040 | Acc: 0.8125 | LR: 4.43e-06
2026-01-07 21:02:36,937 [INFO] Step 13220/19460 | Loss: 1.0675 | Acc: 0.7500 | LR: 4.42e-06
2026-01-07 21:02:40,264 [INFO] Step 13230/19460 | Loss: 0.9572 | Acc: 0.6875 | LR: 4.41e-06
2026-01-07 21:02:43,493 [INFO] Step 13240/19460 | Loss: 0.7726 | Acc: 0.8750 | LR: 4.40e-06
2026-01-07 21:02:46,734 [INFO] Step 13250/19460 | Loss: 0.7203 | Acc: 0.8125 | LR: 4.39e-06
2026-01-07 21:02:49,905 [INFO] Step 13260/19460 | Loss: 0.9465 | Acc: 0.8125 | LR: 4.38e-06
2026-01-07 21:02:53,048 [INFO] Step 13270/19460 | Loss: 0.8535 | Acc: 0.8125 | LR: 4.37e-06
2026-01-07 21:02:56,259 [INFO] Step 13280/19460 | Loss: 0.6077 | Acc: 0.8750 | LR: 4.36e-06
2026-01-07 21:02:59,444 [INFO] Step 13290/19460 | Loss: 0.6093 | Acc: 0.9375 | LR: 4.35e-06
2026-01-07 21:03:03,103 [INFO] Step 13300/19460 | Loss: 0.7864 | Acc: 0.8750 | LR: 4.34e-06
2026-01-07 21:03:11,647 [INFO] [EVAL] Step 13300 | Val Loss: 0.3277 | Val Acc: 0.9144
2026-01-07 21:03:14,777 [INFO] Step 13310/19460 | Loss: 0.7238 | Acc: 0.8125 | LR: 4.33e-06
2026-01-07 21:03:17,854 [INFO] Step 13320/19460 | Loss: 0.7237 | Acc: 0.8125 | LR: 4.32e-06
2026-01-07 21:03:20,890 [INFO] Step 13330/19460 | Loss: 0.8007 | Acc: 0.8125 | LR: 4.31e-06
2026-01-07 21:03:24,124 [INFO] Step 13340/19460 | Loss: 0.6522 | Acc: 0.8125 | LR: 4.30e-06
2026-01-07 21:03:27,370 [INFO] Step 13350/19460 | Loss: 0.9358 | Acc: 0.7500 | LR: 4.29e-06
2026-01-07 21:03:30,457 [INFO] Step 13360/19460 | Loss: 0.5554 | Acc: 1.0000 | LR: 4.28e-06
2026-01-07 21:03:33,739 [INFO] Step 13370/19460 | Loss: 0.8205 | Acc: 0.7500 | LR: 4.27e-06
2026-01-07 21:03:36,809 [INFO] Step 13380/19460 | Loss: 0.9760 | Acc: 0.7500 | LR: 4.26e-06
2026-01-07 21:03:39,977 [INFO] Step 13390/19460 | Loss: 0.5622 | Acc: 0.9375 | LR: 4.25e-06
2026-01-07 21:03:43,810 [INFO] Step 13400/19460 | Loss: 0.5205 | Acc: 0.9375 | LR: 4.24e-06
2026-01-07 21:03:52,291 [INFO] [EVAL] Step 13400 | Val Loss: 0.3179 | Val Acc: 0.9200
2026-01-07 21:03:52,330 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 21:03:52,331 [INFO] New best validation accuracy: 0.9200
2026-01-07 21:03:55,698 [INFO] Step 13410/19460 | Loss: 0.9694 | Acc: 0.7500 | LR: 4.23e-06
2026-01-07 21:03:58,736 [INFO] Step 13420/19460 | Loss: 0.5067 | Acc: 1.0000 | LR: 4.22e-06
2026-01-07 21:04:01,902 [INFO] Step 13430/19460 | Loss: 0.5512 | Acc: 0.9375 | LR: 4.21e-06
2026-01-07 21:04:05,220 [INFO] Step 13440/19460 | Loss: 0.8106 | Acc: 0.8750 | LR: 4.20e-06
2026-01-07 21:04:08,462 [INFO] Step 13450/19460 | Loss: 0.7190 | Acc: 0.8750 | LR: 4.19e-06
2026-01-07 21:04:11,762 [INFO] Step 13460/19460 | Loss: 0.6007 | Acc: 1.0000 | LR: 4.18e-06
2026-01-07 21:04:15,266 [INFO] Step 13470/19460 | Loss: 0.9410 | Acc: 0.7500 | LR: 4.17e-06
2026-01-07 21:04:18,608 [INFO] Step 13480/19460 | Loss: 0.9783 | Acc: 0.6875 | LR: 4.17e-06
2026-01-07 21:04:21,735 [INFO] Step 13490/19460 | Loss: 0.7862 | Acc: 0.7500 | LR: 4.16e-06
2026-01-07 21:04:25,485 [INFO] Step 13500/19460 | Loss: 0.5565 | Acc: 0.9375 | LR: 4.15e-06
2026-01-07 21:04:34,262 [INFO] [EVAL] Step 13500 | Val Loss: 0.3185 | Val Acc: 0.9169
2026-01-07 21:04:34,300 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_13500.npz
2026-01-07 21:04:37,646 [INFO] Step 13510/19460 | Loss: 0.5566 | Acc: 0.9375 | LR: 4.14e-06
2026-01-07 21:04:40,910 [INFO] Step 13520/19460 | Loss: 0.9685 | Acc: 0.8125 | LR: 4.13e-06
2026-01-07 21:04:44,163 [INFO] Step 13530/19460 | Loss: 0.8410 | Acc: 0.8750 | LR: 4.12e-06
2026-01-07 21:04:47,361 [INFO] Step 13540/19460 | Loss: 0.7925 | Acc: 0.7500 | LR: 4.11e-06
2026-01-07 21:04:47,692 [INFO] Epoch 14 complete | Avg Loss: 0.7208 | Avg Acc: 0.8599 | Updates: 969 | Micro-batches: 971 | Skipped: 2 (loss=2, logits=0, grads=0)
2026-01-07 21:04:47,692 [INFO] Epoch 15/20
2026-01-07 21:04:50,822 [INFO] Step 13550/19460 | Loss: 0.5529 | Acc: 0.9375 | LR: 4.10e-06
2026-01-07 21:04:54,082 [INFO] Step 13560/19460 | Loss: 0.8402 | Acc: 0.8125 | LR: 4.09e-06
2026-01-07 21:04:57,523 [INFO] Step 13570/19460 | Loss: 0.5815 | Acc: 0.9375 | LR: 4.08e-06
2026-01-07 21:05:00,843 [INFO] Step 13580/19460 | Loss: 0.6356 | Acc: 0.8750 | LR: 4.07e-06
2026-01-07 21:05:04,157 [INFO] Step 13590/19460 | Loss: 0.5750 | Acc: 0.9375 | LR: 4.06e-06
2026-01-07 21:05:07,860 [INFO] Step 13600/19460 | Loss: 0.6815 | Acc: 0.9375 | LR: 4.05e-06
2026-01-07 21:05:16,632 [INFO] [EVAL] Step 13600 | Val Loss: 0.3328 | Val Acc: 0.9106
2026-01-07 21:05:19,891 [INFO] Step 13610/19460 | Loss: 0.5375 | Acc: 0.9375 | LR: 4.04e-06
2026-01-07 21:05:23,192 [INFO] Step 13620/19460 | Loss: 0.6465 | Acc: 0.8750 | LR: 4.03e-06
2026-01-07 21:05:26,422 [INFO] Step 13630/19460 | Loss: 0.6929 | Acc: 0.8125 | LR: 4.02e-06
2026-01-07 21:05:29,630 [INFO] Step 13640/19460 | Loss: 0.8609 | Acc: 0.8125 | LR: 4.01e-06
2026-01-07 21:05:33,005 [INFO] Step 13650/19460 | Loss: 0.4854 | Acc: 1.0000 | LR: 4.00e-06
2026-01-07 21:05:36,421 [INFO] Step 13660/19460 | Loss: 0.8274 | Acc: 0.8125 | LR: 3.99e-06
2026-01-07 21:05:39,691 [INFO] Step 13670/19460 | Loss: 0.4800 | Acc: 1.0000 | LR: 3.98e-06
2026-01-07 21:05:42,920 [INFO] Step 13680/19460 | Loss: 0.5968 | Acc: 0.8750 | LR: 3.97e-06
2026-01-07 21:05:46,247 [INFO] Step 13690/19460 | Loss: 0.5907 | Acc: 0.8750 | LR: 3.96e-06
2026-01-07 21:05:50,131 [INFO] Step 13700/19460 | Loss: 0.5066 | Acc: 1.0000 | LR: 3.95e-06
2026-01-07 21:05:58,835 [INFO] [EVAL] Step 13700 | Val Loss: 0.3217 | Val Acc: 0.9194
2026-01-07 21:06:02,024 [INFO] Step 13710/19460 | Loss: 0.7534 | Acc: 0.8125 | LR: 3.94e-06
2026-01-07 21:06:05,213 [INFO] Step 13720/19460 | Loss: 0.7406 | Acc: 0.7500 | LR: 3.94e-06
2026-01-07 21:06:08,549 [INFO] Step 13730/19460 | Loss: 0.9010 | Acc: 0.7500 | LR: 3.93e-06
2026-01-07 21:06:11,817 [INFO] Step 13740/19460 | Loss: 0.6142 | Acc: 0.8125 | LR: 3.92e-06
2026-01-07 21:06:14,919 [INFO] Step 13750/19460 | Loss: 0.4888 | Acc: 1.0000 | LR: 3.91e-06
2026-01-07 21:06:18,176 [INFO] Step 13760/19460 | Loss: 0.7525 | Acc: 0.8750 | LR: 3.90e-06
2026-01-07 21:06:21,403 [INFO] Step 13770/19460 | Loss: 0.6506 | Acc: 0.9375 | LR: 3.89e-06
2026-01-07 21:06:24,704 [INFO] Step 13780/19460 | Loss: 1.0569 | Acc: 0.6250 | LR: 3.88e-06
2026-01-07 21:06:28,040 [INFO] Step 13790/19460 | Loss: 0.8784 | Acc: 0.6875 | LR: 3.87e-06
2026-01-07 21:06:31,801 [INFO] Step 13800/19460 | Loss: 0.6983 | Acc: 0.8125 | LR: 3.86e-06
2026-01-07 21:06:40,564 [INFO] [EVAL] Step 13800 | Val Loss: 0.3249 | Val Acc: 0.9175
2026-01-07 21:06:43,868 [INFO] Step 13810/19460 | Loss: 0.6525 | Acc: 0.9375 | LR: 3.85e-06
2026-01-07 21:06:46,960 [INFO] Step 13820/19460 | Loss: 0.6148 | Acc: 0.8750 | LR: 3.84e-06
2026-01-07 21:06:50,167 [INFO] Step 13830/19460 | Loss: 0.7434 | Acc: 0.8125 | LR: 3.83e-06
2026-01-07 21:06:53,431 [INFO] Step 13840/19460 | Loss: 0.5759 | Acc: 0.9375 | LR: 3.82e-06
2026-01-07 21:06:56,691 [INFO] Step 13850/19460 | Loss: 0.6978 | Acc: 0.8750 | LR: 3.81e-06
2026-01-07 21:07:00,067 [INFO] Step 13860/19460 | Loss: 0.5215 | Acc: 0.9375 | LR: 3.80e-06
2026-01-07 21:07:03,296 [INFO] Step 13870/19460 | Loss: 0.6667 | Acc: 0.8750 | LR: 3.80e-06
2026-01-07 21:07:06,663 [INFO] Step 13880/19460 | Loss: 0.6108 | Acc: 0.9375 | LR: 3.79e-06
2026-01-07 21:07:10,003 [INFO] Step 13890/19460 | Loss: 0.5200 | Acc: 0.9375 | LR: 3.78e-06
2026-01-07 21:07:13,849 [INFO] Step 13900/19460 | Loss: 0.8436 | Acc: 0.8750 | LR: 3.77e-06
2026-01-07 21:07:22,676 [INFO] [EVAL] Step 13900 | Val Loss: 0.3204 | Val Acc: 0.9200
2026-01-07 21:07:25,829 [INFO] Step 13910/19460 | Loss: 0.5751 | Acc: 0.9375 | LR: 3.76e-06
2026-01-07 21:07:29,165 [INFO] Step 13920/19460 | Loss: 0.7367 | Acc: 0.8125 | LR: 3.75e-06
2026-01-07 21:07:32,602 [INFO] Step 13930/19460 | Loss: 0.5144 | Acc: 1.0000 | LR: 3.74e-06
2026-01-07 21:07:35,916 [INFO] Step 13940/19460 | Loss: 0.8731 | Acc: 0.8125 | LR: 3.73e-06
2026-01-07 21:07:39,336 [INFO] Step 13950/19460 | Loss: 0.5409 | Acc: 0.9375 | LR: 3.72e-06
2026-01-07 21:07:42,629 [INFO] Step 13960/19460 | Loss: 0.4493 | Acc: 1.0000 | LR: 3.71e-06
2026-01-07 21:07:45,923 [INFO] Step 13970/19460 | Loss: 0.4853 | Acc: 1.0000 | LR: 3.70e-06
2026-01-07 21:07:49,210 [INFO] Step 13980/19460 | Loss: 0.7473 | Acc: 0.8125 | LR: 3.69e-06
2026-01-07 21:07:52,437 [INFO] Step 13990/19460 | Loss: 0.4364 | Acc: 1.0000 | LR: 3.68e-06
2026-01-07 21:07:56,246 [INFO] Step 14000/19460 | Loss: 0.7648 | Acc: 0.8125 | LR: 3.68e-06
2026-01-07 21:08:05,037 [INFO] [EVAL] Step 14000 | Val Loss: 0.3147 | Val Acc: 0.9156
2026-01-07 21:08:05,069 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_14000.npz
2026-01-07 21:08:08,336 [INFO] Step 14010/19460 | Loss: 0.8918 | Acc: 0.8125 | LR: 3.67e-06
2026-01-07 21:08:11,713 [INFO] Step 14020/19460 | Loss: 0.6516 | Acc: 0.8750 | LR: 3.66e-06
2026-01-07 21:08:15,173 [INFO] Step 14030/19460 | Loss: 0.6146 | Acc: 0.8750 | LR: 3.65e-06
2026-01-07 21:08:18,484 [INFO] Step 14040/19460 | Loss: 0.4494 | Acc: 1.0000 | LR: 3.64e-06
2026-01-07 21:08:21,893 [INFO] Step 14050/19460 | Loss: 0.7808 | Acc: 0.8125 | LR: 3.63e-06
2026-01-07 21:08:25,116 [INFO] Step 14060/19460 | Loss: 0.7791 | Acc: 0.8125 | LR: 3.62e-06
2026-01-07 21:08:28,313 [INFO] Step 14070/19460 | Loss: 0.6206 | Acc: 0.8750 | LR: 3.61e-06
2026-01-07 21:08:31,630 [INFO] Step 14080/19460 | Loss: 0.5877 | Acc: 0.9375 | LR: 3.60e-06
2026-01-07 21:08:35,003 [INFO] Step 14090/19460 | Loss: 0.9273 | Acc: 0.7500 | LR: 3.59e-06
2026-01-07 21:08:38,825 [INFO] Step 14100/19460 | Loss: 0.6963 | Acc: 0.9375 | LR: 3.58e-06
2026-01-07 21:08:47,518 [INFO] [EVAL] Step 14100 | Val Loss: 0.3228 | Val Acc: 0.9131
2026-01-07 21:08:50,851 [INFO] Step 14110/19460 | Loss: 0.4714 | Acc: 1.0000 | LR: 3.58e-06
2026-01-07 21:08:54,289 [INFO] Step 14120/19460 | Loss: 0.7597 | Acc: 0.8750 | LR: 3.57e-06
2026-01-07 21:08:57,649 [INFO] Step 14130/19460 | Loss: 0.5236 | Acc: 1.0000 | LR: 3.56e-06
2026-01-07 21:09:00,950 [INFO] Step 14140/19460 | Loss: 0.6326 | Acc: 0.8750 | LR: 3.55e-06
2026-01-07 21:09:04,298 [INFO] Step 14150/19460 | Loss: 0.6809 | Acc: 0.8750 | LR: 3.54e-06
2026-01-07 21:09:07,629 [INFO] Step 14160/19460 | Loss: 0.6756 | Acc: 0.8750 | LR: 3.53e-06
2026-01-07 21:09:10,846 [INFO] Step 14170/19460 | Loss: 0.8990 | Acc: 0.8125 | LR: 3.52e-06
2026-01-07 21:09:14,236 [INFO] Step 14180/19460 | Loss: 0.5160 | Acc: 0.9375 | LR: 3.51e-06
2026-01-07 21:09:17,576 [INFO] Step 14190/19460 | Loss: 0.7354 | Acc: 0.8750 | LR: 3.50e-06
2026-01-07 21:09:21,368 [INFO] Step 14200/19460 | Loss: 0.5652 | Acc: 0.9375 | LR: 3.50e-06
2026-01-07 21:09:30,049 [INFO] [EVAL] Step 14200 | Val Loss: 0.3173 | Val Acc: 0.9150
2026-01-07 21:09:33,279 [INFO] Step 14210/19460 | Loss: 0.4839 | Acc: 1.0000 | LR: 3.49e-06
2026-01-07 21:09:36,556 [INFO] Step 14220/19460 | Loss: 0.8749 | Acc: 0.8125 | LR: 3.48e-06
2026-01-07 21:09:39,832 [INFO] Step 14230/19460 | Loss: 0.8886 | Acc: 0.8125 | LR: 3.47e-06
2026-01-07 21:09:43,073 [INFO] Step 14240/19460 | Loss: 0.8597 | Acc: 0.7500 | LR: 3.46e-06
2026-01-07 21:09:46,445 [INFO] Step 14250/19460 | Loss: 0.5593 | Acc: 1.0000 | LR: 3.45e-06
2026-01-07 21:09:49,743 [INFO] Step 14260/19460 | Loss: 0.5700 | Acc: 0.9375 | LR: 3.44e-06
2026-01-07 21:09:52,955 [INFO] Step 14270/19460 | Loss: 0.7054 | Acc: 0.8750 | LR: 3.43e-06
2026-01-07 21:09:56,250 [INFO] Step 14280/19460 | Loss: 0.5765 | Acc: 0.9375 | LR: 3.42e-06
2026-01-07 21:09:59,521 [INFO] Step 14290/19460 | Loss: 0.9498 | Acc: 0.6875 | LR: 3.42e-06
2026-01-07 21:10:02,036 [WARNING] Skipping batch due to non-finite loss at step=14297 (loss=nan, epoch=15).
2026-01-07 21:10:03,689 [INFO] Step 14300/19460 | Loss: 0.6361 | Acc: 0.8750 | LR: 3.41e-06
2026-01-07 21:10:12,353 [INFO] [EVAL] Step 14300 | Val Loss: 0.3194 | Val Acc: 0.9163
2026-01-07 21:10:15,696 [INFO] Step 14310/19460 | Loss: 0.7439 | Acc: 0.8750 | LR: 3.40e-06
2026-01-07 21:10:18,968 [INFO] Step 14320/19460 | Loss: 0.6560 | Acc: 0.8750 | LR: 3.39e-06
2026-01-07 21:10:22,172 [INFO] Step 14330/19460 | Loss: 0.8186 | Acc: 0.8125 | LR: 3.38e-06
2026-01-07 21:10:25,364 [INFO] Step 14340/19460 | Loss: 0.8280 | Acc: 0.7500 | LR: 3.37e-06
2026-01-07 21:10:28,710 [INFO] Step 14350/19460 | Loss: 0.5461 | Acc: 0.9375 | LR: 3.36e-06
2026-01-07 21:10:31,955 [INFO] Step 14360/19460 | Loss: 0.6024 | Acc: 0.8750 | LR: 3.36e-06
2026-01-07 21:10:35,477 [INFO] Step 14370/19460 | Loss: 0.7591 | Acc: 0.8750 | LR: 3.35e-06
2026-01-07 21:10:38,863 [INFO] Step 14380/19460 | Loss: 0.7517 | Acc: 0.8125 | LR: 3.34e-06
2026-01-07 21:10:42,105 [INFO] Step 14390/19460 | Loss: 0.6076 | Acc: 0.8750 | LR: 3.33e-06
2026-01-07 21:10:46,107 [INFO] Step 14400/19460 | Loss: 0.6191 | Acc: 0.8750 | LR: 3.32e-06
2026-01-07 21:10:54,815 [INFO] [EVAL] Step 14400 | Val Loss: 0.3198 | Val Acc: 0.9131
2026-01-07 21:10:58,153 [INFO] Step 14410/19460 | Loss: 0.7188 | Acc: 0.8750 | LR: 3.31e-06
2026-01-07 21:11:01,413 [INFO] Step 14420/19460 | Loss: 0.8192 | Acc: 0.7500 | LR: 3.30e-06
2026-01-07 21:11:02,622 [WARNING] Skipping batch due to non-finite loss at step=14423 (loss=nan, epoch=15).
2026-01-07 21:11:04,973 [INFO] Step 14430/19460 | Loss: 0.5185 | Acc: 1.0000 | LR: 3.29e-06
2026-01-07 21:11:08,240 [INFO] Step 14440/19460 | Loss: 0.6764 | Acc: 0.8750 | LR: 3.29e-06
2026-01-07 21:11:11,646 [INFO] Step 14450/19460 | Loss: 0.5290 | Acc: 1.0000 | LR: 3.28e-06
2026-01-07 21:11:14,954 [INFO] Step 14460/19460 | Loss: 0.8836 | Acc: 0.7500 | LR: 3.27e-06
2026-01-07 21:11:16,390 [WARNING] Skipping batch due to non-finite loss at step=14464 (loss=nan, epoch=15).
2026-01-07 21:11:18,234 [INFO] Step 14470/19460 | Loss: 0.6701 | Acc: 0.9375 | LR: 3.26e-06
2026-01-07 21:11:21,543 [INFO] Step 14480/19460 | Loss: 0.7126 | Acc: 0.7500 | LR: 3.25e-06
2026-01-07 21:11:24,709 [INFO] Step 14490/19460 | Loss: 0.6986 | Acc: 0.8125 | LR: 3.24e-06
2026-01-07 21:11:25,236 [WARNING] Skipping batch due to non-finite loss at step=14491 (loss=nan, epoch=15).
2026-01-07 21:11:28,669 [INFO] Step 14500/19460 | Loss: 0.7135 | Acc: 0.8125 | LR: 3.23e-06
2026-01-07 21:11:37,430 [INFO] [EVAL] Step 14500 | Val Loss: 0.3247 | Val Acc: 0.9156
2026-01-07 21:11:37,459 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_14500.npz
2026-01-07 21:11:40,209 [INFO] Epoch 15 complete | Avg Loss: 0.7090 | Avg Acc: 0.8641 | Updates: 967 | Micro-batches: 971 | Skipped: 4 (loss=4, logits=0, grads=0)
2026-01-07 21:11:40,209 [INFO] Epoch 16/20
2026-01-07 21:11:40,903 [INFO] Step 14510/19460 | Loss: 0.6069 | Acc: 0.9375 | LR: 3.23e-06
2026-01-07 21:11:44,174 [INFO] Step 14520/19460 | Loss: 0.9158 | Acc: 0.8125 | LR: 3.22e-06
2026-01-07 21:11:47,500 [INFO] Step 14530/19460 | Loss: 0.7653 | Acc: 0.8125 | LR: 3.21e-06
2026-01-07 21:11:50,650 [INFO] Step 14540/19460 | Loss: 0.4811 | Acc: 1.0000 | LR: 3.20e-06
2026-01-07 21:11:53,930 [INFO] Step 14550/19460 | Loss: 0.6632 | Acc: 0.8750 | LR: 3.19e-06
2026-01-07 21:11:57,329 [INFO] Step 14560/19460 | Loss: 0.7300 | Acc: 0.8750 | LR: 3.18e-06
2026-01-07 21:12:00,549 [INFO] Step 14570/19460 | Loss: 0.8674 | Acc: 0.7500 | LR: 3.18e-06
2026-01-07 21:12:03,773 [INFO] Step 14580/19460 | Loss: 0.5861 | Acc: 0.9375 | LR: 3.17e-06
2026-01-07 21:12:07,119 [INFO] Step 14590/19460 | Loss: 0.8616 | Acc: 0.7500 | LR: 3.16e-06
2026-01-07 21:12:11,183 [INFO] Step 14600/19460 | Loss: 0.8002 | Acc: 0.8125 | LR: 3.15e-06
2026-01-07 21:12:19,957 [INFO] [EVAL] Step 14600 | Val Loss: 0.3234 | Val Acc: 0.9137
2026-01-07 21:12:23,223 [INFO] Step 14610/19460 | Loss: 0.6532 | Acc: 0.8750 | LR: 3.14e-06
2026-01-07 21:12:26,523 [INFO] Step 14620/19460 | Loss: 0.8665 | Acc: 0.6875 | LR: 3.13e-06
2026-01-07 21:12:29,809 [INFO] Step 14630/19460 | Loss: 0.8057 | Acc: 0.8750 | LR: 3.13e-06
2026-01-07 21:12:33,069 [INFO] Step 14640/19460 | Loss: 0.5610 | Acc: 1.0000 | LR: 3.12e-06
2026-01-07 21:12:36,403 [INFO] Step 14650/19460 | Loss: 0.7273 | Acc: 0.8125 | LR: 3.11e-06
2026-01-07 21:12:39,610 [INFO] Step 14660/19460 | Loss: 0.6252 | Acc: 0.8750 | LR: 3.10e-06
2026-01-07 21:12:42,745 [INFO] Step 14670/19460 | Loss: 0.5842 | Acc: 0.9375 | LR: 3.09e-06
2026-01-07 21:12:46,185 [INFO] Step 14680/19460 | Loss: 1.0718 | Acc: 0.6875 | LR: 3.08e-06
2026-01-07 21:12:49,500 [INFO] Step 14690/19460 | Loss: 0.7213 | Acc: 0.8750 | LR: 3.08e-06
2026-01-07 21:12:53,414 [INFO] Step 14700/19460 | Loss: 0.5865 | Acc: 0.9375 | LR: 3.07e-06
2026-01-07 21:13:02,135 [INFO] [EVAL] Step 14700 | Val Loss: 0.3182 | Val Acc: 0.9156
2026-01-07 21:13:05,452 [INFO] Step 14710/19460 | Loss: 0.7767 | Acc: 0.8750 | LR: 3.06e-06
2026-01-07 21:13:08,650 [INFO] Step 14720/19460 | Loss: 0.5360 | Acc: 0.9375 | LR: 3.05e-06
2026-01-07 21:13:11,888 [INFO] Step 14730/19460 | Loss: 0.4376 | Acc: 1.0000 | LR: 3.04e-06
2026-01-07 21:13:12,383 [WARNING] Skipping batch due to non-finite loss at step=14731 (loss=nan, epoch=16).
2026-01-07 21:13:15,342 [INFO] Step 14740/19460 | Loss: 0.5896 | Acc: 0.9375 | LR: 3.03e-06
2026-01-07 21:13:18,691 [INFO] Step 14750/19460 | Loss: 0.7517 | Acc: 0.8750 | LR: 3.03e-06
2026-01-07 21:13:22,093 [INFO] Step 14760/19460 | Loss: 1.0519 | Acc: 0.8125 | LR: 3.02e-06
2026-01-07 21:13:25,512 [INFO] Step 14770/19460 | Loss: 0.4699 | Acc: 1.0000 | LR: 3.01e-06
2026-01-07 21:13:28,806 [INFO] Step 14780/19460 | Loss: 0.8600 | Acc: 0.8750 | LR: 3.00e-06
2026-01-07 21:13:31,927 [INFO] Step 14790/19460 | Loss: 0.7129 | Acc: 0.8750 | LR: 2.99e-06
2026-01-07 21:13:35,838 [INFO] Step 14800/19460 | Loss: 0.8263 | Acc: 0.8125 | LR: 2.99e-06
2026-01-07 21:13:44,529 [INFO] [EVAL] Step 14800 | Val Loss: 0.3226 | Val Acc: 0.9106
2026-01-07 21:13:47,971 [INFO] Step 14810/19460 | Loss: 0.7606 | Acc: 0.8125 | LR: 2.98e-06
2026-01-07 21:13:51,388 [INFO] Step 14820/19460 | Loss: 0.7909 | Acc: 0.8750 | LR: 2.97e-06
2026-01-07 21:13:54,623 [INFO] Step 14830/19460 | Loss: 0.4429 | Acc: 1.0000 | LR: 2.96e-06
2026-01-07 21:13:57,782 [INFO] Step 14840/19460 | Loss: 0.5230 | Acc: 0.9375 | LR: 2.95e-06
2026-01-07 21:14:01,074 [INFO] Step 14850/19460 | Loss: 0.7481 | Acc: 0.8750 | LR: 2.95e-06
2026-01-07 21:14:04,403 [INFO] Step 14860/19460 | Loss: 0.6662 | Acc: 0.8750 | LR: 2.94e-06
2026-01-07 21:14:05,306 [WARNING] Skipping batch due to non-finite loss at step=14862 (loss=nan, epoch=16).
2026-01-07 21:14:05,843 [WARNING] Skipping batch due to non-finite loss at step=14863 (loss=nan, epoch=16).
2026-01-07 21:14:08,184 [INFO] Step 14870/19460 | Loss: 0.9627 | Acc: 0.6875 | LR: 2.93e-06
2026-01-07 21:14:11,440 [INFO] Step 14880/19460 | Loss: 0.5063 | Acc: 0.9375 | LR: 2.92e-06
2026-01-07 21:14:14,654 [INFO] Step 14890/19460 | Loss: 0.9270 | Acc: 0.7500 | LR: 2.91e-06
2026-01-07 21:14:18,572 [INFO] Step 14900/19460 | Loss: 0.8381 | Acc: 0.8125 | LR: 2.91e-06
2026-01-07 21:14:27,316 [INFO] [EVAL] Step 14900 | Val Loss: 0.3176 | Val Acc: 0.9156
2026-01-07 21:14:30,713 [INFO] Step 14910/19460 | Loss: 0.5953 | Acc: 0.8750 | LR: 2.90e-06
2026-01-07 21:14:34,009 [INFO] Step 14920/19460 | Loss: 0.5143 | Acc: 0.9375 | LR: 2.89e-06
2026-01-07 21:14:35,994 [WARNING] Skipping batch due to non-finite loss at step=14926 (loss=nan, epoch=16).
2026-01-07 21:14:37,297 [INFO] Step 14930/19460 | Loss: 0.6890 | Acc: 0.8750 | LR: 2.88e-06
2026-01-07 21:14:37,493 [WARNING] Skipping batch due to non-finite loss at step=14930 (loss=nan, epoch=16).
2026-01-07 21:14:40,837 [INFO] Step 14940/19460 | Loss: 0.6595 | Acc: 0.8125 | LR: 2.87e-06
2026-01-07 21:14:44,069 [INFO] Step 14950/19460 | Loss: 0.7544 | Acc: 0.8125 | LR: 2.87e-06
2026-01-07 21:14:47,441 [INFO] Step 14960/19460 | Loss: 0.5526 | Acc: 0.9375 | LR: 2.86e-06
2026-01-07 21:14:50,818 [INFO] Step 14970/19460 | Loss: 0.4932 | Acc: 1.0000 | LR: 2.85e-06
2026-01-07 21:14:54,102 [INFO] Step 14980/19460 | Loss: 0.8374 | Acc: 0.8125 | LR: 2.84e-06
2026-01-07 21:14:57,236 [INFO] Step 14990/19460 | Loss: 0.6755 | Acc: 0.8750 | LR: 2.83e-06
2026-01-07 21:15:01,146 [INFO] Step 15000/19460 | Loss: 1.0814 | Acc: 0.6875 | LR: 2.83e-06
2026-01-07 21:15:09,933 [INFO] [EVAL] Step 15000 | Val Loss: 0.3200 | Val Acc: 0.9137
2026-01-07 21:15:09,964 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_15000.npz
2026-01-07 21:15:13,271 [INFO] Step 15010/19460 | Loss: 0.4561 | Acc: 1.0000 | LR: 2.82e-06
2026-01-07 21:15:16,611 [INFO] Step 15020/19460 | Loss: 0.5707 | Acc: 1.0000 | LR: 2.81e-06
2026-01-07 21:15:20,023 [INFO] Step 15030/19460 | Loss: 0.8945 | Acc: 0.8125 | LR: 2.80e-06
2026-01-07 21:15:22,561 [WARNING] Skipping batch due to non-finite loss at step=15037 (loss=nan, epoch=16).
2026-01-07 21:15:23,662 [INFO] Step 15040/19460 | Loss: 0.8587 | Acc: 0.8125 | LR: 2.80e-06
2026-01-07 21:15:27,086 [INFO] Step 15050/19460 | Loss: 0.7421 | Acc: 0.8125 | LR: 2.79e-06
2026-01-07 21:15:30,315 [INFO] Step 15060/19460 | Loss: 0.8148 | Acc: 0.8125 | LR: 2.78e-06
2026-01-07 21:15:33,596 [INFO] Step 15070/19460 | Loss: 0.5196 | Acc: 1.0000 | LR: 2.77e-06
2026-01-07 21:15:36,037 [WARNING] Skipping batch due to non-finite loss at step=15077 (loss=nan, epoch=16).
2026-01-07 21:15:36,968 [INFO] Step 15080/19460 | Loss: 1.0154 | Acc: 0.6875 | LR: 2.76e-06
2026-01-07 21:15:40,296 [INFO] Step 15090/19460 | Loss: 0.8245 | Acc: 0.8125 | LR: 2.76e-06
2026-01-07 21:15:44,241 [INFO] Step 15100/19460 | Loss: 0.5761 | Acc: 0.8750 | LR: 2.75e-06
2026-01-07 21:15:53,045 [INFO] [EVAL] Step 15100 | Val Loss: 0.3180 | Val Acc: 0.9156
2026-01-07 21:15:56,439 [INFO] Step 15110/19460 | Loss: 0.7924 | Acc: 0.8750 | LR: 2.74e-06
2026-01-07 21:15:59,628 [INFO] Step 15120/19460 | Loss: 0.7723 | Acc: 0.8125 | LR: 2.73e-06
2026-01-07 21:16:02,841 [INFO] Step 15130/19460 | Loss: 0.8195 | Acc: 0.7500 | LR: 2.73e-06
2026-01-07 21:16:06,150 [INFO] Step 15140/19460 | Loss: 0.6299 | Acc: 0.8750 | LR: 2.72e-06
2026-01-07 21:16:09,493 [INFO] Step 15150/19460 | Loss: 0.8039 | Acc: 0.8750 | LR: 2.71e-06
2026-01-07 21:16:13,009 [INFO] Step 15160/19460 | Loss: 0.7537 | Acc: 0.8125 | LR: 2.70e-06
2026-01-07 21:16:16,415 [INFO] Step 15170/19460 | Loss: 0.7063 | Acc: 0.8125 | LR: 2.70e-06
2026-01-07 21:16:17,520 [WARNING] Skipping batch due to non-finite loss at step=15173 (loss=nan, epoch=16).
2026-01-07 21:16:19,801 [INFO] Step 15180/19460 | Loss: 0.5239 | Acc: 0.9375 | LR: 2.69e-06
2026-01-07 21:16:23,302 [INFO] Step 15190/19460 | Loss: 0.8667 | Acc: 0.8125 | LR: 2.68e-06
2026-01-07 21:16:27,279 [INFO] Step 15200/19460 | Loss: 1.0442 | Acc: 0.6875 | LR: 2.67e-06
2026-01-07 21:16:36,027 [INFO] [EVAL] Step 15200 | Val Loss: 0.3250 | Val Acc: 0.9137
2026-01-07 21:16:39,288 [INFO] Step 15210/19460 | Loss: 0.7653 | Acc: 0.7500 | LR: 2.67e-06
2026-01-07 21:16:42,717 [INFO] Step 15220/19460 | Loss: 0.6512 | Acc: 0.8750 | LR: 2.66e-06
2026-01-07 21:16:46,080 [INFO] Step 15230/19460 | Loss: 0.7730 | Acc: 0.8125 | LR: 2.65e-06
2026-01-07 21:16:49,426 [INFO] Step 15240/19460 | Loss: 0.9281 | Acc: 0.7500 | LR: 2.64e-06
2026-01-07 21:16:52,781 [INFO] Step 15250/19460 | Loss: 0.5108 | Acc: 0.9375 | LR: 2.64e-06
2026-01-07 21:16:56,054 [INFO] Step 15260/19460 | Loss: 0.7920 | Acc: 0.7500 | LR: 2.63e-06
2026-01-07 21:16:59,281 [INFO] Step 15270/19460 | Loss: 0.4535 | Acc: 1.0000 | LR: 2.62e-06
2026-01-07 21:17:02,708 [INFO] Step 15280/19460 | Loss: 0.6041 | Acc: 0.9375 | LR: 2.61e-06
2026-01-07 21:17:05,915 [INFO] Step 15290/19460 | Loss: 0.9535 | Acc: 0.7500 | LR: 2.61e-06
2026-01-07 21:17:09,873 [INFO] Step 15300/19460 | Loss: 0.5744 | Acc: 0.9375 | LR: 2.60e-06
2026-01-07 21:17:18,614 [INFO] [EVAL] Step 15300 | Val Loss: 0.3215 | Val Acc: 0.9175
2026-01-07 21:17:21,908 [INFO] Step 15310/19460 | Loss: 1.0054 | Acc: 0.7500 | LR: 2.59e-06
2026-01-07 21:17:25,181 [INFO] Step 15320/19460 | Loss: 0.9984 | Acc: 0.6875 | LR: 2.58e-06
2026-01-07 21:17:28,539 [INFO] Step 15330/19460 | Loss: 0.9393 | Acc: 0.7500 | LR: 2.58e-06
2026-01-07 21:17:31,845 [INFO] Step 15340/19460 | Loss: 0.6455 | Acc: 0.8750 | LR: 2.57e-06
2026-01-07 21:17:35,222 [INFO] Step 15350/19460 | Loss: 0.8273 | Acc: 0.6875 | LR: 2.56e-06
2026-01-07 21:17:38,517 [INFO] Step 15360/19460 | Loss: 0.8423 | Acc: 0.7500 | LR: 2.55e-06
2026-01-07 21:17:42,003 [INFO] Step 15370/19460 | Loss: 0.6825 | Acc: 0.8125 | LR: 2.55e-06
2026-01-07 21:17:45,349 [INFO] Step 15380/19460 | Loss: 0.6216 | Acc: 0.9375 | LR: 2.54e-06
2026-01-07 21:17:48,552 [INFO] Step 15390/19460 | Loss: 0.6583 | Acc: 0.8750 | LR: 2.53e-06
2026-01-07 21:17:52,392 [INFO] Step 15400/19460 | Loss: 0.5308 | Acc: 0.9375 | LR: 2.53e-06
2026-01-07 21:18:01,184 [INFO] [EVAL] Step 15400 | Val Loss: 0.3213 | Val Acc: 0.9163
2026-01-07 21:18:04,468 [INFO] Step 15410/19460 | Loss: 1.1313 | Acc: 0.5625 | LR: 2.52e-06
2026-01-07 21:18:07,837 [INFO] Step 15420/19460 | Loss: 0.7141 | Acc: 0.8750 | LR: 2.51e-06
2026-01-07 21:18:11,026 [INFO] Step 15430/19460 | Loss: 0.5549 | Acc: 0.9375 | LR: 2.50e-06
2026-01-07 21:18:14,366 [INFO] Step 15440/19460 | Loss: 0.8431 | Acc: 0.8125 | LR: 2.50e-06
2026-01-07 21:18:17,782 [INFO] Step 15450/19460 | Loss: 1.0927 | Acc: 0.6875 | LR: 2.49e-06
2026-01-07 21:18:21,210 [INFO] Step 15460/19460 | Loss: 0.7468 | Acc: 0.8750 | LR: 2.48e-06
2026-01-07 21:18:24,438 [INFO] Step 15470/19460 | Loss: 0.8153 | Acc: 0.8125 | LR: 2.48e-06
2026-01-07 21:18:24,802 [INFO] Epoch 16 complete | Avg Loss: 0.7171 | Avg Acc: 0.8599 | Updates: 963 | Micro-batches: 971 | Skipped: 8 (loss=8, logits=0, grads=0)
2026-01-07 21:18:24,802 [INFO] Epoch 17/20
2026-01-07 21:18:27,841 [INFO] Step 15480/19460 | Loss: 0.6149 | Acc: 0.9375 | LR: 2.47e-06
2026-01-07 21:18:31,142 [INFO] Step 15490/19460 | Loss: 0.6492 | Acc: 0.9375 | LR: 2.46e-06
2026-01-07 21:18:35,179 [INFO] Step 15500/19460 | Loss: 1.1271 | Acc: 0.6875 | LR: 2.45e-06
2026-01-07 21:18:43,891 [INFO] [EVAL] Step 15500 | Val Loss: 0.3165 | Val Acc: 0.9169
2026-01-07 21:18:43,925 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_15500.npz
2026-01-07 21:18:47,131 [INFO] Step 15510/19460 | Loss: 0.7112 | Acc: 0.8750 | LR: 2.45e-06
2026-01-07 21:18:50,385 [INFO] Step 15520/19460 | Loss: 0.7644 | Acc: 0.8750 | LR: 2.44e-06
2026-01-07 21:18:53,732 [INFO] Step 15530/19460 | Loss: 0.6416 | Acc: 0.9375 | LR: 2.43e-06
2026-01-07 21:18:56,928 [INFO] Step 15540/19460 | Loss: 0.8377 | Acc: 0.8750 | LR: 2.43e-06
2026-01-07 21:19:00,389 [INFO] Step 15550/19460 | Loss: 0.8793 | Acc: 0.6875 | LR: 2.42e-06
2026-01-07 21:19:03,668 [INFO] Step 15560/19460 | Loss: 0.5224 | Acc: 1.0000 | LR: 2.41e-06
2026-01-07 21:19:07,048 [INFO] Step 15570/19460 | Loss: 0.5513 | Acc: 0.9375 | LR: 2.41e-06
2026-01-07 21:19:10,386 [INFO] Step 15580/19460 | Loss: 0.6875 | Acc: 0.8750 | LR: 2.40e-06
2026-01-07 21:19:13,705 [INFO] Step 15590/19460 | Loss: 1.2370 | Acc: 0.5000 | LR: 2.39e-06
2026-01-07 21:19:17,605 [INFO] Step 15600/19460 | Loss: 0.6735 | Acc: 0.9375 | LR: 2.38e-06
2026-01-07 21:19:26,310 [INFO] [EVAL] Step 15600 | Val Loss: 0.3159 | Val Acc: 0.9150
2026-01-07 21:19:29,829 [INFO] Step 15610/19460 | Loss: 0.5822 | Acc: 0.9375 | LR: 2.38e-06
2026-01-07 21:19:33,220 [INFO] Step 15620/19460 | Loss: 0.9359 | Acc: 0.7500 | LR: 2.37e-06
2026-01-07 21:19:36,442 [INFO] Step 15630/19460 | Loss: 0.6937 | Acc: 0.8750 | LR: 2.36e-06
2026-01-07 21:19:39,671 [INFO] Step 15640/19460 | Loss: 0.6450 | Acc: 0.9375 | LR: 2.36e-06
2026-01-07 21:19:43,002 [INFO] Step 15650/19460 | Loss: 0.5147 | Acc: 1.0000 | LR: 2.35e-06
2026-01-07 21:19:46,261 [INFO] Step 15660/19460 | Loss: 0.8345 | Acc: 0.8125 | LR: 2.34e-06
2026-01-07 21:19:49,482 [INFO] Step 15670/19460 | Loss: 0.6624 | Acc: 0.9375 | LR: 2.34e-06
2026-01-07 21:19:52,745 [INFO] Step 15680/19460 | Loss: 0.6017 | Acc: 0.9375 | LR: 2.33e-06
2026-01-07 21:19:54,637 [WARNING] Skipping batch due to non-finite loss at step=15685 (loss=nan, epoch=17).
2026-01-07 21:19:56,435 [INFO] Step 15690/19460 | Loss: 0.7209 | Acc: 0.8750 | LR: 2.32e-06
2026-01-07 21:20:00,524 [INFO] Step 15700/19460 | Loss: 0.7234 | Acc: 0.8750 | LR: 2.32e-06
2026-01-07 21:20:09,308 [INFO] [EVAL] Step 15700 | Val Loss: 0.3157 | Val Acc: 0.9175
2026-01-07 21:20:12,619 [INFO] Step 15710/19460 | Loss: 0.7224 | Acc: 0.8125 | LR: 2.31e-06
2026-01-07 21:20:15,936 [INFO] Step 15720/19460 | Loss: 0.7721 | Acc: 0.8750 | LR: 2.30e-06
2026-01-07 21:20:19,107 [INFO] Step 15730/19460 | Loss: 0.6046 | Acc: 0.8750 | LR: 2.30e-06
2026-01-07 21:20:22,411 [INFO] Step 15740/19460 | Loss: 0.5809 | Acc: 0.9375 | LR: 2.29e-06
2026-01-07 21:20:25,747 [INFO] Step 15750/19460 | Loss: 1.0430 | Acc: 0.7500 | LR: 2.28e-06
2026-01-07 21:20:28,920 [INFO] Step 15760/19460 | Loss: 0.6905 | Acc: 0.8750 | LR: 2.28e-06
2026-01-07 21:20:32,422 [INFO] Step 15770/19460 | Loss: 0.6305 | Acc: 0.8750 | LR: 2.27e-06
2026-01-07 21:20:35,845 [INFO] Step 15780/19460 | Loss: 0.7774 | Acc: 0.8125 | LR: 2.26e-06
2026-01-07 21:20:39,282 [INFO] Step 15790/19460 | Loss: 1.0598 | Acc: 0.6875 | LR: 2.26e-06
2026-01-07 21:20:43,170 [INFO] Step 15800/19460 | Loss: 0.5058 | Acc: 0.9375 | LR: 2.25e-06
2026-01-07 21:20:51,948 [INFO] [EVAL] Step 15800 | Val Loss: 0.3227 | Val Acc: 0.9144
2026-01-07 21:20:55,536 [INFO] Step 15810/19460 | Loss: 0.6273 | Acc: 0.8750 | LR: 2.24e-06
2026-01-07 21:20:58,867 [INFO] Step 15820/19460 | Loss: 0.9258 | Acc: 0.7500 | LR: 2.24e-06
2026-01-07 21:21:02,152 [INFO] Step 15830/19460 | Loss: 0.8889 | Acc: 0.6875 | LR: 2.23e-06
2026-01-07 21:21:05,498 [INFO] Step 15840/19460 | Loss: 0.5186 | Acc: 1.0000 | LR: 2.22e-06
2026-01-07 21:21:08,703 [INFO] Step 15850/19460 | Loss: 0.6559 | Acc: 0.8750 | LR: 2.22e-06
2026-01-07 21:21:12,064 [INFO] Step 15860/19460 | Loss: 0.7421 | Acc: 0.7500 | LR: 2.21e-06
2026-01-07 21:21:15,417 [INFO] Step 15870/19460 | Loss: 0.7910 | Acc: 0.8125 | LR: 2.20e-06
2026-01-07 21:21:18,706 [INFO] Step 15880/19460 | Loss: 0.6994 | Acc: 0.8750 | LR: 2.20e-06
2026-01-07 21:21:22,019 [INFO] Step 15890/19460 | Loss: 0.6919 | Acc: 0.8750 | LR: 2.19e-06
2026-01-07 21:21:26,050 [INFO] Step 15900/19460 | Loss: 0.5920 | Acc: 0.8750 | LR: 2.18e-06
2026-01-07 21:21:34,820 [INFO] [EVAL] Step 15900 | Val Loss: 0.3215 | Val Acc: 0.9106
2026-01-07 21:21:38,112 [INFO] Step 15910/19460 | Loss: 0.6456 | Acc: 0.8125 | LR: 2.18e-06
2026-01-07 21:21:41,422 [INFO] Step 15920/19460 | Loss: 0.7112 | Acc: 0.8750 | LR: 2.17e-06
2026-01-07 21:21:44,783 [INFO] Step 15930/19460 | Loss: 0.8079 | Acc: 0.8125 | LR: 2.16e-06
2026-01-07 21:21:47,999 [INFO] Step 15940/19460 | Loss: 0.6527 | Acc: 0.8125 | LR: 2.16e-06
2026-01-07 21:21:51,355 [INFO] Step 15950/19460 | Loss: 0.5567 | Acc: 0.9375 | LR: 2.15e-06
2026-01-07 21:21:54,549 [INFO] Step 15960/19460 | Loss: 0.7635 | Acc: 0.8750 | LR: 2.15e-06
2026-01-07 21:21:57,840 [INFO] Step 15970/19460 | Loss: 0.8187 | Acc: 0.8125 | LR: 2.14e-06
2026-01-07 21:22:01,252 [INFO] Step 15980/19460 | Loss: 0.5492 | Acc: 1.0000 | LR: 2.13e-06
2026-01-07 21:22:04,658 [INFO] Step 15990/19460 | Loss: 0.9096 | Acc: 0.7500 | LR: 2.13e-06
2026-01-07 21:22:08,540 [INFO] Step 16000/19460 | Loss: 0.8985 | Acc: 0.7500 | LR: 2.12e-06
2026-01-07 21:22:17,265 [INFO] [EVAL] Step 16000 | Val Loss: 0.3207 | Val Acc: 0.9119
2026-01-07 21:22:17,293 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_16000.npz
2026-01-07 21:22:20,544 [INFO] Step 16010/19460 | Loss: 0.5823 | Acc: 0.9375 | LR: 2.11e-06
2026-01-07 21:22:23,689 [INFO] Step 16020/19460 | Loss: 0.7381 | Acc: 0.8750 | LR: 2.11e-06
2026-01-07 21:22:26,917 [INFO] Step 16030/19460 | Loss: 0.7198 | Acc: 0.9375 | LR: 2.10e-06
2026-01-07 21:22:30,202 [INFO] Step 16040/19460 | Loss: 0.6024 | Acc: 0.8750 | LR: 2.09e-06
2026-01-07 21:22:33,549 [INFO] Step 16050/19460 | Loss: 0.9855 | Acc: 0.6875 | LR: 2.09e-06
2026-01-07 21:22:36,882 [INFO] Step 16060/19460 | Loss: 0.5639 | Acc: 0.9375 | LR: 2.08e-06
2026-01-07 21:22:40,309 [INFO] Step 16070/19460 | Loss: 0.7981 | Acc: 0.8750 | LR: 2.08e-06
2026-01-07 21:22:43,534 [INFO] Step 16080/19460 | Loss: 0.5577 | Acc: 0.9375 | LR: 2.07e-06
2026-01-07 21:22:46,895 [INFO] Step 16090/19460 | Loss: 0.6607 | Acc: 0.9375 | LR: 2.06e-06
2026-01-07 21:22:47,730 [WARNING] Skipping batch due to non-finite loss at step=16092 (loss=nan, epoch=17).
2026-01-07 21:22:50,913 [INFO] Step 16100/19460 | Loss: 0.5983 | Acc: 0.9375 | LR: 2.06e-06
2026-01-07 21:22:59,705 [INFO] [EVAL] Step 16100 | Val Loss: 0.3221 | Val Acc: 0.9137
2026-01-07 21:23:03,175 [INFO] Step 16110/19460 | Loss: 0.7958 | Acc: 0.8125 | LR: 2.05e-06
2026-01-07 21:23:06,495 [INFO] Step 16120/19460 | Loss: 0.7696 | Acc: 0.8750 | LR: 2.05e-06
2026-01-07 21:23:09,897 [INFO] Step 16130/19460 | Loss: 0.5152 | Acc: 0.9375 | LR: 2.04e-06
2026-01-07 21:23:12,007 [WARNING] Skipping batch due to non-finite loss at step=16136 (loss=nan, epoch=17).
2026-01-07 21:23:13,225 [INFO] Step 16140/19460 | Loss: 0.8513 | Acc: 0.7500 | LR: 2.03e-06
2026-01-07 21:23:16,487 [INFO] Step 16150/19460 | Loss: 0.6185 | Acc: 0.9375 | LR: 2.03e-06
2026-01-07 21:23:19,881 [INFO] Step 16160/19460 | Loss: 0.7646 | Acc: 0.8125 | LR: 2.02e-06
2026-01-07 21:23:23,269 [INFO] Step 16170/19460 | Loss: 0.8354 | Acc: 0.8125 | LR: 2.02e-06
2026-01-07 21:23:26,484 [INFO] Step 16180/19460 | Loss: 0.5567 | Acc: 0.9375 | LR: 2.01e-06
2026-01-07 21:23:29,898 [INFO] Step 16190/19460 | Loss: 0.7355 | Acc: 0.8125 | LR: 2.00e-06
2026-01-07 21:23:33,819 [INFO] Step 16200/19460 | Loss: 0.7290 | Acc: 0.8125 | LR: 2.00e-06
2026-01-07 21:23:42,658 [INFO] [EVAL] Step 16200 | Val Loss: 0.3173 | Val Acc: 0.9144
2026-01-07 21:23:46,078 [INFO] Step 16210/19460 | Loss: 0.9161 | Acc: 0.8125 | LR: 1.99e-06
2026-01-07 21:23:49,284 [INFO] Step 16220/19460 | Loss: 0.6611 | Acc: 0.8750 | LR: 1.99e-06
2026-01-07 21:23:52,676 [INFO] Step 16230/19460 | Loss: 0.5643 | Acc: 0.9375 | LR: 1.98e-06
2026-01-07 21:23:56,014 [INFO] Step 16240/19460 | Loss: 0.6265 | Acc: 0.9375 | LR: 1.97e-06
2026-01-07 21:23:59,331 [INFO] Step 16250/19460 | Loss: 0.9845 | Acc: 0.6250 | LR: 1.97e-06
2026-01-07 21:24:02,759 [INFO] Step 16260/19460 | Loss: 0.4541 | Acc: 1.0000 | LR: 1.96e-06
2026-01-07 21:24:06,011 [INFO] Step 16270/19460 | Loss: 0.4982 | Acc: 1.0000 | LR: 1.96e-06
2026-01-07 21:24:09,353 [INFO] Step 16280/19460 | Loss: 0.8188 | Acc: 0.7500 | LR: 1.95e-06
2026-01-07 21:24:12,765 [INFO] Step 16290/19460 | Loss: 1.0195 | Acc: 0.8125 | LR: 1.94e-06
2026-01-07 21:24:16,571 [INFO] Step 16300/19460 | Loss: 0.4738 | Acc: 1.0000 | LR: 1.94e-06
2026-01-07 21:24:25,272 [INFO] [EVAL] Step 16300 | Val Loss: 0.3194 | Val Acc: 0.9131
2026-01-07 21:24:28,605 [INFO] Step 16310/19460 | Loss: 0.8815 | Acc: 0.8125 | LR: 1.93e-06
2026-01-07 21:24:31,797 [INFO] Step 16320/19460 | Loss: 0.5183 | Acc: 1.0000 | LR: 1.93e-06
2026-01-07 21:24:34,945 [INFO] Step 16330/19460 | Loss: 0.7219 | Acc: 0.8750 | LR: 1.92e-06
2026-01-07 21:24:38,338 [INFO] Step 16340/19460 | Loss: 0.6025 | Acc: 0.8750 | LR: 1.92e-06
2026-01-07 21:24:41,749 [INFO] Step 16350/19460 | Loss: 0.6915 | Acc: 0.8750 | LR: 1.91e-06
2026-01-07 21:24:45,081 [INFO] Step 16360/19460 | Loss: 0.9357 | Acc: 0.7500 | LR: 1.90e-06
2026-01-07 21:24:48,395 [INFO] Step 16370/19460 | Loss: 0.8476 | Acc: 0.8125 | LR: 1.90e-06
2026-01-07 21:24:51,740 [INFO] Step 16380/19460 | Loss: 0.6363 | Acc: 0.8750 | LR: 1.89e-06
2026-01-07 21:24:55,130 [INFO] Step 16390/19460 | Loss: 0.6707 | Acc: 0.9375 | LR: 1.89e-06
2026-01-07 21:24:58,952 [INFO] Step 16400/19460 | Loss: 0.7644 | Acc: 0.8125 | LR: 1.88e-06
2026-01-07 21:25:07,731 [INFO] [EVAL] Step 16400 | Val Loss: 0.3149 | Val Acc: 0.9163
2026-01-07 21:25:11,192 [INFO] Step 16410/19460 | Loss: 0.9225 | Acc: 0.7500 | LR: 1.88e-06
2026-01-07 21:25:14,622 [INFO] Step 16420/19460 | Loss: 0.8111 | Acc: 0.7500 | LR: 1.87e-06
2026-01-07 21:25:17,995 [INFO] Step 16430/19460 | Loss: 0.6481 | Acc: 0.8750 | LR: 1.86e-06
2026-01-07 21:25:20,954 [INFO] Epoch 17 complete | Avg Loss: 0.7083 | Avg Acc: 0.8634 | Updates: 968 | Micro-batches: 971 | Skipped: 3 (loss=3, logits=0, grads=0)
2026-01-07 21:25:20,954 [INFO] Epoch 18/20
2026-01-07 21:25:21,350 [INFO] Step 16440/19460 | Loss: 1.0941 | Acc: 0.7500 | LR: 1.86e-06
2026-01-07 21:25:24,711 [INFO] Step 16450/19460 | Loss: 0.6431 | Acc: 0.8750 | LR: 1.85e-06
2026-01-07 21:25:27,923 [INFO] Step 16460/19460 | Loss: 0.6016 | Acc: 0.8750 | LR: 1.85e-06
2026-01-07 21:25:31,169 [INFO] Step 16470/19460 | Loss: 0.6058 | Acc: 0.9375 | LR: 1.84e-06
2026-01-07 21:25:34,590 [INFO] Step 16480/19460 | Loss: 0.7174 | Acc: 0.8750 | LR: 1.84e-06
2026-01-07 21:25:38,023 [INFO] Step 16490/19460 | Loss: 0.6823 | Acc: 0.8750 | LR: 1.83e-06
2026-01-07 21:25:41,994 [INFO] Step 16500/19460 | Loss: 1.0626 | Acc: 0.8125 | LR: 1.83e-06
2026-01-07 21:25:50,704 [INFO] [EVAL] Step 16500 | Val Loss: 0.3196 | Val Acc: 0.9131
2026-01-07 21:25:50,736 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_16500.npz
2026-01-07 21:25:53,883 [INFO] Step 16510/19460 | Loss: 0.4331 | Acc: 1.0000 | LR: 1.82e-06
2026-01-07 21:25:57,179 [INFO] Step 16520/19460 | Loss: 0.5014 | Acc: 1.0000 | LR: 1.81e-06
2026-01-07 21:26:00,405 [INFO] Step 16530/19460 | Loss: 0.6328 | Acc: 1.0000 | LR: 1.81e-06
2026-01-07 21:26:03,683 [INFO] Step 16540/19460 | Loss: 1.0734 | Acc: 0.7500 | LR: 1.80e-06
2026-01-07 21:26:06,971 [INFO] Step 16550/19460 | Loss: 0.8229 | Acc: 0.8125 | LR: 1.80e-06
2026-01-07 21:26:10,359 [INFO] Step 16560/19460 | Loss: 0.9175 | Acc: 0.7500 | LR: 1.79e-06
2026-01-07 21:26:13,641 [INFO] Step 16570/19460 | Loss: 0.5483 | Acc: 0.9375 | LR: 1.79e-06
2026-01-07 21:26:16,942 [INFO] Step 16580/19460 | Loss: 0.7826 | Acc: 0.8125 | LR: 1.78e-06
2026-01-07 21:26:19,019 [WARNING] Skipping batch due to non-finite loss at step=16586 (loss=nan, epoch=18).
2026-01-07 21:26:20,460 [INFO] Step 16590/19460 | Loss: 0.8975 | Acc: 0.7500 | LR: 1.78e-06
2026-01-07 21:26:24,377 [INFO] Step 16600/19460 | Loss: 0.6281 | Acc: 0.8125 | LR: 1.77e-06
2026-01-07 21:26:33,096 [INFO] [EVAL] Step 16600 | Val Loss: 0.3161 | Val Acc: 0.9187
2026-01-07 21:26:36,447 [INFO] Step 16610/19460 | Loss: 0.8211 | Acc: 0.7500 | LR: 1.77e-06
2026-01-07 21:26:39,967 [INFO] Step 16620/19460 | Loss: 0.7701 | Acc: 0.8125 | LR: 1.76e-06
2026-01-07 21:26:43,210 [INFO] Step 16630/19460 | Loss: 0.8124 | Acc: 0.8125 | LR: 1.76e-06
2026-01-07 21:26:46,432 [INFO] Step 16640/19460 | Loss: 0.6251 | Acc: 0.9375 | LR: 1.75e-06
2026-01-07 21:26:49,760 [INFO] Step 16650/19460 | Loss: 0.5393 | Acc: 0.9375 | LR: 1.75e-06
2026-01-07 21:26:53,052 [INFO] Step 16660/19460 | Loss: 0.7816 | Acc: 0.8750 | LR: 1.74e-06
2026-01-07 21:26:56,464 [INFO] Step 16670/19460 | Loss: 0.6012 | Acc: 0.9375 | LR: 1.74e-06
2026-01-07 21:26:59,813 [INFO] Step 16680/19460 | Loss: 0.8679 | Acc: 0.7500 | LR: 1.73e-06
2026-01-07 21:27:03,090 [INFO] Step 16690/19460 | Loss: 0.8101 | Acc: 0.8125 | LR: 1.72e-06
2026-01-07 21:27:07,104 [INFO] Step 16700/19460 | Loss: 0.4776 | Acc: 0.9375 | LR: 1.72e-06
2026-01-07 21:27:15,922 [INFO] [EVAL] Step 16700 | Val Loss: 0.3190 | Val Acc: 0.9187
2026-01-07 21:27:19,416 [INFO] Step 16710/19460 | Loss: 0.8537 | Acc: 0.8750 | LR: 1.71e-06
2026-01-07 21:27:22,791 [INFO] Step 16720/19460 | Loss: 0.8035 | Acc: 0.7500 | LR: 1.71e-06
2026-01-07 21:27:26,064 [INFO] Step 16730/19460 | Loss: 0.7115 | Acc: 0.8750 | LR: 1.70e-06
2026-01-07 21:27:29,286 [INFO] Step 16740/19460 | Loss: 0.8280 | Acc: 0.8125 | LR: 1.70e-06
2026-01-07 21:27:32,706 [INFO] Step 16750/19460 | Loss: 0.8193 | Acc: 0.7500 | LR: 1.69e-06
2026-01-07 21:27:35,844 [INFO] Step 16760/19460 | Loss: 0.5296 | Acc: 0.9375 | LR: 1.69e-06
2026-01-07 21:27:39,151 [INFO] Step 16770/19460 | Loss: 0.4535 | Acc: 1.0000 | LR: 1.68e-06
2026-01-07 21:27:42,585 [INFO] Step 16780/19460 | Loss: 0.9007 | Acc: 0.8750 | LR: 1.68e-06
2026-01-07 21:27:45,808 [INFO] Step 16790/19460 | Loss: 1.0092 | Acc: 0.6250 | LR: 1.67e-06
2026-01-07 21:27:49,879 [INFO] Step 16800/19460 | Loss: 0.7534 | Acc: 0.7500 | LR: 1.67e-06
2026-01-07 21:27:58,590 [INFO] [EVAL] Step 16800 | Val Loss: 0.3176 | Val Acc: 0.9150
2026-01-07 21:28:01,961 [INFO] Step 16810/19460 | Loss: 0.4944 | Acc: 1.0000 | LR: 1.66e-06
2026-01-07 21:28:05,238 [INFO] Step 16820/19460 | Loss: 0.6840 | Acc: 0.9375 | LR: 1.66e-06
2026-01-07 21:28:08,564 [INFO] Step 16830/19460 | Loss: 0.7666 | Acc: 0.7500 | LR: 1.65e-06
2026-01-07 21:28:11,831 [INFO] Step 16840/19460 | Loss: 0.6048 | Acc: 0.8750 | LR: 1.65e-06
2026-01-07 21:28:15,317 [INFO] Step 16850/19460 | Loss: 0.9173 | Acc: 0.7500 | LR: 1.64e-06
2026-01-07 21:28:18,766 [INFO] Step 16860/19460 | Loss: 0.7314 | Acc: 0.8125 | LR: 1.64e-06
2026-01-07 21:28:22,131 [INFO] Step 16870/19460 | Loss: 0.5842 | Acc: 0.9375 | LR: 1.64e-06
2026-01-07 21:28:25,360 [INFO] Step 16880/19460 | Loss: 0.4974 | Acc: 0.9375 | LR: 1.63e-06
2026-01-07 21:28:28,695 [INFO] Step 16890/19460 | Loss: 0.5213 | Acc: 0.9375 | LR: 1.63e-06
2026-01-07 21:28:32,714 [INFO] Step 16900/19460 | Loss: 0.5241 | Acc: 0.9375 | LR: 1.62e-06
2026-01-07 21:28:41,396 [INFO] [EVAL] Step 16900 | Val Loss: 0.3209 | Val Acc: 0.9163
2026-01-07 21:28:44,770 [INFO] Step 16910/19460 | Loss: 0.8082 | Acc: 0.8125 | LR: 1.62e-06
2026-01-07 21:28:48,097 [INFO] Step 16920/19460 | Loss: 0.7316 | Acc: 0.8125 | LR: 1.61e-06
2026-01-07 21:28:51,112 [INFO] Step 16930/19460 | Loss: 0.5134 | Acc: 1.0000 | LR: 1.61e-06
2026-01-07 21:28:54,399 [INFO] Step 16940/19460 | Loss: 0.7623 | Acc: 0.8750 | LR: 1.60e-06
2026-01-07 21:28:57,671 [INFO] Step 16950/19460 | Loss: 0.4998 | Acc: 1.0000 | LR: 1.60e-06
2026-01-07 21:29:01,036 [INFO] Step 16960/19460 | Loss: 0.7042 | Acc: 0.8750 | LR: 1.59e-06
2026-01-07 21:29:04,328 [INFO] Step 16970/19460 | Loss: 0.5873 | Acc: 0.9375 | LR: 1.59e-06
2026-01-07 21:29:07,670 [INFO] Step 16980/19460 | Loss: 0.6974 | Acc: 0.8750 | LR: 1.58e-06
2026-01-07 21:29:10,970 [INFO] Step 16990/19460 | Loss: 0.6599 | Acc: 0.9375 | LR: 1.58e-06
2026-01-07 21:29:14,940 [INFO] Step 17000/19460 | Loss: 0.6807 | Acc: 0.8750 | LR: 1.57e-06
2026-01-07 21:29:23,644 [INFO] [EVAL] Step 17000 | Val Loss: 0.3146 | Val Acc: 0.9169
2026-01-07 21:29:23,676 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_17000.npz
2026-01-07 21:29:27,003 [INFO] Step 17010/19460 | Loss: 0.6059 | Acc: 0.9375 | LR: 1.57e-06
2026-01-07 21:29:30,378 [INFO] Step 17020/19460 | Loss: 0.8560 | Acc: 0.7500 | LR: 1.56e-06
2026-01-07 21:29:33,685 [INFO] Step 17030/19460 | Loss: 0.4497 | Acc: 1.0000 | LR: 1.56e-06
2026-01-07 21:29:37,016 [INFO] Step 17040/19460 | Loss: 0.5098 | Acc: 0.9375 | LR: 1.56e-06
2026-01-07 21:29:40,350 [INFO] Step 17050/19460 | Loss: 0.5845 | Acc: 0.8750 | LR: 1.55e-06
2026-01-07 21:29:43,601 [INFO] Step 17060/19460 | Loss: 0.6390 | Acc: 0.9375 | LR: 1.55e-06
2026-01-07 21:29:46,902 [INFO] Step 17070/19460 | Loss: 0.7329 | Acc: 0.8750 | LR: 1.54e-06
2026-01-07 21:29:50,207 [INFO] Step 17080/19460 | Loss: 0.5260 | Acc: 0.9375 | LR: 1.54e-06
2026-01-07 21:29:53,599 [INFO] Step 17090/19460 | Loss: 0.6320 | Acc: 0.8750 | LR: 1.53e-06
2026-01-07 21:29:57,780 [INFO] Step 17100/19460 | Loss: 0.5963 | Acc: 0.9375 | LR: 1.53e-06
2026-01-07 21:30:06,549 [INFO] [EVAL] Step 17100 | Val Loss: 0.3140 | Val Acc: 0.9169
2026-01-07 21:30:10,074 [INFO] Step 17110/19460 | Loss: 0.6510 | Acc: 0.9375 | LR: 1.52e-06
2026-01-07 21:30:13,583 [INFO] Step 17120/19460 | Loss: 1.0566 | Acc: 0.7500 | LR: 1.52e-06
2026-01-07 21:30:14,139 [WARNING] Skipping batch due to non-finite loss at step=17121 (loss=nan, epoch=18).
2026-01-07 21:30:17,205 [INFO] Step 17130/19460 | Loss: 0.5050 | Acc: 0.9375 | LR: 1.52e-06
2026-01-07 21:30:20,576 [INFO] Step 17140/19460 | Loss: 0.9218 | Acc: 0.6875 | LR: 1.51e-06
2026-01-07 21:30:24,014 [INFO] Step 17150/19460 | Loss: 0.9719 | Acc: 0.6250 | LR: 1.51e-06
2026-01-07 21:30:27,381 [INFO] Step 17160/19460 | Loss: 0.6309 | Acc: 0.8750 | LR: 1.50e-06
2026-01-07 21:30:27,863 [WARNING] Skipping batch due to non-finite loss at step=17161 (loss=nan, epoch=18).
2026-01-07 21:30:28,654 [WARNING] Skipping batch due to non-finite loss at step=17163 (loss=nan, epoch=18).
2026-01-07 21:30:29,240 [WARNING] Skipping batch due to non-finite loss at step=17164 (loss=nan, epoch=18).
2026-01-07 21:30:31,211 [INFO] Step 17170/19460 | Loss: 0.5668 | Acc: 0.9375 | LR: 1.50e-06
2026-01-07 21:30:34,014 [WARNING] Skipping batch due to non-finite loss at step=17178 (loss=nan, epoch=18).
2026-01-07 21:30:34,652 [INFO] Step 17180/19460 | Loss: 0.7645 | Acc: 0.8750 | LR: 1.49e-06
2026-01-07 21:30:37,985 [INFO] Step 17190/19460 | Loss: 0.7802 | Acc: 0.8125 | LR: 1.49e-06
2026-01-07 21:30:41,954 [INFO] Step 17200/19460 | Loss: 0.6631 | Acc: 0.9375 | LR: 1.49e-06
2026-01-07 21:30:50,765 [INFO] [EVAL] Step 17200 | Val Loss: 0.3151 | Val Acc: 0.9163
2026-01-07 21:30:54,170 [INFO] Step 17210/19460 | Loss: 0.9763 | Acc: 0.7500 | LR: 1.48e-06
2026-01-07 21:30:57,406 [INFO] Step 17220/19460 | Loss: 0.6042 | Acc: 0.8750 | LR: 1.48e-06
2026-01-07 21:31:00,767 [INFO] Step 17230/19460 | Loss: 0.9205 | Acc: 0.7500 | LR: 1.47e-06
2026-01-07 21:31:04,053 [INFO] Step 17240/19460 | Loss: 0.7329 | Acc: 0.7500 | LR: 1.47e-06
2026-01-07 21:31:07,418 [INFO] Step 17250/19460 | Loss: 0.6108 | Acc: 0.8750 | LR: 1.46e-06
2026-01-07 21:31:10,779 [INFO] Step 17260/19460 | Loss: 0.4897 | Acc: 0.9375 | LR: 1.46e-06
2026-01-07 21:31:14,219 [INFO] Step 17270/19460 | Loss: 0.7470 | Acc: 0.8750 | LR: 1.46e-06
2026-01-07 21:31:17,500 [INFO] Step 17280/19460 | Loss: 0.5252 | Acc: 0.9375 | LR: 1.45e-06
2026-01-07 21:31:20,724 [INFO] Step 17290/19460 | Loss: 0.8854 | Acc: 0.8125 | LR: 1.45e-06
2026-01-07 21:31:24,586 [INFO] Step 17300/19460 | Loss: 1.0122 | Acc: 0.7500 | LR: 1.44e-06
2026-01-07 21:31:33,321 [INFO] [EVAL] Step 17300 | Val Loss: 0.3121 | Val Acc: 0.9213
2026-01-07 21:31:33,353 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 21:31:33,354 [INFO] New best validation accuracy: 0.9213
2026-01-07 21:31:36,617 [INFO] Step 17310/19460 | Loss: 1.2454 | Acc: 0.5625 | LR: 1.44e-06
2026-01-07 21:31:39,727 [WARNING] Skipping batch due to non-finite loss at step=17319 (loss=nan, epoch=18).
2026-01-07 21:31:40,052 [INFO] Step 17320/19460 | Loss: 0.6091 | Acc: 0.9375 | LR: 1.44e-06
2026-01-07 21:31:43,571 [INFO] Step 17330/19460 | Loss: 0.6997 | Acc: 0.8750 | LR: 1.43e-06
2026-01-07 21:31:46,929 [INFO] Step 17340/19460 | Loss: 0.6953 | Acc: 0.8750 | LR: 1.43e-06
2026-01-07 21:31:50,299 [INFO] Step 17350/19460 | Loss: 0.6427 | Acc: 0.9375 | LR: 1.42e-06
2026-01-07 21:31:53,699 [INFO] Step 17360/19460 | Loss: 0.4886 | Acc: 1.0000 | LR: 1.42e-06
2026-01-07 21:31:54,554 [WARNING] Skipping batch due to non-finite loss at step=17362 (loss=nan, epoch=18).
2026-01-07 21:31:57,348 [INFO] Step 17370/19460 | Loss: 0.7473 | Acc: 0.8750 | LR: 1.42e-06
2026-01-07 21:32:00,704 [INFO] Step 17380/19460 | Loss: 0.5705 | Acc: 0.9375 | LR: 1.41e-06
2026-01-07 21:32:04,055 [INFO] Step 17390/19460 | Loss: 0.5796 | Acc: 0.9375 | LR: 1.41e-06
2026-01-07 21:32:07,973 [INFO] Step 17400/19460 | Loss: 0.8367 | Acc: 0.8125 | LR: 1.40e-06
2026-01-07 21:32:16,695 [INFO] [EVAL] Step 17400 | Val Loss: 0.3128 | Val Acc: 0.9206
2026-01-07 21:32:17,440 [INFO] Epoch 18 complete | Avg Loss: 0.7035 | Avg Acc: 0.8685 | Updates: 963 | Micro-batches: 971 | Skipped: 8 (loss=8, logits=0, grads=0)
2026-01-07 21:32:17,440 [INFO] Epoch 19/20
2026-01-07 21:32:20,229 [INFO] Step 17410/19460 | Loss: 0.6441 | Acc: 0.8750 | LR: 1.40e-06
2026-01-07 21:32:20,414 [WARNING] Skipping batch due to non-finite loss at step=17410 (loss=nan, epoch=19).
2026-01-07 21:32:23,659 [INFO] Step 17420/19460 | Loss: 0.6782 | Acc: 0.8750 | LR: 1.40e-06
2026-01-07 21:32:27,013 [INFO] Step 17430/19460 | Loss: 1.1881 | Acc: 0.6875 | LR: 1.39e-06
2026-01-07 21:32:30,343 [INFO] Step 17440/19460 | Loss: 0.6221 | Acc: 0.8750 | LR: 1.39e-06
2026-01-07 21:32:33,785 [INFO] Step 17450/19460 | Loss: 0.8474 | Acc: 0.8125 | LR: 1.39e-06
2026-01-07 21:32:36,735 [WARNING] Skipping batch due to non-finite loss at step=17458 (loss=nan, epoch=19).
2026-01-07 21:32:37,368 [INFO] Step 17460/19460 | Loss: 0.5862 | Acc: 0.8750 | LR: 1.38e-06
2026-01-07 21:32:40,728 [INFO] Step 17470/19460 | Loss: 0.6373 | Acc: 0.9375 | LR: 1.38e-06
2026-01-07 21:32:44,217 [INFO] Step 17480/19460 | Loss: 0.5526 | Acc: 1.0000 | LR: 1.37e-06
2026-01-07 21:32:47,663 [INFO] Step 17490/19460 | Loss: 0.8908 | Acc: 0.8125 | LR: 1.37e-06
2026-01-07 21:32:51,614 [INFO] Step 17500/19460 | Loss: 0.5938 | Acc: 0.8750 | LR: 1.37e-06
2026-01-07 21:33:00,369 [INFO] [EVAL] Step 17500 | Val Loss: 0.3117 | Val Acc: 0.9200
2026-01-07 21:33:00,390 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_17500.npz
2026-01-07 21:33:03,717 [INFO] Step 17510/19460 | Loss: 0.8895 | Acc: 0.8125 | LR: 1.36e-06
2026-01-07 21:33:06,947 [INFO] Step 17520/19460 | Loss: 0.8802 | Acc: 0.7500 | LR: 1.36e-06
2026-01-07 21:33:10,425 [INFO] Step 17530/19460 | Loss: 0.5461 | Acc: 0.9375 | LR: 1.36e-06
2026-01-07 21:33:12,231 [WARNING] Skipping batch due to non-finite loss at step=17535 (loss=nan, epoch=19).
2026-01-07 21:33:13,833 [INFO] Step 17540/19460 | Loss: 0.7911 | Acc: 0.8750 | LR: 1.35e-06
2026-01-07 21:33:17,247 [INFO] Step 17550/19460 | Loss: 0.6286 | Acc: 0.9375 | LR: 1.35e-06
2026-01-07 21:33:20,526 [INFO] Step 17560/19460 | Loss: 0.7561 | Acc: 0.8125 | LR: 1.34e-06
2026-01-07 21:33:23,765 [INFO] Step 17570/19460 | Loss: 0.7276 | Acc: 0.8125 | LR: 1.34e-06
2026-01-07 21:33:27,040 [INFO] Step 17580/19460 | Loss: 0.6015 | Acc: 0.9375 | LR: 1.34e-06
2026-01-07 21:33:30,473 [INFO] Step 17590/19460 | Loss: 0.5623 | Acc: 0.9375 | LR: 1.33e-06
2026-01-07 21:33:34,322 [INFO] Step 17600/19460 | Loss: 0.6865 | Acc: 0.9375 | LR: 1.33e-06
2026-01-07 21:33:43,038 [INFO] [EVAL] Step 17600 | Val Loss: 0.3114 | Val Acc: 0.9163
2026-01-07 21:33:46,290 [INFO] Step 17610/19460 | Loss: 0.6396 | Acc: 0.8750 | LR: 1.33e-06
2026-01-07 21:33:49,603 [INFO] Step 17620/19460 | Loss: 0.4297 | Acc: 1.0000 | LR: 1.32e-06
2026-01-07 21:33:52,982 [INFO] Step 17630/19460 | Loss: 0.5970 | Acc: 0.9375 | LR: 1.32e-06
2026-01-07 21:33:56,424 [INFO] Step 17640/19460 | Loss: 0.8094 | Acc: 0.8125 | LR: 1.32e-06
2026-01-07 21:33:59,732 [INFO] Step 17650/19460 | Loss: 1.2422 | Acc: 0.6875 | LR: 1.31e-06
2026-01-07 21:34:03,115 [INFO] Step 17660/19460 | Loss: 0.6560 | Acc: 0.9375 | LR: 1.31e-06
2026-01-07 21:34:05,228 [WARNING] Skipping batch due to non-finite loss at step=17666 (loss=nan, epoch=19).
2026-01-07 21:34:06,556 [INFO] Step 17670/19460 | Loss: 0.7548 | Acc: 0.8750 | LR: 1.31e-06
2026-01-07 21:34:09,994 [INFO] Step 17680/19460 | Loss: 0.9562 | Acc: 0.7500 | LR: 1.30e-06
2026-01-07 21:34:13,237 [INFO] Step 17690/19460 | Loss: 0.5574 | Acc: 1.0000 | LR: 1.30e-06
2026-01-07 21:34:17,060 [INFO] Step 17700/19460 | Loss: 0.4935 | Acc: 1.0000 | LR: 1.30e-06
2026-01-07 21:34:25,785 [INFO] [EVAL] Step 17700 | Val Loss: 0.3145 | Val Acc: 0.9169
2026-01-07 21:34:29,164 [INFO] Step 17710/19460 | Loss: 0.7860 | Acc: 0.8125 | LR: 1.29e-06
2026-01-07 21:34:32,541 [INFO] Step 17720/19460 | Loss: 0.8513 | Acc: 0.8125 | LR: 1.29e-06
2026-01-07 21:34:35,858 [INFO] Step 17730/19460 | Loss: 0.4661 | Acc: 1.0000 | LR: 1.29e-06
2026-01-07 21:34:39,158 [INFO] Step 17740/19460 | Loss: 0.6539 | Acc: 0.8750 | LR: 1.28e-06
2026-01-07 21:34:42,668 [INFO] Step 17750/19460 | Loss: 0.4549 | Acc: 1.0000 | LR: 1.28e-06
2026-01-07 21:34:46,043 [INFO] Step 17760/19460 | Loss: 0.6689 | Acc: 0.8750 | LR: 1.28e-06
2026-01-07 21:34:49,419 [INFO] Step 17770/19460 | Loss: 0.7756 | Acc: 0.8750 | LR: 1.27e-06
2026-01-07 21:34:53,031 [INFO] Step 17780/19460 | Loss: 0.4853 | Acc: 1.0000 | LR: 1.27e-06
2026-01-07 21:34:56,323 [INFO] Step 17790/19460 | Loss: 0.7271 | Acc: 0.8125 | LR: 1.27e-06
2026-01-07 21:35:00,247 [INFO] Step 17800/19460 | Loss: 0.6527 | Acc: 0.8750 | LR: 1.26e-06
2026-01-07 21:35:09,042 [INFO] [EVAL] Step 17800 | Val Loss: 0.3119 | Val Acc: 0.9219
2026-01-07 21:35:09,076 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_best.npz
2026-01-07 21:35:09,078 [INFO] New best validation accuracy: 0.9219
2026-01-07 21:35:12,482 [INFO] Step 17810/19460 | Loss: 0.7364 | Acc: 0.8125 | LR: 1.26e-06
2026-01-07 21:35:15,831 [INFO] Step 17820/19460 | Loss: 0.8414 | Acc: 0.7500 | LR: 1.26e-06
2026-01-07 21:35:16,310 [WARNING] Skipping batch due to non-finite loss at step=17821 (loss=nan, epoch=19).
2026-01-07 21:35:19,406 [INFO] Step 17830/19460 | Loss: 0.4898 | Acc: 0.9375 | LR: 1.25e-06
2026-01-07 21:35:22,664 [INFO] Step 17840/19460 | Loss: 0.9629 | Acc: 0.6875 | LR: 1.25e-06
2026-01-07 21:35:26,000 [INFO] Step 17850/19460 | Loss: 0.7893 | Acc: 0.8125 | LR: 1.25e-06
2026-01-07 21:35:29,218 [INFO] Step 17860/19460 | Loss: 0.7156 | Acc: 0.8125 | LR: 1.24e-06
2026-01-07 21:35:32,732 [INFO] Step 17870/19460 | Loss: 0.4459 | Acc: 1.0000 | LR: 1.24e-06
2026-01-07 21:35:36,089 [INFO] Step 17880/19460 | Loss: 0.5422 | Acc: 0.9375 | LR: 1.24e-06
2026-01-07 21:35:39,475 [INFO] Step 17890/19460 | Loss: 0.4871 | Acc: 1.0000 | LR: 1.24e-06
2026-01-07 21:35:43,566 [INFO] Step 17900/19460 | Loss: 0.6606 | Acc: 0.8750 | LR: 1.23e-06
2026-01-07 21:35:52,387 [INFO] [EVAL] Step 17900 | Val Loss: 0.3173 | Val Acc: 0.9150
2026-01-07 21:35:55,704 [INFO] Step 17910/19460 | Loss: 0.6064 | Acc: 0.8750 | LR: 1.23e-06
2026-01-07 21:35:59,140 [INFO] Step 17920/19460 | Loss: 0.5927 | Acc: 0.8750 | LR: 1.23e-06
2026-01-07 21:36:02,638 [INFO] Step 17930/19460 | Loss: 0.7461 | Acc: 0.8125 | LR: 1.22e-06
2026-01-07 21:36:05,986 [INFO] Step 17940/19460 | Loss: 0.5473 | Acc: 0.9375 | LR: 1.22e-06
2026-01-07 21:36:09,136 [INFO] Step 17950/19460 | Loss: 0.6967 | Acc: 0.8750 | LR: 1.22e-06
2026-01-07 21:36:12,371 [INFO] Step 17960/19460 | Loss: 0.5216 | Acc: 0.9375 | LR: 1.22e-06
2026-01-07 21:36:15,685 [INFO] Step 17970/19460 | Loss: 0.7479 | Acc: 0.8750 | LR: 1.21e-06
2026-01-07 21:36:19,039 [INFO] Step 17980/19460 | Loss: 0.8722 | Acc: 0.7500 | LR: 1.21e-06
2026-01-07 21:36:22,465 [INFO] Step 17990/19460 | Loss: 0.8698 | Acc: 0.7500 | LR: 1.21e-06
2026-01-07 21:36:26,379 [INFO] Step 18000/19460 | Loss: 0.6283 | Acc: 0.9375 | LR: 1.20e-06
2026-01-07 21:36:35,081 [INFO] [EVAL] Step 18000 | Val Loss: 0.3134 | Val Acc: 0.9194
2026-01-07 21:36:35,104 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_18000.npz
2026-01-07 21:36:38,617 [INFO] Step 18010/19460 | Loss: 0.5635 | Acc: 1.0000 | LR: 1.20e-06
2026-01-07 21:36:41,890 [INFO] Step 18020/19460 | Loss: 0.8809 | Acc: 0.7500 | LR: 1.20e-06
2026-01-07 21:36:45,209 [INFO] Step 18030/19460 | Loss: 0.5399 | Acc: 0.9375 | LR: 1.20e-06
2026-01-07 21:36:48,550 [INFO] Step 18040/19460 | Loss: 0.5965 | Acc: 0.9375 | LR: 1.19e-06
2026-01-07 21:36:51,806 [INFO] Step 18050/19460 | Loss: 0.6615 | Acc: 0.9375 | LR: 1.19e-06
2026-01-07 21:36:55,205 [INFO] Step 18060/19460 | Loss: 0.5057 | Acc: 0.9375 | LR: 1.19e-06
2026-01-07 21:36:58,525 [INFO] Step 18070/19460 | Loss: 1.1115 | Acc: 0.6250 | LR: 1.19e-06
2026-01-07 21:37:01,861 [WARNING] Skipping batch due to non-finite loss at step=18079 (loss=nan, epoch=19).
2026-01-07 21:37:02,222 [INFO] Step 18080/19460 | Loss: 0.5137 | Acc: 0.9375 | LR: 1.18e-06
2026-01-07 21:37:05,473 [INFO] Step 18090/19460 | Loss: 0.7119 | Acc: 0.8750 | LR: 1.18e-06
2026-01-07 21:37:09,536 [INFO] Step 18100/19460 | Loss: 0.5012 | Acc: 1.0000 | LR: 1.18e-06
2026-01-07 21:37:18,320 [INFO] [EVAL] Step 18100 | Val Loss: 0.3125 | Val Acc: 0.9187
2026-01-07 21:37:21,707 [INFO] Step 18110/19460 | Loss: 0.7047 | Acc: 0.8750 | LR: 1.17e-06
2026-01-07 21:37:22,508 [WARNING] Skipping batch due to non-finite loss at step=18112 (loss=nan, epoch=19).
2026-01-07 21:37:25,277 [INFO] Step 18120/19460 | Loss: 0.7300 | Acc: 0.7500 | LR: 1.17e-06
2026-01-07 21:37:28,723 [INFO] Step 18130/19460 | Loss: 0.6935 | Acc: 0.8750 | LR: 1.17e-06
2026-01-07 21:37:31,990 [INFO] Step 18140/19460 | Loss: 0.6323 | Acc: 0.8750 | LR: 1.17e-06
2026-01-07 21:37:35,328 [INFO] Step 18150/19460 | Loss: 0.6849 | Acc: 0.8750 | LR: 1.16e-06
2026-01-07 21:37:38,712 [INFO] Step 18160/19460 | Loss: 0.6074 | Acc: 0.8750 | LR: 1.16e-06
2026-01-07 21:37:42,130 [INFO] Step 18170/19460 | Loss: 0.9993 | Acc: 0.7500 | LR: 1.16e-06
2026-01-07 21:37:45,544 [INFO] Step 18180/19460 | Loss: 0.8366 | Acc: 0.8750 | LR: 1.16e-06
2026-01-07 21:37:48,907 [INFO] Step 18190/19460 | Loss: 0.5960 | Acc: 0.9375 | LR: 1.15e-06
2026-01-07 21:37:51,988 [WARNING] Skipping batch due to non-finite loss at step=18199 (loss=nan, epoch=19).
2026-01-07 21:37:52,932 [INFO] Step 18200/19460 | Loss: 0.8145 | Acc: 0.8125 | LR: 1.15e-06
2026-01-07 21:38:01,711 [INFO] [EVAL] Step 18200 | Val Loss: 0.3139 | Val Acc: 0.9175
2026-01-07 21:38:04,973 [INFO] Step 18210/19460 | Loss: 0.7703 | Acc: 0.8750 | LR: 1.15e-06
2026-01-07 21:38:08,250 [INFO] Step 18220/19460 | Loss: 0.6862 | Acc: 0.9375 | LR: 1.15e-06
2026-01-07 21:38:08,749 [WARNING] Skipping batch due to non-finite loss at step=18221 (loss=nan, epoch=19).
2026-01-07 21:38:11,822 [INFO] Step 18230/19460 | Loss: 0.7760 | Acc: 0.8750 | LR: 1.15e-06
2026-01-07 21:38:15,092 [INFO] Step 18240/19460 | Loss: 0.5961 | Acc: 0.9375 | LR: 1.14e-06
2026-01-07 21:38:15,830 [WARNING] Skipping batch due to non-finite loss at step=18242 (loss=nan, epoch=19).
2026-01-07 21:38:18,624 [INFO] Step 18250/19460 | Loss: 0.6885 | Acc: 0.9375 | LR: 1.14e-06
2026-01-07 21:38:22,115 [INFO] Step 18260/19460 | Loss: 0.6648 | Acc: 0.9375 | LR: 1.14e-06
2026-01-07 21:38:25,459 [INFO] Step 18270/19460 | Loss: 0.9016 | Acc: 0.8125 | LR: 1.14e-06
2026-01-07 21:38:28,872 [INFO] Step 18280/19460 | Loss: 0.6492 | Acc: 0.9375 | LR: 1.13e-06
2026-01-07 21:38:32,127 [INFO] Step 18290/19460 | Loss: 0.7421 | Acc: 0.8750 | LR: 1.13e-06
2026-01-07 21:38:36,092 [INFO] Step 18300/19460 | Loss: 0.9490 | Acc: 0.8125 | LR: 1.13e-06
2026-01-07 21:38:44,855 [INFO] [EVAL] Step 18300 | Val Loss: 0.3132 | Val Acc: 0.9169
2026-01-07 21:38:48,102 [INFO] Step 18310/19460 | Loss: 0.7979 | Acc: 0.8125 | LR: 1.13e-06
2026-01-07 21:38:51,368 [INFO] Step 18320/19460 | Loss: 0.6079 | Acc: 0.8750 | LR: 1.12e-06
2026-01-07 21:38:54,775 [INFO] Step 18330/19460 | Loss: 0.6226 | Acc: 0.9375 | LR: 1.12e-06
2026-01-07 21:38:57,988 [INFO] Step 18340/19460 | Loss: 1.0008 | Acc: 0.7500 | LR: 1.12e-06
2026-01-07 21:39:01,315 [INFO] Step 18350/19460 | Loss: 0.6320 | Acc: 0.8750 | LR: 1.12e-06
2026-01-07 21:39:04,721 [INFO] Step 18360/19460 | Loss: 0.5858 | Acc: 0.9375 | LR: 1.12e-06
2026-01-07 21:39:05,710 [INFO] Epoch 19 complete | Avg Loss: 0.7054 | Avg Acc: 0.8669 | Updates: 961 | Micro-batches: 971 | Skipped: 10 (loss=10, logits=0, grads=0)
2026-01-07 21:39:05,710 [INFO] Epoch 20/20
2026-01-07 21:39:07,972 [INFO] Step 18370/19460 | Loss: 0.6171 | Acc: 0.8125 | LR: 1.11e-06
2026-01-07 21:39:11,408 [INFO] Step 18380/19460 | Loss: 0.4582 | Acc: 1.0000 | LR: 1.11e-06
2026-01-07 21:39:14,762 [INFO] Step 18390/19460 | Loss: 0.9103 | Acc: 0.8750 | LR: 1.11e-06
2026-01-07 21:39:18,790 [INFO] Step 18400/19460 | Loss: 0.7245 | Acc: 0.8125 | LR: 1.11e-06
2026-01-07 21:39:27,493 [INFO] [EVAL] Step 18400 | Val Loss: 0.3119 | Val Acc: 0.9206
2026-01-07 21:39:30,603 [WARNING] Skipping batch due to non-finite loss at step=18409 (loss=nan, epoch=20).
2026-01-07 21:39:30,919 [INFO] Step 18410/19460 | Loss: 0.6948 | Acc: 0.8750 | LR: 1.11e-06
2026-01-07 21:39:34,353 [INFO] Step 18420/19460 | Loss: 0.8706 | Acc: 0.8125 | LR: 1.10e-06
2026-01-07 21:39:37,751 [INFO] Step 18430/19460 | Loss: 0.4858 | Acc: 0.9375 | LR: 1.10e-06
2026-01-07 21:39:41,102 [INFO] Step 18440/19460 | Loss: 0.9005 | Acc: 0.8125 | LR: 1.10e-06
2026-01-07 21:39:44,496 [INFO] Step 18450/19460 | Loss: 0.7445 | Acc: 0.8125 | LR: 1.10e-06
2026-01-07 21:39:47,963 [INFO] Step 18460/19460 | Loss: 0.7406 | Acc: 0.8125 | LR: 1.10e-06
2026-01-07 21:39:49,837 [WARNING] Skipping batch due to non-finite loss at step=18465 (loss=nan, epoch=20).
2026-01-07 21:39:51,594 [INFO] Step 18470/19460 | Loss: 0.6867 | Acc: 0.8750 | LR: 1.09e-06
2026-01-07 21:39:55,020 [INFO] Step 18480/19460 | Loss: 0.5046 | Acc: 1.0000 | LR: 1.09e-06
2026-01-07 21:39:58,415 [INFO] Step 18490/19460 | Loss: 0.6009 | Acc: 0.9375 | LR: 1.09e-06
2026-01-07 21:40:02,459 [INFO] Step 18500/19460 | Loss: 0.6649 | Acc: 0.8125 | LR: 1.09e-06
2026-01-07 21:40:11,193 [INFO] [EVAL] Step 18500 | Val Loss: 0.3144 | Val Acc: 0.9194
2026-01-07 21:40:11,228 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_18500.npz
2026-01-07 21:40:14,652 [INFO] Step 18510/19460 | Loss: 0.8514 | Acc: 0.8750 | LR: 1.09e-06
2026-01-07 21:40:16,789 [WARNING] Skipping batch due to non-finite loss at step=18516 (loss=nan, epoch=20).
2026-01-07 21:40:18,122 [INFO] Step 18520/19460 | Loss: 0.7419 | Acc: 0.8125 | LR: 1.08e-06
2026-01-07 21:40:21,491 [INFO] Step 18530/19460 | Loss: 0.7833 | Acc: 0.8125 | LR: 1.08e-06
2026-01-07 21:40:24,879 [INFO] Step 18540/19460 | Loss: 0.6551 | Acc: 0.9375 | LR: 1.08e-06
2026-01-07 21:40:28,248 [INFO] Step 18550/19460 | Loss: 0.7038 | Acc: 0.8750 | LR: 1.08e-06
2026-01-07 21:40:31,536 [INFO] Step 18560/19460 | Loss: 0.5193 | Acc: 0.9375 | LR: 1.08e-06
2026-01-07 21:40:34,738 [INFO] Step 18570/19460 | Loss: 0.7415 | Acc: 0.8750 | LR: 1.08e-06
2026-01-07 21:40:37,735 [WARNING] Skipping batch due to non-finite loss at step=18578 (loss=nan, epoch=20).
2026-01-07 21:40:38,426 [INFO] Step 18580/19460 | Loss: 0.8427 | Acc: 0.8125 | LR: 1.07e-06
2026-01-07 21:40:41,782 [INFO] Step 18590/19460 | Loss: 0.5587 | Acc: 0.8750 | LR: 1.07e-06
2026-01-07 21:40:45,774 [INFO] Step 18600/19460 | Loss: 0.5787 | Acc: 0.9375 | LR: 1.07e-06
2026-01-07 21:40:54,545 [INFO] [EVAL] Step 18600 | Val Loss: 0.3144 | Val Acc: 0.9187
2026-01-07 21:40:57,883 [INFO] Step 18610/19460 | Loss: 0.7735 | Acc: 0.8750 | LR: 1.07e-06
2026-01-07 21:41:00,152 [WARNING] Skipping batch due to non-finite loss at step=18616 (loss=nan, epoch=20).
2026-01-07 21:41:01,492 [INFO] Step 18620/19460 | Loss: 0.8554 | Acc: 0.8750 | LR: 1.07e-06
2026-01-07 21:41:04,821 [INFO] Step 18630/19460 | Loss: 0.8228 | Acc: 0.7500 | LR: 1.07e-06
2026-01-07 21:41:07,629 [WARNING] Skipping batch due to non-finite loss at step=18638 (loss=nan, epoch=20).
2026-01-07 21:41:08,314 [INFO] Step 18640/19460 | Loss: 0.5807 | Acc: 0.8750 | LR: 1.06e-06
2026-01-07 21:41:11,707 [INFO] Step 18650/19460 | Loss: 0.9388 | Acc: 0.8125 | LR: 1.06e-06
2026-01-07 21:41:14,891 [INFO] Step 18660/19460 | Loss: 0.7063 | Acc: 0.8750 | LR: 1.06e-06
2026-01-07 21:41:18,185 [INFO] Step 18670/19460 | Loss: 0.6851 | Acc: 0.8750 | LR: 1.06e-06
2026-01-07 21:41:21,563 [INFO] Step 18680/19460 | Loss: 0.7961 | Acc: 0.8125 | LR: 1.06e-06
2026-01-07 21:41:25,045 [INFO] Step 18690/19460 | Loss: 0.8418 | Acc: 0.8125 | LR: 1.06e-06
2026-01-07 21:41:26,917 [WARNING] Skipping batch due to non-finite loss at step=18695 (loss=nan, epoch=20).
2026-01-07 21:41:29,246 [INFO] Step 18700/19460 | Loss: 0.8788 | Acc: 0.7500 | LR: 1.06e-06
2026-01-07 21:41:37,999 [INFO] [EVAL] Step 18700 | Val Loss: 0.3113 | Val Acc: 0.9187
2026-01-07 21:41:41,285 [INFO] Step 18710/19460 | Loss: 0.6953 | Acc: 0.8125 | LR: 1.05e-06
2026-01-07 21:41:44,781 [INFO] Step 18720/19460 | Loss: 1.0035 | Acc: 0.7500 | LR: 1.05e-06
2026-01-07 21:41:47,939 [INFO] Step 18730/19460 | Loss: 0.5745 | Acc: 0.9375 | LR: 1.05e-06
2026-01-07 21:41:51,321 [INFO] Step 18740/19460 | Loss: 0.9537 | Acc: 0.6875 | LR: 1.05e-06
2026-01-07 21:41:54,692 [INFO] Step 18750/19460 | Loss: 0.9306 | Acc: 0.7500 | LR: 1.05e-06
2026-01-07 21:41:58,088 [INFO] Step 18760/19460 | Loss: 0.6929 | Acc: 0.8750 | LR: 1.05e-06
2026-01-07 21:42:01,359 [INFO] Step 18770/19460 | Loss: 0.7240 | Acc: 0.8750 | LR: 1.05e-06
2026-01-07 21:42:04,810 [INFO] Step 18780/19460 | Loss: 0.8346 | Acc: 0.7500 | LR: 1.04e-06
2026-01-07 21:42:08,043 [INFO] Step 18790/19460 | Loss: 0.9257 | Acc: 0.6875 | LR: 1.04e-06
2026-01-07 21:42:11,943 [INFO] Step 18800/19460 | Loss: 0.6180 | Acc: 0.9375 | LR: 1.04e-06
2026-01-07 21:42:20,705 [INFO] [EVAL] Step 18800 | Val Loss: 0.3136 | Val Acc: 0.9206
2026-01-07 21:42:21,602 [WARNING] Skipping batch due to non-finite loss at step=18802 (loss=nan, epoch=20).
2026-01-07 21:42:24,294 [INFO] Step 18810/19460 | Loss: 1.0122 | Acc: 0.7500 | LR: 1.04e-06
2026-01-07 21:42:27,629 [INFO] Step 18820/19460 | Loss: 0.5810 | Acc: 0.9375 | LR: 1.04e-06
2026-01-07 21:42:31,101 [INFO] Step 18830/19460 | Loss: 0.5363 | Acc: 0.9375 | LR: 1.04e-06
2026-01-07 21:42:34,457 [INFO] Step 18840/19460 | Loss: 0.5708 | Acc: 0.9375 | LR: 1.04e-06
2026-01-07 21:42:37,844 [INFO] Step 18850/19460 | Loss: 0.7700 | Acc: 0.8125 | LR: 1.04e-06
2026-01-07 21:42:38,004 [WARNING] Skipping batch due to non-finite loss at step=18850 (loss=nan, epoch=20).
2026-01-07 21:42:41,406 [INFO] Step 18860/19460 | Loss: 0.4849 | Acc: 1.0000 | LR: 1.03e-06
2026-01-07 21:42:44,834 [INFO] Step 18870/19460 | Loss: 0.6132 | Acc: 0.9375 | LR: 1.03e-06
2026-01-07 21:42:48,301 [INFO] Step 18880/19460 | Loss: 1.0919 | Acc: 0.7500 | LR: 1.03e-06
2026-01-07 21:42:51,557 [INFO] Step 18890/19460 | Loss: 0.6927 | Acc: 0.9375 | LR: 1.03e-06
2026-01-07 21:42:55,399 [INFO] Step 18900/19460 | Loss: 0.5249 | Acc: 1.0000 | LR: 1.03e-06
2026-01-07 21:43:04,210 [INFO] [EVAL] Step 18900 | Val Loss: 0.3137 | Val Acc: 0.9175
2026-01-07 21:43:07,445 [INFO] Step 18910/19460 | Loss: 1.0208 | Acc: 0.6875 | LR: 1.03e-06
2026-01-07 21:43:10,728 [INFO] Step 18920/19460 | Loss: 0.7268 | Acc: 0.8750 | LR: 1.03e-06
2026-01-07 21:43:14,153 [INFO] Step 18930/19460 | Loss: 0.7181 | Acc: 0.8750 | LR: 1.03e-06
2026-01-07 21:43:17,570 [INFO] Step 18940/19460 | Loss: 0.4822 | Acc: 1.0000 | LR: 1.03e-06
2026-01-07 21:43:21,039 [INFO] Step 18950/19460 | Loss: 1.0067 | Acc: 0.8125 | LR: 1.03e-06
2026-01-07 21:43:24,417 [INFO] Step 18960/19460 | Loss: 1.3212 | Acc: 0.5625 | LR: 1.02e-06
2026-01-07 21:43:26,852 [WARNING] Skipping batch due to non-finite loss at step=18967 (loss=nan, epoch=20).
2026-01-07 21:43:27,851 [INFO] Step 18970/19460 | Loss: 0.6797 | Acc: 0.8125 | LR: 1.02e-06
2026-01-07 21:43:31,369 [INFO] Step 18980/19460 | Loss: 0.5764 | Acc: 0.9375 | LR: 1.02e-06
2026-01-07 21:43:34,682 [INFO] Step 18990/19460 | Loss: 0.6244 | Acc: 0.9375 | LR: 1.02e-06
2026-01-07 21:43:38,551 [INFO] Step 19000/19460 | Loss: 0.7026 | Acc: 0.8750 | LR: 1.02e-06
2026-01-07 21:43:47,290 [INFO] [EVAL] Step 19000 | Val Loss: 0.3146 | Val Acc: 0.9175
2026-01-07 21:43:47,321 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_19000.npz
2026-01-07 21:43:50,605 [INFO] Step 19010/19460 | Loss: 0.6163 | Acc: 0.9375 | LR: 1.02e-06
2026-01-07 21:43:53,891 [INFO] Step 19020/19460 | Loss: 0.7204 | Acc: 0.8750 | LR: 1.02e-06
2026-01-07 21:43:57,161 [INFO] Step 19030/19460 | Loss: 0.5495 | Acc: 0.9375 | LR: 1.02e-06
2026-01-07 21:44:00,649 [INFO] Step 19040/19460 | Loss: 0.9140 | Acc: 0.8125 | LR: 1.02e-06
2026-01-07 21:44:03,999 [INFO] Step 19050/19460 | Loss: 0.7351 | Acc: 0.7500 | LR: 1.02e-06
2026-01-07 21:44:07,293 [INFO] Step 19060/19460 | Loss: 0.4766 | Acc: 1.0000 | LR: 1.02e-06
2026-01-07 21:44:10,726 [INFO] Step 19070/19460 | Loss: 0.5578 | Acc: 1.0000 | LR: 1.01e-06
2026-01-07 21:44:14,110 [INFO] Step 19080/19460 | Loss: 0.5957 | Acc: 0.8750 | LR: 1.01e-06
2026-01-07 21:44:17,654 [INFO] Step 19090/19460 | Loss: 0.6001 | Acc: 0.8750 | LR: 1.01e-06
2026-01-07 21:44:21,529 [INFO] Step 19100/19460 | Loss: 0.8216 | Acc: 0.6875 | LR: 1.01e-06
2026-01-07 21:44:30,293 [INFO] [EVAL] Step 19100 | Val Loss: 0.3136 | Val Acc: 0.9200
2026-01-07 21:44:33,763 [INFO] Step 19110/19460 | Loss: 0.7932 | Acc: 0.9375 | LR: 1.01e-06
2026-01-07 21:44:37,153 [INFO] Step 19120/19460 | Loss: 0.6317 | Acc: 0.8750 | LR: 1.01e-06
2026-01-07 21:44:40,596 [INFO] Step 19130/19460 | Loss: 0.9018 | Acc: 0.7500 | LR: 1.01e-06
2026-01-07 21:44:44,285 [INFO] Step 19140/19460 | Loss: 0.6807 | Acc: 0.8750 | LR: 1.01e-06
2026-01-07 21:44:47,690 [INFO] Step 19150/19460 | Loss: 0.6810 | Acc: 0.9375 | LR: 1.01e-06
2026-01-07 21:44:51,325 [INFO] Step 19160/19460 | Loss: 0.8480 | Acc: 0.8125 | LR: 1.01e-06
2026-01-07 21:44:54,736 [INFO] Step 19170/19460 | Loss: 0.6770 | Acc: 0.8125 | LR: 1.01e-06
2026-01-07 21:44:58,047 [INFO] Step 19180/19460 | Loss: 0.4881 | Acc: 1.0000 | LR: 1.01e-06
2026-01-07 21:45:01,288 [INFO] Step 19190/19460 | Loss: 0.4630 | Acc: 1.0000 | LR: 1.01e-06
2026-01-07 21:45:05,398 [INFO] Step 19200/19460 | Loss: 0.5344 | Acc: 0.9375 | LR: 1.01e-06
2026-01-07 21:45:14,077 [INFO] [EVAL] Step 19200 | Val Loss: 0.3129 | Val Acc: 0.9181
2026-01-07 21:45:17,655 [INFO] Step 19210/19460 | Loss: 0.5815 | Acc: 0.9375 | LR: 1.01e-06
2026-01-07 21:45:21,039 [INFO] Step 19220/19460 | Loss: 0.7087 | Acc: 0.8750 | LR: 1.01e-06
2026-01-07 21:45:24,477 [INFO] Step 19230/19460 | Loss: 0.6201 | Acc: 0.9375 | LR: 1.01e-06
2026-01-07 21:45:27,843 [INFO] Step 19240/19460 | Loss: 0.4627 | Acc: 1.0000 | LR: 1.00e-06
2026-01-07 21:45:31,066 [INFO] Step 19250/19460 | Loss: 0.7918 | Acc: 0.8125 | LR: 1.00e-06
2026-01-07 21:45:34,551 [INFO] Step 19260/19460 | Loss: 0.5546 | Acc: 0.9375 | LR: 1.00e-06
2026-01-07 21:45:37,977 [INFO] Step 19270/19460 | Loss: 0.6966 | Acc: 0.8750 | LR: 1.00e-06
2026-01-07 21:45:41,251 [INFO] Step 19280/19460 | Loss: 0.8724 | Acc: 0.7500 | LR: 1.00e-06
2026-01-07 21:45:44,683 [INFO] Step 19290/19460 | Loss: 0.9412 | Acc: 0.6875 | LR: 1.00e-06
2026-01-07 21:45:48,817 [INFO] Step 19300/19460 | Loss: 0.5851 | Acc: 0.9375 | LR: 1.00e-06
2026-01-07 21:45:57,649 [INFO] [EVAL] Step 19300 | Val Loss: 0.3150 | Val Acc: 0.9169
2026-01-07 21:46:00,870 [INFO] Step 19310/19460 | Loss: 0.6709 | Acc: 0.9375 | LR: 1.00e-06
2026-01-07 21:46:04,067 [INFO] Epoch 20 complete | Avg Loss: 0.7014 | Avg Acc: 0.8700 | Updates: 956 | Micro-batches: 971 | Skipped: 15 (loss=15, logits=0, grads=0)
2026-01-07 21:46:13,067 [INFO] Final validation: Loss=0.3144, Acc=0.9163
2026-01-07 21:46:13,101 [INFO] Saved encoder weights (254 arrays) to checkpoints/paralinguistics_v9_encoder_save/encoder_step_19319.npz
2026-01-07 21:46:13,103 [INFO] Training completed in 8084.06s (2.25h)
2026-01-07 21:46:13,103 [INFO] Best validation accuracy: 0.9219
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4831: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 141: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6695: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4822: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 84: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8645: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4829: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1116: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4824: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6698: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12074: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8647: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8648: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7084: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8644: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4826: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6694: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12629: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4823: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4828: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4827: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4830: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12073: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12071: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4825: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12075: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12072: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12070: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 12616: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8646: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6696: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6697: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 7088: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 8643: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1117: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4832: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 4821: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 1118: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
Warning: Skipping sample 6693: shapes (80,257) and (0,) not aligned: 257 (dim 1) != 0 (dim 0)
