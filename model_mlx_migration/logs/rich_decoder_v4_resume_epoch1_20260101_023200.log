/opt/homebrew/lib/python3.14/site-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
Train: 19734 samples
Val: 2193 samples
Cache-only mode: 8876/19734 samples (45.0% cached)
Cache-only mode: 0/2193 samples (0.0% cached)
WARNING: No cached validation samples. Validation will be skipped.
Using cached encoder features from: data/v3_multitask/encoder_cache
Prosody features: ENABLED (from cache)
Model parameters:
  Total: 4,769,288
  Encoder LoRA: 1,474,560
  Classification head: 3,294,728
  Converted model parameters to bfloat16
Loading weights from checkpoints/rich_decoder_v4/epoch_1.npz
  Loaded 78 weight tensors

============================================================
Training RichDecoder v4 (Optimized)
============================================================
  Epochs: 10
  Batch size: 4 x 4 = 16
  Total steps: 22190
  bfloat16: True
  Prefetch: 4 batches
  Early stopping: True (patience=3)

Epoch 1/10
  Step 50: loss=0.5402, acc=75.00%, lr=5.00e-06
  Step 100: loss=1.1919, acc=50.00%, lr=1.00e-05
  Step 150: loss=1.2567, acc=50.00%, lr=1.50e-05
  Step 200: loss=0.7340, acc=75.00%, lr=2.00e-05
  Step 250: loss=0.0861, acc=100.00%, lr=2.50e-05
  Step 300: loss=0.1585, acc=100.00%, lr=3.00e-05
  Step 350: loss=0.0215, acc=100.00%, lr=3.50e-05
  Step 400: loss=1.0625, acc=75.00%, lr=4.00e-05
  Step 450: loss=0.0004, acc=100.00%, lr=4.50e-05
  Step 500: loss=0.8002, acc=50.00%, lr=5.00e-05
  Step 550: loss=0.5560, acc=75.00%, lr=5.00e-05
  Step 600: loss=0.2813, acc=75.00%, lr=5.00e-05
  Step 650: loss=0.8306, acc=50.00%, lr=5.00e-05
  Step 700: loss=0.2778, acc=100.00%, lr=5.00e-05
  Step 750: loss=0.5779, acc=75.00%, lr=5.00e-05
  Step 800: loss=0.3176, acc=100.00%, lr=5.00e-05
  Step 850: loss=0.1501, acc=100.00%, lr=5.00e-05
  Step 900: loss=0.3637, acc=100.00%, lr=5.00e-05
  Step 950: loss=0.2839, acc=100.00%, lr=4.99e-05
  Step 1000: loss=0.7684, acc=75.00%, lr=4.99e-05
  Step 1050: loss=1.3003, acc=25.00%, lr=4.99e-05
  Step 1100: loss=0.0664, acc=100.00%, lr=4.99e-05
  Step 1150: loss=0.7185, acc=100.00%, lr=4.99e-05
  Step 1200: loss=0.0578, acc=100.00%, lr=4.99e-05
  Step 1250: loss=0.6987, acc=50.00%, lr=4.99e-05
  Step 1300: loss=0.2387, acc=100.00%, lr=4.98e-05
  Step 1350: loss=0.0195, acc=100.00%, lr=4.98e-05
  Step 1400: loss=0.7374, acc=75.00%, lr=4.98e-05
  Step 1450: loss=0.0390, acc=100.00%, lr=4.98e-05
  Step 1500: loss=1.2096, acc=25.00%, lr=4.97e-05
  Step 1550: loss=0.0064, acc=100.00%, lr=4.97e-05
  Step 1600: loss=0.5293, acc=75.00%, lr=4.97e-05
  Step 1650: loss=0.2382, acc=100.00%, lr=4.97e-05
  Step 1700: loss=0.0099, acc=100.00%, lr=4.96e-05
  Step 1750: loss=0.1778, acc=100.00%, lr=4.96e-05
  Step 1800: loss=0.0083, acc=100.00%, lr=4.96e-05
  Step 1850: loss=0.9379, acc=75.00%, lr=4.95e-05
  Step 1900: loss=0.4477, acc=75.00%, lr=4.95e-05
  Step 1950: loss=0.1781, acc=100.00%, lr=4.95e-05
  Step 2000: loss=0.1153, acc=100.00%, lr=4.94e-05
  Step 2050: loss=0.2620, acc=100.00%, lr=4.94e-05
  Step 2100: loss=0.9684, acc=50.00%, lr=4.93e-05
  Step 2150: loss=0.5039, acc=100.00%, lr=4.93e-05
  Step 2200: loss=0.3832, acc=75.00%, lr=4.93e-05
  Epoch 1 complete: avg_loss=0.5514, time=241.8s
Epoch 2/10
  Step 2250: loss=0.9664, acc=75.00%, lr=4.92e-05
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1014, in <module>
    main()
    ~~~~^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 1010, in main
    trainer.train(train_loader, val_loader)
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/model_mlx_migration/tools/whisper_mlx/train_rich_decoder_v4.py", line 798, in train
    self.optimizer.update(self.model, accumulated_grads)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 29, in update
    model.update(self.apply_gradients(gradients, model))
                 ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 109, in apply_gradients
    return tree_map(self.apply_single, gradients, parameters, self.state)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 54, in tree_map
    k: tree_map(fn, child, *(r[k] for r in rest), is_leaf=is_leaf)
       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 2 more times]
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/utils.py", line 58, in tree_map
    return fn(tree, *rest)
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 586, in apply_single
    return super().apply_single(
           ~~~~~~~~~~~~~~~~~~~~^
        gradient, parameter * (1 - lr * self.weight_decay), state
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/ayates/Library/Python/3.14/lib/python/site-packages/mlx/optimizers/optimizers.py", line 523, in apply_single
    m = b1 * m + (1 - b1) * gradient
                 ~~~~~~~~~^~~~~~~~~~
RuntimeError: [metal::malloc] Resource limit (499000) exceeded.
