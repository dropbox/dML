[stderr] 2026-01-07 08:01:38,449 [INFO] Acquired training lock
[stderr] 2026-01-07 08:01:38,449 [INFO] Loading encoder from checkpoints/zipformer/en-streaming/exp/pretrained.pt
[stderr] 2026-01-07 08:01:38,540 [INFO] Encoder loaded: output_dim=512
[stderr] 2026-01-07 08:01:38,540 [INFO] Loading data from data/emotion/combined_emotion_hf (dataset=combined)
[stderr] 2026-01-07 08:01:38,969 [INFO] Train batches: 237
[stderr] 2026-01-07 08:01:38,969 [INFO] Val batches: 26
[stderr] 2026-01-07 08:01:38,995 [INFO] Head num_classes: 6
[stderr] 2026-01-07 08:01:38,996 [INFO] Created emotion head with 988,166 parameters
[stderr] 2026-01-07 08:01:38,996 [INFO] Stage 4: 11,754,487 params (UNFROZEN)
[stderr] 2026-01-07 08:01:38,996 [INFO] Stage 5: 3,906,660 params (UNFROZEN)
[stderr] 2026-01-07 08:01:38,996 [INFO] Total unfrozen encoder params: 15,661,147
2026-01-07 08:01:38,996 [INFO] Total trainable parameters: 16,649,313
[stderr] 2026-01-07 08:01:38,996 [INFO] Fine-tuning with encoder unfreezing: lr=4e-05 (encoder_lr parameter is ignored - both encoder and head use same lr)
2026-01-07 08:01:38,996 [INFO] ============================================================
2026-01-07 08:01:38,996 [INFO] Training Configuration
2026-01-07 08:01:38,996 [INFO] ============================================================
[stderr] 2026-01-07 08:01:38,996 [INFO] Head type: emotion
2026-01-07 08:01:38,996 [INFO] Encoder dim: 512
2026-01-07 08:01:38,996 [INFO] Batch size: 32
2026-01-07 08:01:38,996 [INFO] Head learning rate: 4e-05
2026-01-07 08:01:38,996 [INFO] Encoder learning rate: 1e-05
2026-01-07 08:01:38,996 [INFO] Unfrozen encoder stages: 2
[stderr] 2026-01-07 08:01:38,996 [INFO] Gradient clipping: 1.0
2026-01-07 08:01:38,996 [INFO] Cache clearing: every 100 steps
2026-01-07 08:01:38,996 [INFO] Label smoothing: 0.1
2026-01-07 08:01:38,996 [INFO] SpecAugment: True
2026-01-07 08:01:38,996 [INFO] Param dtype: float32
2026-01-07 08:01:38,997 [INFO] Epochs: 10
[stderr] 2026-01-07 08:01:38,997 [INFO] Max steps: 2370
2026-01-07 08:01:38,997 [INFO] Label key: emotion_labels
2026-01-07 08:01:38,997 [INFO] ============================================================
[stderr] 2026-01-07 08:01:38,997 [INFO] Created EncoderHeadModel for fine-tuning (reused across all steps)
[stderr] 2026-01-07 08:01:38,997 [INFO] Epoch 1/10
[stderr] 2026-01-07 08:01:42,881 [INFO] Step 10/2370 | Loss: 1.7862 | Acc: 0.1875 | LR: 7.20e-07
[stderr] 2026-01-07 08:01:46,181 [INFO] Step 20/2370 | Loss: 1.7934 | Acc: 0.1562 | LR: 1.52e-06
[stderr] 2026-01-07 08:01:49,551 [INFO] Step 30/2370 | Loss: 1.7974 | Acc: 0.0625 | LR: 2.32e-06
[stderr] 2026-01-07 08:01:52,941 [INFO] Step 40/2370 | Loss: 1.7925 | Acc: 0.1250 | LR: 3.12e-06
[stderr] 2026-01-07 08:01:56,316 [INFO] Step 50/2370 | Loss: 1.7916 | Acc: 0.1250 | LR: 3.92e-06
[stderr] 2026-01-07 08:01:59,635 [INFO] Step 60/2370 | Loss: 1.7848 | Acc: 0.2500 | LR: 4.72e-06
[stderr] 2026-01-07 08:02:03,028 [INFO] Step 70/2370 | Loss: 1.7950 | Acc: 0.1875 | LR: 5.52e-06
[stderr] 2026-01-07 08:02:06,355 [INFO] Step 80/2370 | Loss: 1.7962 | Acc: 0.1562 | LR: 6.32e-06
[stderr] 2026-01-07 08:02:09,737 [INFO] Step 90/2370 | Loss: 1.7765 | Acc: 0.1562 | LR: 7.12e-06
[stderr] 2026-01-07 08:02:13,544 [INFO] Step 100/2370 | Loss: 1.7945 | Acc: 0.1875 | LR: 7.92e-06
[stderr] 2026-01-07 08:02:17,396 [INFO] [EVAL] Step 100 | Val Loss: 1.7784 | Val Acc: 0.2452
[stderr] 2026-01-07 08:02:17,399 [INFO] New best validation accuracy: 0.2452
[stderr] 2026-01-07 08:02:21,038 [INFO] Step 110/2370 | Loss: 1.7861 | Acc: 0.1562 | LR: 8.72e-06
[stderr] 2026-01-07 08:02:24,782 [INFO] Step 120/2370 | Loss: 1.7681 | Acc: 0.2812 | LR: 9.52e-06
[stderr] 2026-01-07 08:02:28,586 [INFO] Step 130/2370 | Loss: 1.7818 | Acc: 0.2812 | LR: 1.03e-05
[stderr] 2026-01-07 08:02:32,314 [INFO] Step 140/2370 | Loss: 1.7478 | Acc: 0.2812 | LR: 1.11e-05
[stderr] 2026-01-07 08:02:36,055 [INFO] Step 150/2370 | Loss: 1.7593 | Acc: 0.3438 | LR: 1.19e-05
[stderr] 2026-01-07 08:02:39,873 [INFO] Step 160/2370 | Loss: 1.7326 | Acc: 0.3750 | LR: 1.27e-05
[stderr] 2026-01-07 08:02:43,620 [INFO] Step 170/2370 | Loss: 1.7335 | Acc: 0.3125 | LR: 1.35e-05
[stderr] 2026-01-07 08:02:47,298 [INFO] Step 180/2370 | Loss: 1.8853 | Acc: 0.0938 | LR: 1.43e-05
[stderr] 2026-01-07 08:02:50,889 [INFO] Step 190/2370 | Loss: 1.7492 | Acc: 0.0938 | LR: 1.51e-05
[stderr] 2026-01-07 08:02:56,152 [INFO] Step 200/2370 | Loss: 1.8049 | Acc: 0.3125 | LR: 1.59e-05
[stderr] 2026-01-07 08:02:59,921 [INFO] [EVAL] Step 200 | Val Loss: 1.7716 | Val Acc: 0.2188
[stderr] 2026-01-07 08:03:03,131 [INFO] Step 210/2370 | Loss: 1.8038 | Acc: 0.1875 | LR: 1.67e-05
[stderr] 2026-01-07 08:03:06,364 [INFO] Step 220/2370 | Loss: 1.7366 | Acc: 0.1562 | LR: 1.75e-05
[stderr] 2026-01-07 08:03:09,551 [INFO] Step 230/2370 | Loss: 1.6856 | Acc: 0.4062 | LR: 1.83e-05
[stderr] 2026-01-07 08:03:11,825 [INFO] Epoch 1 complete | Avg Loss: 1.7700 | Avg Acc: 0.2164 | Updates: 237 | Micro-batches: 237 | Skipped: 0 (loss=0, logits=0, grads=0)
[stderr] 2026-01-07 08:03:11,825 [INFO] Epoch 2/10
[stderr] 2026-01-07 08:03:12,895 [INFO] Step 240/2370 | Loss: 1.6759 | Acc: 0.2812 | LR: 1.91e-05
[stderr] 2026-01-07 08:03:16,255 [INFO] Step 250/2370 | Loss: 1.6500 | Acc: 0.4062 | LR: 1.99e-05
[stderr] 2026-01-07 08:03:19,594 [INFO] Step 260/2370 | Loss: 1.7764 | Acc: 0.2188 | LR: 2.07e-05
[stderr] 2026-01-07 08:03:22,880 [INFO] Step 270/2370 | Loss: 1.7457 | Acc: 0.1562 | LR: 2.15e-05
[stderr] 2026-01-07 08:03:26,208 [INFO] Step 280/2370 | Loss: 1.7861 | Acc: 0.1562 | LR: 2.23e-05
[stderr] 2026-01-07 08:03:29,539 [INFO] Step 290/2370 | Loss: 1.7416 | Acc: 0.2500 | LR: 2.31e-05
[stderr] 2026-01-07 08:03:33,337 [INFO] Step 300/2370 | Loss: 1.7569 | Acc: 0.2812 | LR: 2.39e-05
[stderr] 2026-01-07 08:03:37,060 [INFO] [EVAL] Step 300 | Val Loss: 1.7367 | Val Acc: 0.2524
[stderr] 2026-01-07 08:03:37,064 [INFO] New best validation accuracy: 0.2524
[stderr] 2026-01-07 08:03:40,440 [INFO] Step 310/2370 | Loss: 1.7718 | Acc: 0.3750 | LR: 2.47e-05
[stderr] 2026-01-07 08:03:43,819 [INFO] Step 320/2370 | Loss: 1.7270 | Acc: 0.2500 | LR: 2.55e-05
[stderr] 2026-01-07 08:03:47,149 [INFO] Step 330/2370 | Loss: 1.7222 | Acc: 0.2500 | LR: 2.63e-05
[stderr] 2026-01-07 08:03:50,365 [INFO] Step 340/2370 | Loss: 1.6934 | Acc: 0.2812 | LR: 2.71e-05
[stderr] 2026-01-07 08:03:53,701 [INFO] Step 350/2370 | Loss: 1.7400 | Acc: 0.2188 | LR: 2.79e-05
[stderr] 2026-01-07 08:03:56,933 [INFO] Step 360/2370 | Loss: 1.6565 | Acc: 0.3750 | LR: 2.87e-05
[stderr] 2026-01-07 08:04:00,238 [INFO] Step 370/2370 | Loss: 1.6098 | Acc: 0.4062 | LR: 2.95e-05
[stderr] 2026-01-07 08:04:03,667 [INFO] Step 380/2370 | Loss: 1.7101 | Acc: 0.2812 | LR: 3.03e-05
[stderr] 2026-01-07 08:04:07,015 [INFO] Step 390/2370 | Loss: 1.7568 | Acc: 0.1562 | LR: 3.11e-05
[stderr] 2026-01-07 08:04:10,607 [INFO] Step 400/2370 | Loss: 1.6824 | Acc: 0.2812 | LR: 3.19e-05
[stderr] 2026-01-07 08:04:14,355 [INFO] [EVAL] Step 400 | Val Loss: 1.7230 | Val Acc: 0.2380
[stderr] 2026-01-07 08:04:17,598 [INFO] Step 410/2370 | Loss: 1.6939 | Acc: 0.2812 | LR: 3.27e-05
[stderr] 2026-01-07 08:04:20,911 [INFO] Step 420/2370 | Loss: 1.6673 | Acc: 0.4375 | LR: 3.35e-05
[stderr] 2026-01-07 08:04:24,178 [INFO] Step 430/2370 | Loss: 1.7784 | Acc: 0.3125 | LR: 3.43e-05
[stderr] 2026-01-07 08:04:27,398 [INFO] Step 440/2370 | Loss: 1.7063 | Acc: 0.2188 | LR: 3.51e-05
[stderr] 2026-01-07 08:04:30,623 [INFO] Step 450/2370 | Loss: 1.7494 | Acc: 0.1562 | LR: 3.59e-05
[stderr] 2026-01-07 08:04:33,974 [INFO] Step 460/2370 | Loss: 1.7288 | Acc: 0.1562 | LR: 3.67e-05
[stderr] 2026-01-07 08:04:37,318 [INFO] Step 470/2370 | Loss: 1.8665 | Acc: 0.1250 | LR: 3.75e-05
[stderr] 2026-01-07 08:04:38,725 [INFO] Epoch 2 complete | Avg Loss: 1.7146 | Avg Acc: 0.2783 | Updates: 237 | Micro-batches: 237 | Skipped: 0 (loss=0, logits=0, grads=0)
[stderr] 2026-01-07 08:04:38,725 [INFO] Epoch 3/10
[stderr] 2026-01-07 08:04:40,814 [INFO] Step 480/2370 | Loss: 1.7324 | Acc: 0.3125 | LR: 3.83e-05
[stderr] 2026-01-07 08:04:44,146 [INFO] Step 490/2370 | Loss: 1.7732 | Acc: 0.2500 | LR: 3.91e-05
[stderr] 2026-01-07 08:04:47,882 [INFO] Step 500/2370 | Loss: 1.6515 | Acc: 0.3438 | LR: 3.99e-05
[stderr] 2026-01-07 08:04:51,597 [INFO] [EVAL] Step 500 | Val Loss: 1.6697 | Val Acc: 0.2873
[stderr] 2026-01-07 08:04:51,599 [INFO] New best validation accuracy: 0.2873
[stderr] 2026-01-07 08:04:54,916 [INFO] Step 510/2370 | Loss: 1.5814 | Acc: 0.4688 | LR: 4.00e-05
[stderr] 2026-01-07 08:04:58,254 [INFO] Step 520/2370 | Loss: 1.8688 | Acc: 0.2500 | LR: 4.00e-05
[stderr] 2026-01-07 08:05:01,534 [INFO] Step 530/2370 | Loss: 1.7798 | Acc: 0.2812 | LR: 4.00e-05
[stderr] 2026-01-07 08:05:04,882 [INFO] Step 540/2370 | Loss: 1.6904 | Acc: 0.3125 | LR: 4.00e-05
[stderr] 2026-01-07 08:05:08,310 [INFO] Step 550/2370 | Loss: 1.6085 | Acc: 0.4062 | LR: 3.99e-05
[stderr] 2026-01-07 08:05:11,693 [INFO] Step 560/2370 | Loss: 1.6803 | Acc: 0.3750 | LR: 3.99e-05
[stderr] 2026-01-07 08:05:15,035 [INFO] Step 570/2370 | Loss: 1.6780 | Acc: 0.3438 | LR: 3.99e-05
[stderr] 2026-01-07 08:05:18,352 [INFO] Step 580/2370 | Loss: 1.5972 | Acc: 0.4688 | LR: 3.98e-05
[stderr] 2026-01-07 08:05:21,705 [INFO] Step 590/2370 | Loss: 1.6761 | Acc: 0.2188 | LR: 3.98e-05
[stderr] 2026-01-07 08:05:25,517 [INFO] Step 600/2370 | Loss: 1.7052 | Acc: 0.3125 | LR: 3.97e-05
[stderr] 2026-01-07 08:05:29,222 [INFO] [EVAL] Step 600 | Val Loss: 1.7001 | Val Acc: 0.2584
[stderr] 2026-01-07 08:05:32,537 [INFO] Step 610/2370 | Loss: 1.6671 | Acc: 0.2500 | LR: 3.97e-05
[stderr] 2026-01-07 08:05:34,036 [WARNING] Skipping batch due to non-finite loss at step=614 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:35,981 [INFO] Step 620/2370 | Loss: 1.7482 | Acc: 0.2812 | LR: 3.96e-05
[stderr] 2026-01-07 08:05:36,492 [WARNING] Skipping batch due to non-finite loss at step=621 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:36,690 [WARNING] Skipping batch due to non-finite loss at step=621 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:37,867 [WARNING] Skipping batch due to non-finite loss at step=624 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:38,036 [WARNING] Skipping batch due to non-finite loss at step=624 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:40,015 [INFO] Step 630/2370 | Loss: 1.6455 | Acc: 0.3438 | LR: 3.95e-05
[stderr] 2026-01-07 08:05:43,223 [INFO] Step 640/2370 | Loss: 1.6929 | Acc: 0.2500 | LR: 3.95e-05
[stderr] 2026-01-07 08:05:46,497 [INFO] Step 650/2370 | Loss: 1.6192 | Acc: 0.3750 | LR: 3.94e-05
[stderr] 2026-01-07 08:05:49,732 [WARNING] Skipping batch due to non-finite loss at step=659 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:49,882 [WARNING] Skipping batch due to non-finite loss at step=659 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:50,205 [INFO] Step 660/2370 | Loss: 1.6714 | Acc: 0.4375 | LR: 3.93e-05
[stderr] 2026-01-07 08:05:52,032 [WARNING] Skipping batch due to non-finite loss at step=665 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:52,875 [WARNING] Skipping batch due to non-finite loss at step=667 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:53,755 [WARNING] Skipping batch due to non-finite loss at step=669 (loss=nan, epoch=3).
[stderr] 2026-01-07 08:05:54,097 [INFO] Step 670/2370 | Loss: 1.6193 | Acc: 0.3750 | LR: 3.92e-05
[stderr] 2026-01-07 08:05:57,527 [INFO] Step 680/2370 | Loss: 1.7012 | Acc: 0.2812 | LR: 3.91e-05
[stderr] 2026-01-07 08:06:00,900 [INFO] Step 690/2370 | Loss: 1.6557 | Acc: 0.4688 | LR: 3.90e-05
[stderr] 2026-01-07 08:06:03,839 [INFO] Epoch 3 complete | Avg Loss: 1.6649 | Avg Acc: 0.3142 | Updates: 224 | Micro-batches: 237 | Skipped: 13 (loss=13, logits=0, grads=0)
[stderr] 2026-01-07 08:06:03,839 [INFO] Epoch 4/10
[stderr] 2026-01-07 08:06:05,071 [INFO] Step 700/2370 | Loss: 1.6798 | Acc: 0.4062 | LR: 3.89e-05
[stderr] 2026-01-07 08:06:08,831 [INFO] [EVAL] Step 700 | Val Loss: 1.5817 | Val Acc: 0.3113
[stderr] 2026-01-07 08:06:08,832 [INFO] New best validation accuracy: 0.3113
[stderr] 2026-01-07 08:06:12,180 [INFO] Step 710/2370 | Loss: 1.6965 | Acc: 0.3125 | LR: 3.88e-05
[stderr] 2026-01-07 08:06:15,398 [INFO] Step 720/2370 | Loss: 1.7858 | Acc: 0.3750 | LR: 3.87e-05
[stderr] 2026-01-07 08:06:18,729 [INFO] Step 730/2370 | Loss: 1.6229 | Acc: 0.3438 | LR: 3.86e-05
[stderr] 2026-01-07 08:06:22,059 [INFO] Step 740/2370 | Loss: 1.5260 | Acc: 0.4062 | LR: 3.84e-05
[stderr] 2026-01-07 08:06:24,553 [WARNING] Skipping batch due to non-finite loss at step=747 (loss=nan, epoch=4).
[stderr] 2026-01-07 08:06:25,571 [INFO] Step 750/2370 | Loss: 1.7257 | Acc: 0.2812 | LR: 3.83e-05
[stderr] 2026-01-07 08:06:28,902 [INFO] Step 760/2370 | Loss: 1.7522 | Acc: 0.2500 | LR: 3.82e-05
[stderr] 2026-01-07 08:06:32,160 [INFO] Step 770/2370 | Loss: 1.6524 | Acc: 0.2188 | LR: 3.80e-05
[stderr] 2026-01-07 08:06:35,517 [INFO] Step 780/2370 | Loss: 1.6042 | Acc: 0.3125 | LR: 3.79e-05
[stderr] 2026-01-07 08:06:38,895 [INFO] Step 790/2370 | Loss: 1.7097 | Acc: 0.2188 | LR: 3.77e-05
[stderr] 2026-01-07 08:06:42,605 [INFO] Step 800/2370 | Loss: 1.6137 | Acc: 0.3750 | LR: 3.76e-05
[stderr] 2026-01-07 08:06:46,386 [INFO] [EVAL] Step 800 | Val Loss: 1.5430 | Val Acc: 0.3594
[stderr] 2026-01-07 08:06:46,389 [INFO] New best validation accuracy: 0.3594
[stderr] 2026-01-07 08:06:49,766 [INFO] Step 810/2370 | Loss: 1.6548 | Acc: 0.2188 | LR: 3.74e-05
[stderr] 2026-01-07 08:06:53,154 [INFO] Step 820/2370 | Loss: 1.5951 | Acc: 0.4062 | LR: 3.73e-05
[stderr] 2026-01-07 08:06:56,472 [INFO] Step 830/2370 | Loss: 1.5457 | Acc: 0.4375 | LR: 3.71e-05
[stderr] 2026-01-07 08:06:59,860 [INFO] Step 840/2370 | Loss: 1.6539 | Acc: 0.2500 | LR: 3.69e-05
[stderr] 2026-01-07 08:07:03,237 [INFO] Step 850/2370 | Loss: 1.5969 | Acc: 0.3125 | LR: 3.67e-05
[stderr] 2026-01-07 08:07:06,561 [INFO] Step 860/2370 | Loss: 1.7154 | Acc: 0.2812 | LR: 3.66e-05
[stderr] 2026-01-07 08:07:09,813 [INFO] Step 870/2370 | Loss: 1.4655 | Acc: 0.4688 | LR: 3.64e-05
[stderr] 2026-01-07 08:07:11,273 [WARNING] Skipping batch due to non-finite loss at step=874 (loss=nan, epoch=4).
[stderr] 2026-01-07 08:07:12,828 [WARNING] Skipping batch due to non-finite loss at step=878 (loss=nan, epoch=4).
[stderr] 2026-01-07 08:07:13,489 [INFO] Step 880/2370 | Loss: 1.4995 | Acc: 0.3750 | LR: 3.62e-05
[stderr] 2026-01-07 08:07:13,972 [WARNING] Skipping batch due to non-finite loss at step=881 (loss=nan, epoch=4).
[stderr] 2026-01-07 08:07:17,033 [INFO] Step 890/2370 | Loss: 1.6292 | Acc: 0.4062 | LR: 3.60e-05
[stderr] 2026-01-07 08:07:20,839 [INFO] Step 900/2370 | Loss: 1.4649 | Acc: 0.2812 | LR: 3.58e-05
[stderr] 2026-01-07 08:07:24,546 [INFO] [EVAL] Step 900 | Val Loss: 1.5592 | Val Acc: 0.3558
[stderr] 2026-01-07 08:07:27,801 [WARNING] Skipping batch due to non-finite loss at step=909 (loss=nan, epoch=4).
[stderr] 2026-01-07 08:07:28,172 [INFO] Step 910/2370 | Loss: 1.5748 | Acc: 0.3438 | LR: 3.56e-05
[stderr] 2026-01-07 08:07:28,666 [WARNING] Skipping batch due to non-finite loss at step=911 (loss=nan, epoch=4).
[stderr] 2026-01-07 08:07:31,656 [INFO] Step 920/2370 | Loss: 1.3933 | Acc: 0.5312 | LR: 3.54e-05
[stderr] 2026-01-07 08:07:34,638 [INFO] Epoch 4 complete | Avg Loss: 1.5961 | Avg Acc: 0.3626 | Updates: 231 | Micro-batches: 237 | Skipped: 6 (loss=6, logits=0, grads=0)
[stderr] 2026-01-07 08:07:34,638 [INFO] Epoch 5/10
[stderr] 2026-01-07 08:07:35,085 [INFO] Step 930/2370 | Loss: 1.6390 | Acc: 0.2500 | LR: 3.52e-05
[stderr] 2026-01-07 08:07:35,251 [WARNING] Skipping batch due to non-finite loss at step=930 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:07:38,515 [INFO] Step 940/2370 | Loss: 1.5589 | Acc: 0.3750 | LR: 3.49e-05
[stderr] 2026-01-07 08:07:41,804 [INFO] Step 950/2370 | Loss: 1.5863 | Acc: 0.3438 | LR: 3.47e-05
[stderr] 2026-01-07 08:07:45,108 [INFO] Step 960/2370 | Loss: 1.6087 | Acc: 0.4062 | LR: 3.45e-05
[stderr] 2026-01-07 08:07:48,488 [INFO] Step 970/2370 | Loss: 1.6407 | Acc: 0.2812 | LR: 3.43e-05
[stderr] 2026-01-07 08:07:51,755 [INFO] Step 980/2370 | Loss: 1.5522 | Acc: 0.3750 | LR: 3.40e-05
[stderr] 2026-01-07 08:07:55,096 [INFO] Step 990/2370 | Loss: 1.5869 | Acc: 0.3750 | LR: 3.38e-05
[stderr] 2026-01-07 08:07:57,944 [WARNING] Skipping batch due to non-finite loss at step=998 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:07:59,055 [INFO] Step 1000/2370 | Loss: 1.6677 | Acc: 0.2812 | LR: 3.35e-05
[stderr] 2026-01-07 08:08:02,776 [INFO] [EVAL] Step 1000 | Val Loss: 1.4535 | Val Acc: 0.3894
[stderr] 2026-01-07 08:08:02,777 [INFO] New best validation accuracy: 0.3894
[stderr] 2026-01-07 08:08:06,108 [INFO] Step 1010/2370 | Loss: 1.6656 | Acc: 0.2188 | LR: 3.33e-05
[stderr] 2026-01-07 08:08:09,415 [INFO] Step 1020/2370 | Loss: 1.5937 | Acc: 0.3750 | LR: 3.30e-05
[stderr] 2026-01-07 08:08:10,554 [WARNING] Skipping batch due to non-finite loss at step=1023 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:12,390 [WARNING] Skipping batch due to non-finite loss at step=1028 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:13,064 [INFO] Step 1030/2370 | Loss: 1.7062 | Acc: 0.2188 | LR: 3.28e-05
[stderr] 2026-01-07 08:08:13,214 [WARNING] Skipping batch due to non-finite loss at step=1030 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:16,557 [INFO] Step 1040/2370 | Loss: 1.6884 | Acc: 0.3125 | LR: 3.25e-05
[stderr] 2026-01-07 08:08:19,731 [WARNING] Skipping batch due to non-finite loss at step=1049 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:19,916 [WARNING] Skipping batch due to non-finite loss at step=1049 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:20,105 [WARNING] Skipping batch due to non-finite loss at step=1049 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:20,461 [INFO] Step 1050/2370 | Loss: 1.5750 | Acc: 0.4688 | LR: 3.23e-05
[stderr] 2026-01-07 08:08:20,990 [WARNING] Skipping batch due to non-finite loss at step=1051 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:21,852 [WARNING] Skipping batch due to non-finite loss at step=1053 (loss=nan, epoch=5).
[stderr] 2026-01-07 08:08:24,367 [INFO] Step 1060/2370 | Loss: 1.5400 | Acc: 0.3125 | LR: 3.20e-05
[stderr] 2026-01-07 08:08:31,131 [INFO] Step 1070/2370 | Loss: 1.5581 | Acc: 0.3438 | LR: 3.17e-05
[stderr] 2026-01-07 08:08:35,706 [INFO] Step 1080/2370 | Loss: 1.5029 | Acc: 0.5625 | LR: 3.15e-05
[stderr] 2026-01-07 08:08:39,427 [INFO] Step 1090/2370 | Loss: 1.5116 | Acc: 0.3750 | LR: 3.12e-05
[stderr] 2026-01-07 08:08:43,165 [INFO] Step 1100/2370 | Loss: 1.5444 | Acc: 0.4375 | LR: 3.09e-05
[stderr] 2026-01-07 08:08:46,895 [INFO] [EVAL] Step 1100 | Val Loss: 1.4661 | Val Acc: 0.3914
[stderr] 2026-01-07 08:08:46,897 [INFO] New best validation accuracy: 0.3914
[stderr] 2026-01-07 08:08:50,401 [INFO] Step 1110/2370 | Loss: 1.5611 | Acc: 0.3438 | LR: 3.07e-05
[stderr] 2026-01-07 08:08:53,901 [INFO] Step 1120/2370 | Loss: 1.7495 | Acc: 0.2188 | LR: 3.04e-05
[stderr] 2026-01-07 08:08:55,599 [INFO] Epoch 5 complete | Avg Loss: 1.5453 | Avg Acc: 0.3846 | Updates: 196 | Micro-batches: 237 | Skipped: 41 (loss=41, logits=0, grads=0)
[stderr] 2026-01-07 08:08:55,599 [INFO] Epoch 6/10
[stderr] 2026-01-07 08:08:56,919 [WARNING] Skipping batch due to non-finite loss at step=1128 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:08:57,555 [INFO] Step 1130/2370 | Loss: 1.5213 | Acc: 0.4375 | LR: 3.01e-05
[stderr] 2026-01-07 08:09:00,484 [WARNING] Skipping batch due to non-finite loss at step=1138 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:01,170 [INFO] Step 1140/2370 | Loss: 1.6980 | Acc: 0.3438 | LR: 2.98e-05
[stderr] 2026-01-07 08:09:04,407 [INFO] Step 1150/2370 | Loss: 1.4283 | Acc: 0.3438 | LR: 2.95e-05
[stderr] 2026-01-07 08:09:07,632 [INFO] Step 1160/2370 | Loss: 1.4742 | Acc: 0.2812 | LR: 2.92e-05
[stderr] 2026-01-07 08:09:08,477 [WARNING] Skipping batch due to non-finite loss at step=1162 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:09,301 [WARNING] Skipping batch due to non-finite loss at step=1164 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:09,777 [WARNING] Skipping batch due to non-finite loss at step=1165 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:11,433 [INFO] Step 1170/2370 | Loss: 1.6131 | Acc: 0.5000 | LR: 2.89e-05
[stderr] 2026-01-07 08:09:11,604 [WARNING] Skipping batch due to non-finite loss at step=1170 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:14,464 [WARNING] Skipping batch due to non-finite loss at step=1178 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:15,152 [INFO] Step 1180/2370 | Loss: 1.3704 | Acc: 0.4688 | LR: 2.86e-05
[stderr] 2026-01-07 08:09:15,653 [WARNING] Skipping batch due to non-finite loss at step=1181 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:18,526 [INFO] Step 1190/2370 | Loss: 1.4631 | Acc: 0.4688 | LR: 2.83e-05
[stderr] 2026-01-07 08:09:19,351 [WARNING] Skipping batch due to non-finite loss at step=1192 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:22,411 [INFO] Step 1200/2370 | Loss: 1.7438 | Acc: 0.3438 | LR: 2.80e-05
[stderr] 2026-01-07 08:09:26,160 [INFO] [EVAL] Step 1200 | Val Loss: 1.4443 | Val Acc: 0.3981
[stderr] 2026-01-07 08:09:26,162 [INFO] New best validation accuracy: 0.3981
[stderr] 2026-01-07 08:09:29,015 [WARNING] Skipping batch due to non-finite loss at step=1208 (loss=nan, epoch=6).
[stderr] 2026-01-07 08:09:29,739 [INFO] Step 1210/2370 | Loss: 1.4719 | Acc: 0.4375 | LR: 2.77e-05
[stderr] 2026-01-07 08:09:33,237 [INFO] Step 1220/2370 | Loss: 1.4429 | Acc: 0.4375 | LR: 2.74e-05
[stderr] 2026-01-07 08:09:36,870 [INFO] Step 1230/2370 | Loss: 1.5448 | Acc: 0.3438 | LR: 2.71e-05
[stderr] 2026-01-07 08:09:40,511 [INFO] Step 1240/2370 | Loss: 1.4300 | Acc: 0.4375 | LR: 2.68e-05
[stderr] 2026-01-07 08:09:44,430 [INFO] Step 1250/2370 | Loss: 1.4730 | Acc: 0.4375 | LR: 2.65e-05
[stderr] 2026-01-07 08:09:48,502 [INFO] Step 1260/2370 | Loss: 1.6207 | Acc: 0.3750 | LR: 2.62e-05
[stderr] 2026-01-07 08:09:51,810 [INFO] Step 1270/2370 | Loss: 1.5638 | Acc: 0.3750 | LR: 2.59e-05
[stderr] 2026-01-07 08:09:55,892 [INFO] Step 1280/2370 | Loss: 1.4600 | Acc: 0.4375 | LR: 2.56e-05
[stderr] 2026-01-07 08:09:59,435 [INFO] Step 1290/2370 | Loss: 1.5955 | Acc: 0.3438 | LR: 2.52e-05
[stderr] 2026-01-07 08:10:03,611 [INFO] Step 1300/2370 | Loss: 1.4512 | Acc: 0.4375 | LR: 2.49e-05
[stderr] 2026-01-07 08:10:07,309 [INFO] [EVAL] Step 1300 | Val Loss: 1.4118 | Val Acc: 0.4190
[stderr] 2026-01-07 08:10:07,311 [INFO] New best validation accuracy: 0.4190
[stderr] 2026-01-07 08:10:10,780 [INFO] Step 1310/2370 | Loss: 1.6284 | Acc: 0.4062 | LR: 2.46e-05
[stderr] 2026-01-07 08:10:14,461 [INFO] Step 1320/2370 | Loss: 1.5104 | Acc: 0.3750 | LR: 2.43e-05
[stderr] 2026-01-07 08:10:17,836 [INFO] Step 1330/2370 | Loss: 1.4794 | Acc: 0.4062 | LR: 2.40e-05
[stderr] 2026-01-07 08:10:17,837 [INFO] Epoch 6 complete | Avg Loss: 1.5064 | Avg Acc: 0.4155 | Updates: 205 | Micro-batches: 237 | Skipped: 32 (loss=32, logits=0, grads=0)
2026-01-07 08:10:17,837 [INFO] Epoch 7/10
[stderr] 2026-01-07 08:10:18,822 [WARNING] Skipping batch due to non-finite loss at step=1332 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:19,689 [WARNING] Skipping batch due to non-finite loss at step=1334 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:20,534 [WARNING] Skipping batch due to non-finite loss at step=1336 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:21,894 [INFO] Step 1340/2370 | Loss: 1.5539 | Acc: 0.3750 | LR: 2.36e-05
[stderr] 2026-01-07 08:10:23,047 [WARNING] Skipping batch due to non-finite loss at step=1343 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:23,197 [WARNING] Skipping batch due to non-finite loss at step=1343 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:25,604 [INFO] Step 1350/2370 | Loss: 1.4889 | Acc: 0.5625 | LR: 2.33e-05
[stderr] 2026-01-07 08:10:26,764 [WARNING] Skipping batch due to non-finite loss at step=1353 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:27,290 [WARNING] Skipping batch due to non-finite loss at step=1354 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:27,801 [WARNING] Skipping batch due to non-finite loss at step=1355 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:28,655 [WARNING] Skipping batch due to non-finite loss at step=1357 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:29,616 [INFO] Step 1360/2370 | Loss: 1.4725 | Acc: 0.4375 | LR: 2.30e-05
[stderr] 2026-01-07 08:10:32,922 [INFO] Step 1370/2370 | Loss: 1.4434 | Acc: 0.3750 | LR: 2.27e-05
[stderr] 2026-01-07 08:10:33,445 [WARNING] Skipping batch due to non-finite loss at step=1371 (loss=nan, epoch=7).
[stderr] 2026-01-07 08:10:37,064 [INFO] Step 1380/2370 | Loss: 1.4929 | Acc: 0.4062 | LR: 2.23e-05
[stderr] 2026-01-07 08:10:40,612 [INFO] Step 1390/2370 | Loss: 1.4737 | Acc: 0.4688 | LR: 2.20e-05
[stderr] 2026-01-07 08:10:44,635 [INFO] Step 1400/2370 | Loss: 1.4760 | Acc: 0.4375 | LR: 2.17e-05
[stderr] 2026-01-07 08:10:48,352 [INFO] [EVAL] Step 1400 | Val Loss: 1.4326 | Val Acc: 0.4304
[stderr] 2026-01-07 08:10:48,353 [INFO] New best validation accuracy: 0.4304
[stderr] 2026-01-07 08:10:51,680 [INFO] Step 1410/2370 | Loss: 1.4734 | Acc: 0.4688 | LR: 2.14e-05
[stderr] 2026-01-07 08:10:55,319 [INFO] Step 1420/2370 | Loss: 1.4978 | Acc: 0.4688 | LR: 2.10e-05
[stderr] 2026-01-07 08:10:58,764 [INFO] Step 1430/2370 | Loss: 1.4658 | Acc: 0.3438 | LR: 2.07e-05
[stderr] 2026-01-07 08:11:02,294 [INFO] Step 1440/2370 | Loss: 1.4752 | Acc: 0.4688 | LR: 2.04e-05
[stderr] 2026-01-07 08:11:05,741 [INFO] Step 1450/2370 | Loss: 1.4158 | Acc: 0.6250 | LR: 2.00e-05
[stderr] 2026-01-07 08:11:10,403 [INFO] Step 1460/2370 | Loss: 1.6640 | Acc: 0.2188 | LR: 1.97e-05
[stderr] 2026-01-07 08:11:14,821 [INFO] Step 1470/2370 | Loss: 1.6176 | Acc: 0.4062 | LR: 1.94e-05
[stderr] 2026-01-07 08:11:18,703 [INFO] Step 1480/2370 | Loss: 1.5301 | Acc: 0.4688 | LR: 1.91e-05
[stderr] 2026-01-07 08:11:23,895 [INFO] Step 1490/2370 | Loss: 1.4722 | Acc: 0.3438 | LR: 1.87e-05
[stderr] 2026-01-07 08:11:28,553 [INFO] Step 1500/2370 | Loss: 1.5302 | Acc: 0.4688 | LR: 1.84e-05
[stderr] 2026-01-07 08:11:32,264 [INFO] [EVAL] Step 1500 | Val Loss: 1.3959 | Val Acc: 0.4390
[stderr] 2026-01-07 08:11:32,265 [INFO] New best validation accuracy: 0.4390
[stderr] 2026-01-07 08:11:36,441 [INFO] Step 1510/2370 | Loss: 1.4017 | Acc: 0.3438 | LR: 1.81e-05
[stderr] 2026-01-07 08:11:36,441 [INFO] Epoch 7 complete | Avg Loss: 1.4966 | Avg Acc: 0.4234 | Updates: 180 | Micro-batches: 237 | Skipped: 57 (loss=57, logits=0, grads=0)
2026-01-07 08:11:36,441 [INFO] Epoch 8/10
[stderr] 2026-01-07 08:11:36,765 [WARNING] Skipping batch due to non-finite loss at step=1510 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:36,926 [WARNING] Skipping batch due to non-finite loss at step=1510 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:37,086 [WARNING] Skipping batch due to non-finite loss at step=1510 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:37,561 [WARNING] Skipping batch due to non-finite loss at step=1511 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:38,359 [WARNING] Skipping batch due to non-finite loss at step=1513 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:38,880 [WARNING] Skipping batch due to non-finite loss at step=1514 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:39,061 [WARNING] Skipping batch due to non-finite loss at step=1514 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:39,216 [WARNING] Skipping batch due to non-finite loss at step=1514 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:39,378 [WARNING] Skipping batch due to non-finite loss at step=1514 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:39,941 [WARNING] Skipping batch due to non-finite loss at step=1515 (loss=nan, epoch=8).
[stderr] 2026-01-07 08:11:42,225 [INFO] Step 1520/2370 | Loss: 1.5265 | Acc: 0.4375 | LR: 1.78e-05
[stderr] 2026-01-07 08:11:45,829 [INFO] Step 1530/2370 | Loss: 1.4599 | Acc: 0.5625 | LR: 1.74e-05
[stderr] 2026-01-07 08:11:49,986 [INFO] Step 1540/2370 | Loss: 1.7002 | Acc: 0.4375 | LR: 1.71e-05
[stderr] 2026-01-07 08:11:53,912 [INFO] Step 1550/2370 | Loss: 1.3701 | Acc: 0.5312 | LR: 1.68e-05
[stderr] 2026-01-07 08:11:59,480 [INFO] Step 1560/2370 | Loss: 1.4024 | Acc: 0.4375 | LR: 1.65e-05
[stderr] 2026-01-07 08:12:03,818 [INFO] Step 1570/2370 | Loss: 1.4826 | Acc: 0.5000 | LR: 1.61e-05
[stderr] 2026-01-07 08:12:07,931 [INFO] Step 1580/2370 | Loss: 1.5759 | Acc: 0.3438 | LR: 1.58e-05
[stderr] 2026-01-07 08:12:13,605 [INFO] Step 1590/2370 | Loss: 1.4104 | Acc: 0.4688 | LR: 1.55e-05
[stderr] 2026-01-07 08:12:20,100 [INFO] Step 1600/2370 | Loss: 1.5963 | Acc: 0.3750 | LR: 1.52e-05
[stderr] 2026-01-07 08:12:23,780 [INFO] [EVAL] Step 1600 | Val Loss: 1.4374 | Val Acc: 0.4243
[stderr] 2026-01-07 08:12:27,864 [INFO] Step 1610/2370 | Loss: 1.5143 | Acc: 0.3750 | LR: 1.49e-05
[stderr] 2026-01-07 08:12:31,309 [INFO] Step 1620/2370 | Loss: 1.4538 | Acc: 0.4375 | LR: 1.46e-05
[stderr] 2026-01-07 08:12:35,212 [INFO] Step 1630/2370 | Loss: 1.6474 | Acc: 0.4375 | LR: 1.43e-05
[stderr] 2026-01-07 08:12:39,353 [INFO] Step 1640/2370 | Loss: 1.5345 | Acc: 0.3750 | LR: 1.39e-05
[stderr] 2026-01-07 08:12:42,675 [INFO] Step 1650/2370 | Loss: 1.6064 | Acc: 0.3750 | LR: 1.36e-05
[stderr] 2026-01-07 08:12:46,073 [INFO] Step 1660/2370 | Loss: 1.4085 | Acc: 0.4375 | LR: 1.33e-05
[stderr] 2026-01-07 08:12:49,614 [INFO] Step 1670/2370 | Loss: 1.7425 | Acc: 0.3125 | LR: 1.30e-05
[stderr] 2026-01-07 08:12:51,288 [INFO] Epoch 8 complete | Avg Loss: 1.4747 | Avg Acc: 0.4299 | Updates: 164 | Micro-batches: 237 | Skipped: 73 (loss=73, logits=0, grads=0)
[stderr] 2026-01-07 08:12:51,288 [INFO] Epoch 9/10
[stderr] 2026-01-07 08:12:52,898 [WARNING] Skipping batch due to non-finite loss at step=1678 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:53,553 [INFO] Step 1680/2370 | Loss: 1.4225 | Acc: 0.3750 | LR: 1.27e-05
[stderr] 2026-01-07 08:12:54,029 [WARNING] Skipping batch due to non-finite loss at step=1681 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:55,843 [WARNING] Skipping batch due to non-finite loss at step=1686 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:56,349 [WARNING] Skipping batch due to non-finite loss at step=1687 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:56,510 [WARNING] Skipping batch due to non-finite loss at step=1687 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:57,528 [INFO] Step 1690/2370 | Loss: 1.4583 | Acc: 0.4688 | LR: 1.24e-05
[stderr] 2026-01-07 08:12:59,010 [WARNING] Skipping batch due to non-finite loss at step=1694 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:59,183 [WARNING] Skipping batch due to non-finite loss at step=1694 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:59,336 [WARNING] Skipping batch due to non-finite loss at step=1694 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:12:59,529 [WARNING] Skipping batch due to non-finite loss at step=1694 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:13:00,030 [WARNING] Skipping batch due to non-finite loss at step=1695 (loss=nan, epoch=9).
[stderr] 2026-01-07 08:13:02,326 [INFO] Step 1700/2370 | Loss: 1.5422 | Acc: 0.3750 | LR: 1.21e-05
[stderr] 2026-01-07 08:13:05,996 [INFO] [EVAL] Step 1700 | Val Loss: 1.3869 | Val Acc: 0.4670
[stderr] 2026-01-07 08:13:05,998 [INFO] New best validation accuracy: 0.4670
[stderr] 2026-01-07 08:13:11,105 [INFO] Step 1710/2370 | Loss: 1.4440 | Acc: 0.4375 | LR: 1.18e-05
[stderr] 2026-01-07 08:13:15,127 [INFO] Step 1720/2370 | Loss: 1.3615 | Acc: 0.5000 | LR: 1.15e-05
[stderr] 2026-01-07 08:13:19,956 [INFO] Step 1730/2370 | Loss: 1.4102 | Acc: 0.4062 | LR: 1.13e-05
[stderr] 2026-01-07 08:13:23,924 [INFO] Step 1740/2370 | Loss: 1.4824 | Acc: 0.4062 | LR: 1.10e-05
[stderr] 2026-01-07 08:13:27,369 [INFO] Step 1750/2370 | Loss: 1.5355 | Acc: 0.4375 | LR: 1.07e-05
[stderr] 2026-01-07 08:13:30,932 [INFO] Step 1760/2370 | Loss: 1.3797 | Acc: 0.5000 | LR: 1.04e-05
[stderr] 2026-01-07 08:13:35,121 [INFO] Step 1770/2370 | Loss: 1.5129 | Acc: 0.4375 | LR: 1.01e-05
[stderr] 2026-01-07 08:13:40,372 [INFO] Step 1780/2370 | Loss: 1.3954 | Acc: 0.4375 | LR: 9.85e-06
[stderr] 2026-01-07 08:13:44,776 [INFO] Step 1790/2370 | Loss: 1.4464 | Acc: 0.4688 | LR: 9.57e-06
[stderr] 2026-01-07 08:13:49,066 [INFO] Step 1800/2370 | Loss: 1.4739 | Acc: 0.4375 | LR: 9.30e-06
[stderr] 2026-01-07 08:13:52,795 [INFO] [EVAL] Step 1800 | Val Loss: 1.3807 | Val Acc: 0.4554
[stderr] 2026-01-07 08:13:56,514 [INFO] Step 1810/2370 | Loss: 1.4489 | Acc: 0.4062 | LR: 9.04e-06
[stderr] 2026-01-07 08:14:00,907 [INFO] Step 1820/2370 | Loss: 1.4102 | Acc: 0.4688 | LR: 8.77e-06
[stderr] 2026-01-07 08:14:04,439 [INFO] Step 1830/2370 | Loss: 1.5972 | Acc: 0.3750 | LR: 8.51e-06
[stderr] 2026-01-07 08:14:07,760 [INFO] Epoch 9 complete | Avg Loss: 1.4593 | Avg Acc: 0.4409 | Updates: 165 | Micro-batches: 237 | Skipped: 72 (loss=72, logits=0, grads=0)
[stderr] 2026-01-07 08:14:07,761 [INFO] Epoch 10/10
[stderr] 2026-01-07 08:14:08,071 [WARNING] Skipping batch due to non-finite loss at step=1839 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:08,270 [WARNING] Skipping batch due to non-finite loss at step=1839 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:08,454 [WARNING] Skipping batch due to non-finite loss at step=1839 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:08,627 [WARNING] Skipping batch due to non-finite loss at step=1839 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:08,972 [INFO] Step 1840/2370 | Loss: 1.5734 | Acc: 0.4375 | LR: 8.26e-06
[stderr] 2026-01-07 08:14:09,127 [WARNING] Skipping batch due to non-finite loss at step=1840 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:09,644 [WARNING] Skipping batch due to non-finite loss at step=1841 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:12,018 [WARNING] Skipping batch due to non-finite loss at step=1847 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:12,564 [WARNING] Skipping batch due to non-finite loss at step=1848 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:13,302 [INFO] Step 1850/2370 | Loss: 1.2881 | Acc: 0.5625 | LR: 8.00e-06
[stderr] 2026-01-07 08:14:17,087 [INFO] Step 1860/2370 | Loss: 1.4905 | Acc: 0.4375 | LR: 7.75e-06
[stderr] 2026-01-07 08:14:17,238 [WARNING] Skipping batch due to non-finite loss at step=1860 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:17,774 [WARNING] Skipping batch due to non-finite loss at step=1861 (loss=nan, epoch=10).
[stderr] 2026-01-07 08:14:21,467 [INFO] Step 1870/2370 | Loss: 1.4363 | Acc: 0.6250 | LR: 7.51e-06
[stderr] 2026-01-07 08:14:26,114 [INFO] Step 1880/2370 | Loss: 1.4670 | Acc: 0.4688 | LR: 7.27e-06
[stderr] 2026-01-07 08:14:30,657 [INFO] Step 1890/2370 | Loss: 1.2344 | Acc: 0.5312 | LR: 7.03e-06
[stderr] 2026-01-07 08:14:37,282 [INFO] Step 1900/2370 | Loss: 1.3602 | Acc: 0.5938 | LR: 6.79e-06
[stderr] 2026-01-07 08:14:41,037 [INFO] [EVAL] Step 1900 | Val Loss: 1.3909 | Val Acc: 0.4391
[stderr] 2026-01-07 08:14:46,093 [INFO] Step 1910/2370 | Loss: 1.5771 | Acc: 0.4062 | LR: 6.56e-06
[stderr] 2026-01-07 08:14:50,525 [INFO] Step 1920/2370 | Loss: 1.4468 | Acc: 0.5000 | LR: 6.33e-06
[stderr] 2026-01-07 08:14:55,363 [INFO] Step 1930/2370 | Loss: 1.5428 | Acc: 0.4688 | LR: 6.11e-06
[stderr] 2026-01-07 08:14:59,790 [INFO] Step 1940/2370 | Loss: 1.4097 | Acc: 0.5312 | LR: 5.89e-06
[stderr] 2026-01-07 08:15:04,807 [INFO] Step 1950/2370 | Loss: 1.2532 | Acc: 0.5625 | LR: 5.68e-06
[stderr] 2026-01-07 08:15:09,275 [INFO] Step 1960/2370 | Loss: 1.4928 | Acc: 0.4688 | LR: 5.47e-06
[stderr] 2026-01-07 08:15:14,119 [INFO] Step 1970/2370 | Loss: 1.2702 | Acc: 0.6250 | LR: 5.26e-06
[stderr] 2026-01-07 08:15:18,140 [INFO] Step 1980/2370 | Loss: 1.3450 | Acc: 0.4688 | LR: 5.06e-06
[stderr] 2026-01-07 08:15:19,692 [INFO] Epoch 10 complete | Avg Loss: 1.4420 | Avg Acc: 0.4629 | Updates: 145 | Micro-batches: 237 | Skipped: 92 (loss=92, logits=0, grads=0)
[stderr] 2026-01-07 08:15:23,560 [INFO] Final validation: Loss=1.3737, Acc=0.4601
[stderr] 2026-01-07 08:15:23,561 [INFO] Training completed in 824.56s (0.23h)
[stderr] 2026-01-07 08:15:23,561 [INFO] Best validation accuracy: 0.4670
