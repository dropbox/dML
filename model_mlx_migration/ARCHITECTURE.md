# Complete System Architecture

**Last Updated:** 2026-01-02
**Status:** Production + Active Development
**Role:** CANONICAL SOURCE OF TRUTH for system architecture

---

## Related Documents

| Document | Role | Status |
|----------|------|--------|
| **ARCHITECTURE.md** (this file) | System overview, component status | **CANONICAL** |
| **[ARCHITECTURE_GOD_TIER_STT_V3.md](reports/main/ARCHITECTURE_GOD_TIER_STT_V3.md)** | Hardened spec: overlap, streaming, gates | **ACTIVE** |
| [REVIEW_GOD_TIER_STT_STREAMING_RISKS_PATCH](reports/main/REVIEW_GOD_TIER_STT_STREAMING_RISKS_PATCH_2026-01-02-21-33.md) | Risk review informing V3 | Reference |
| [SPEECH_ENHANCEMENT_SOTA_RESEARCH](reports/main/SPEECH_ENHANCEMENT_SOTA_RESEARCH_2026-01-02.md) | Denoising, AEC, dereverberation research | Reference |
| [ARCHITECTURE_GOD_TIER_STT.md](reports/main/ARCHITECTURE_GOD_TIER_STT.md) | V2.0 | **SUPERSEDED** |
| [ARCHITECTURE_SPEAKER_ADAPTIVE_SOTA_PLUS_v1](reports/main/archive/architecture_2026-01-02/ARCHITECTURE_SPEAKER_ADAPTIVE_SOTA_PLUS_v1.md) | V1.0 | **ARCHIVED** |

---

## Critical Contract Decisions (Binding)

These decisions resolve prior document conflicts and are BINDING:

| ID | Decision | Rationale |
|----|----------|-----------|
| **C1** | We do speaker-local tracking, NOT diarization | Session-local IDs; downstream does global clustering |
| **C2** | Paralinguistics: 50-class schema, 11 trained (96.96%) | Report metrics with class count |
| **C3** | Para tokens are OUT-OF-BAND, not in vocabulary | Checkpoint compatibility preserved |
| **C4** | MossFormer2 3-speaker uses 8kHzâ†’16kHz resampling | Expect -3dB SI-SDRi vs 2-speaker |
| **C5** | Alignment IDs namespaced: `spk_{idx}_a{seq}` | No cross-speaker backtracks |
| **C6** | Latency: 1s chunk, batch=1, M2 Max baseline | CTC p50: 77ms, Decoder p50: 227ms |

See [ARCHITECTURE_GOD_TIER_STT_V3.md](reports/main/ARCHITECTURE_GOD_TIER_STT_V3.md) for full rationale, detailed design, and falsifiable benchmark gates.

---

## System Overview

A streaming, multi-speaker, rich audio understanding system built on Whisper MLX with multi-head outputs for emotion, pitch, phonemes, paralinguistics, speaker embeddings, and language identification.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              COMPLETE SYSTEM DIAGRAM                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         RAW AUDIO INPUT (16kHz)                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚                                            â”‚
â”‚                                        â–¼                                            â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘                    LAYER 0: UPSTREAM PREPROCESSING                           â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                        â”‚                                            â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚            â–¼                           â–¼                           â–¼               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚   SILERO VAD    â”‚       â”‚    OVERLAP      â”‚       â”‚   MOSSFORMER2   â”‚          â”‚
â”‚   â”‚                 â”‚       â”‚    DETECTOR     â”‚       â”‚      MLX        â”‚          â”‚
â”‚   â”‚   âœ… Production â”‚       â”‚                 â”‚       â”‚                 â”‚          â”‚
â”‚   â”‚   Latency: 2ms  â”‚       â”‚   âŒ Need Train â”‚       â”‚   âœ… Production â”‚          â”‚
â”‚   â”‚                 â”‚       â”‚   Latency: 5ms  â”‚       â”‚   Latency: 30ms â”‚          â”‚
â”‚   â”‚   Output:       â”‚       â”‚                 â”‚       â”‚   SI-SDRi: 21dB â”‚          â”‚
â”‚   â”‚   - speech_prob â”‚       â”‚   Output:       â”‚       â”‚   Speed: 9.5Ã—RT â”‚          â”‚
â”‚   â”‚   - is_speech   â”‚       â”‚   - num_spkrs   â”‚       â”‚                 â”‚          â”‚
â”‚   â”‚                 â”‚       â”‚     (0,1,2,3)   â”‚       â”‚   Output:       â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   - N separate  â”‚          â”‚
â”‚            â”‚                         â”‚                â”‚     waveforms   â”‚          â”‚
â”‚            â”‚                         â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                         â”‚                         â”‚                   â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                      â”‚                                              â”‚
â”‚                                      â–¼                                              â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                      â”‚     CONDITIONAL ROUTING       â”‚                             â”‚
â”‚                      â”‚                               â”‚                             â”‚
â”‚                      â”‚  if num_speakers <= 1:       â”‚                             â”‚
â”‚                      â”‚    â†’ FAST PATH (no sep)      â”‚                             â”‚
â”‚                      â”‚                               â”‚                             â”‚
â”‚                      â”‚  if num_speakers >= 2:       â”‚                             â”‚
â”‚                      â”‚    â†’ SEPARATION PATH         â”‚                             â”‚
â”‚                      â”‚      (MossFormer2)           â”‚                             â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                      â”‚                                              â”‚
â”‚                                      â–¼                                              â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘                    LAYER 1: WHISPER ENCODER                                  â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                      â”‚                                              â”‚
â”‚                                      â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                     WHISPER ENCODER (Frozen)                                 â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   âœ… Production                                                              â”‚   â”‚
â”‚   â”‚   Model: large-v3                                                            â”‚   â”‚
â”‚   â”‚   Params: 1.5B (frozen)                                                      â”‚   â”‚
â”‚   â”‚   Output: 1280-dim embeddings @ 50Hz                                         â”‚   â”‚
â”‚   â”‚   Latency: ~45ms                                                             â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   Audio â†’ Mel Spectrogram â†’ Conv Stem â†’ 32Ã— Transformer Blocks â†’ Features   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                      â”‚                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                    â”‚                                   â”‚                           â”‚
â”‚                    â–¼                                   â–¼                           â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘  LAYER 2A: CTC STREAM       â•‘   â•‘  LAYER 2B: DECODER STREAM               â•‘   â”‚
â”‚   â•‘  (Low Latency ~60ms)        â•‘   â•‘  (High Quality ~200-300ms)              â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                    â”‚                                   â”‚                           â”‚
â”‚                    â–¼                                   â–¼                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚      RICH CTC HEAD          â”‚   â”‚          RICH DECODER                   â”‚   â”‚
â”‚   â”‚                             â”‚   â”‚                                         â”‚   â”‚
â”‚   â”‚   âœ… Production             â”‚   â”‚   ğŸ”„ Training (82.34%)                  â”‚   â”‚
â”‚   â”‚   Latency: ~10ms            â”‚   â”‚   Latency: ~150-200ms                   â”‚   â”‚
â”‚   â”‚                             â”‚   â”‚                                         â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚   Architecture:                         â”‚   â”‚
â”‚   â”‚   â”‚ Text CTC (51,865)     â”‚ â”‚   â”‚   - Whisper Decoder (frozen)            â”‚   â”‚
â”‚   â”‚   â”‚ Emotion (8 classes)   â”‚ â”‚   â”‚   - LoRA Adapters (trainable)           â”‚   â”‚
â”‚   â”‚   â”‚ Pitch (F0 Hz)         â”‚ â”‚   â”‚   - Prosody Cross-Attention             â”‚   â”‚
â”‚   â”‚   â”‚ Para (50 classes)     â”‚ â”‚   â”‚     (sees CTC emotion + pitch)          â”‚   â”‚
â”‚   â”‚   â”‚ Phonemes (178 Misaki) â”‚ â”‚   â”‚                                         â”‚   â”‚
â”‚   â”‚   â”‚ Speaker Embed (256)   â”‚ â”‚   â”‚   Outputs:                              â”‚   â”‚
â”‚   â”‚   â”‚ Language (100)        â”‚ â”‚   â”‚   - Text tokens + timestamps            â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚   - Punctuation (!?.,)                  â”‚   â”‚
â”‚   â”‚                             â”‚   â”‚   - Emotion (refined)                   â”‚   â”‚
â”‚   â”‚   All outputs @ 50Hz        â”‚   â”‚   - Phoneme deviation score             â”‚   â”‚
â”‚   â”‚   Frame-aligned             â”‚   â”‚   - Confidence scores                   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                  â”‚                                      â”‚                          â”‚
â”‚                  â”‚         Prosody Conditioning         â”‚                          â”‚
â”‚                  â”‚    (emotion_seq, pitch_seq) â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                          â”‚
â”‚                  â”‚                                      â”‚                          â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                     â”‚                                               â”‚
â”‚                                     â–¼                                               â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘                    LAYER 3: DUAL-STREAM FUSION                               â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                     â”‚                                               â”‚
â”‚                                     â–¼                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    DUAL-STREAM CONSUMER                                      â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   âœ… Production (RichStreamConsumer)                                         â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   Event Types:                                                               â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚   TOKEN     â”‚   CONFIRM   â”‚    DIFF     â”‚  BACKTRACK  â”‚    FINAL    â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  (CTC new)  â”‚(decoder ok) â”‚(decoder fix)â”‚ (CTC revise)â”‚ (committed) â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   Timeline:                                                                  â”‚   â”‚
â”‚   â”‚   t=0ms    CTC: token "Hello" (provisional)                                 â”‚   â”‚
â”‚   â”‚   t=20ms   CTC: token "<|LAUGH|>" (provisional)                             â”‚   â”‚
â”‚   â”‚   t=200ms  Decoder: confirm "Hello" (final)                                 â”‚   â”‚
â”‚   â”‚   t=220ms  Decoder: confirm "<|LAUGH|>" (final)                             â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚                                               â”‚
â”‚                                     â–¼                                               â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘                    LAYER 4: POST-PROCESSING                                  â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                     â”‚                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â–¼             â–¼                   â–¼                   â–¼             â–¼            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ SPEAKER   â”‚ â”‚ CUSTOM    â”‚ â”‚  HALLUCINATION  â”‚ â”‚UNRECOG    â”‚ â”‚  ADAPTATION   â”‚   â”‚
â”‚ â”‚ BUFFER    â”‚ â”‚ VOCAB     â”‚ â”‚   DETECTION     â”‚ â”‚WORD MEM   â”‚ â”‚  DATA         â”‚   â”‚
â”‚ â”‚           â”‚ â”‚           â”‚ â”‚                 â”‚ â”‚           â”‚ â”‚  COLLECTION   â”‚   â”‚
â”‚ â”‚ âœ… Design â”‚ â”‚ âœ… Design â”‚ â”‚  âœ… Production  â”‚ â”‚ âœ… Design â”‚ â”‚  âœ… Design    â”‚   â”‚
â”‚ â”‚           â”‚ â”‚           â”‚ â”‚                 â”‚ â”‚           â”‚ â”‚               â”‚   â”‚
â”‚ â”‚ Track IDs â”‚ â”‚ Hotword   â”‚ â”‚ Phoneme verify  â”‚ â”‚ Cluster   â”‚ â”‚ Per-speaker   â”‚   â”‚
â”‚ â”‚ EMA embed â”‚ â”‚ boosting  â”‚ â”‚ 55.6% recall    â”‚ â”‚ unknowns  â”‚ â”‚ fine-tune     â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚                                               â”‚
â”‚                                     â–¼                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         RICH TOKEN OUTPUT                                    â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   {                                                                          â”‚   â”‚
â”‚   â”‚     "alignment_id": "a1",                                                    â”‚   â”‚
â”‚   â”‚     "stream": "decoder",                                                     â”‚   â”‚
â”‚   â”‚     "token": "Hello",                                                        â”‚   â”‚
â”‚   â”‚     "start_time_ms": 0.0,                                                    â”‚   â”‚
â”‚   â”‚     "end_time_ms": 320.0,                                                    â”‚   â”‚
â”‚   â”‚     "confidence": 0.95,                                                      â”‚   â”‚
â”‚   â”‚     "language": "en",                                                        â”‚   â”‚
â”‚   â”‚     "emotion": "happy",                                                      â”‚   â”‚
â”‚   â”‚     "pitch_hz": 185.5,                                                       â”‚   â”‚
â”‚   â”‚     "phonemes": ["h", "É™", "l", "oÊŠ"],                                       â”‚   â”‚
â”‚   â”‚     "phoneme_deviation": 0.05,                                               â”‚   â”‚
â”‚   â”‚     "para_class": null,                                                      â”‚   â”‚
â”‚   â”‚     "speaker_id": 0,                                                         â”‚   â”‚
â”‚   â”‚     "speaker_embedding": [0.12, -0.34, ...]                                  â”‚   â”‚
â”‚   â”‚   }                                                                          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### Layer 0: Upstream Preprocessing

#### Silero VAD
```
Status:     âœ… Production
Location:   tools/whisper_mlx/upstream/vad.py (or external)
Latency:    ~2ms
Purpose:    Voice activity detection, speech/silence/music classification
Output:     speech_prob (float), is_speech (bool)
```

#### Overlap Detector
```
Status:     âŒ Need to Train
Location:   tools/whisper_mlx/upstream/overlap_detector.py (planned)
Latency:    ~5ms (target)
Purpose:    Count simultaneous speakers per frame
Output:     num_speakers (0, 1, 2, or 3)
Architecture: Small CNN on mel features â†’ softmax over [0,1,2,3]
Training:   LibriMix, VoxConverse, LibriCSS
```

#### MossFormer2 MLX (Source Separation)
```
Status:     âœ… Production (third-party, tested)
Location:   tools/third_party/mossformer_ss_mlx/
Latency:    ~30ms per 100ms chunk
Speed:      9.5Ã— real-time (warm)
Quality:    ~21 dB SI-SDRi (SOTA for available models)
Models:     2spk (16kHz), 3spk (8kHz), WHAMR (8kHz)
License:    Apache 2.0
Source:     github.com/starkdmi/mossformer_ss_mlx
```

---

### Layer 1: Whisper Encoder

```
Status:     âœ… Production (frozen)
Location:   tools/whisper_mlx/model.py
Model:      Whisper large-v3
Parameters: 1.5B (frozen, not trainable)
Output:     1280-dimensional embeddings @ 50Hz (20ms per frame)
Latency:    ~45ms for encoder pass

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio (16kHz) â†’ Mel Spectrogram (80-dim, 25ms window)     â”‚
â”‚       â†“                                                     â”‚
â”‚  Conv Stem (2 conv layers)                                 â”‚
â”‚       â†“                                                     â”‚
â”‚  32Ã— Transformer Blocks                                     â”‚
â”‚  - Self-attention                                           â”‚
â”‚  - Feed-forward (4Ã— hidden)                                 â”‚
â”‚  - LayerNorm                                                â”‚
â”‚       â†“                                                     â”‚
â”‚  Encoder Output: (batch, T, 1280) where T = audio_len/320  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Layer 2A: RichCTC Head (Streaming Path)

```
Status:     âœ… Production
Location:   tools/whisper_mlx/rich_ctc_head.py
Latency:    ~10ms
Output:     All heads @ 50Hz, frame-aligned
```

#### Sub-Heads:

| Head | Output Dim | Status | Accuracy | Training Data |
|------|------------|--------|----------|---------------|
| **Text CTC** | 51,865 | âœ… | 43.45% WER (greedy) | LibriSpeech |
| **Emotion** | 8 classes | âœ… | ~85% | RAVDESS, CREMA-D, MELD |
| **Pitch** | 1 (F0 Hz) | âœ… | - | MIR-1K, PTDB-TUG |
| **Paralinguistics** | 50 classes | âœ… | 96.96% | VocalSound, SEP-28k |
| **Phonemes (Kokoro)** | 178 Misaki | âœ… | 19.5% PER | LibriSpeech (MFA aligned) |
| **Speaker Embedding** | 256-dim | âœ… | - | VoxCeleb-style |
| **Language** | 100 classes | âœ… | 98.61% | CommonVoice, OpenSLR |

#### Paralinguistics Classes (50):
```python
# Universal Non-Verbal (0-10)
speech, laughter, cough, sigh, breath, cry, yawn, throat_clear, sneeze, gasp, groan

# English Fillers (11-15)
um_en, uh_en, hmm_en, er_en, ah_en

# Chinese Fillers (16-19)
nage_zh, zhege_zh, jiushi_zh, en_zh

# Japanese Fillers (20-24)
eto_ja, ano_ja, ee_ja, maa_ja, un_ja

# Korean Fillers (25-28)
eo_ko, eum_ko, geuge_ko, mwo_ko

# Hindi Fillers (29-32)
matlab_hi, wo_hi, yeh_hi, haan_hi

# Other Languages (33-39)
este_es, pues_es, euh_fr, ben_fr, aeh_de, also_de, yani_ar

# Singing Vocalizations (40-49)
sing_a, sing_e, sing_i, sing_o, sing_u, vibrato, trill, vocal_fry, falsetto, belt
```

#### Phoneme Inventory (178 Misaki):
```
Based on Misaki G2P (hexgrad/misaki)
- IPA-based phoneme set
- Covers English, Japanese, Chinese
- Used for hallucination detection
- Used for lip sync / TTS alignment
```

---

### Layer 2B: RichDecoder (Refinement Path)

```
Status:     ğŸ”„ Training (82.34% accuracy)
Location:   tools/whisper_mlx/rich_decoder.py
Latency:    ~150-200ms
Purpose:    High-quality text + refinement of CTC outputs
```

#### Architecture:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RICH DECODER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Whisper Decoder (frozen)                                   â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â”€ LoRA Adapters (trainable, rank=8)                 â”‚
â”‚       â”‚   - q_proj, v_proj, fc1, fc2                        â”‚
â”‚       â”‚   - Last 12 layers only                             â”‚
â”‚       â”‚   - ~4M trainable params                            â”‚
â”‚       â”‚                                                      â”‚
â”‚       â””â”€â”€ Prosody Cross-Attention (NEW)                     â”‚
â”‚           - Attends to CTC emotion_seq (8-dim)              â”‚
â”‚           - Attends to CTC pitch_seq (1-dim)                â”‚
â”‚           - Enables prosody-aware decoding                  â”‚
â”‚                                                              â”‚
â”‚  Output Heads:                                               â”‚
â”‚  â”œâ”€â”€ Text Logits (51,865 + 50 para tokens = 51,915)        â”‚
â”‚  â”œâ”€â”€ Emotion (8 classes, refined)                           â”‚
â”‚  â”œâ”€â”€ Phoneme Deviation (hallucination score)                â”‚
â”‚  â””â”€â”€ Confidence Scores                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Layer 3: Dual-Stream Fusion

```
Status:     âœ… Production
Location:   tools/whisper_mlx/dual_stream.py
Purpose:    Merge CTC (fast) and Decoder (accurate) streams
```

#### Stream Events:
```python
class EventType(Enum):
    TOKEN = "token"         # CTC emits new provisional token
    CONFIRM = "confirm"     # Decoder confirms CTC was correct
    DIFF = "diff"           # Decoder corrects CTC
    BACKTRACK = "backtrack" # CTC revises previous output
    FINAL = "final"         # Token committed, won't change

class StreamEvent:
    event_type: EventType
    alignment_id: str       # Links CTC â†” Decoder
    stream: str             # "ctc" or "decoder"
    timestamp_ms: float
    token: Optional[RichToken]
    diff: Optional[Dict]    # {"field": "token", "ctc": "their", "decoder": "there"}
```

#### Consumer Logic:
```python
class RichStreamConsumer:
    def on_ctc_token(self, event):
        # Show immediately (provisional)
        self.display(event.token, provisional=True)

    def on_decoder_confirm(self, event):
        # Mark as final
        self.mark_final(event.alignment_id)

    def on_decoder_diff(self, event):
        # Apply correction
        self.apply_diff(event.alignment_id, event.diff)

    def on_backtrack(self, event):
        # Remove tokens after backtrack point
        self.remove_after(event.backtrack_to_id)
```

---

### Layer 4: Post-Processing

#### Speaker Buffer
```
Status:     âœ… Designed
Purpose:    Track and assign consistent speaker IDs across time
Method:     Cosine similarity of speaker embeddings + EMA update
Threshold:  0.7 for same-speaker match
```

#### Custom Vocabulary
```
Status:     âœ… Designed
Purpose:    Hotword boosting for names, jargon, domain terms
Method:     Trie-based prefix matching â†’ logit biasing during decode
Boost:      +5.0 logits for matching tokens
```

#### Hallucination Detection
```
Status:     âœ… Production
Location:   tools/whisper_mlx/kokoro_phoneme_head.py
Method:     Compare CTC phonemes vs expected phonemes from decoder text
Metrics:    55.6% recall, 15% FPR
Threshold:  similarity < 0.7 â†’ don't commit token
```

#### Unrecognized Word Memory
```
Status:     âœ… Designed
Purpose:    Track recurring unknown words for vocabulary updates
Method:     Cluster by phoneme hash, collect context, export candidates
```

#### Adaptation Data Collection
```
Status:     âœ… Designed
Purpose:    Collect per-speaker data for fine-tuning
Output:     SpeakerAdaptationData (audio, transcript, embedding, quality signals)
```

---

## Performance Metrics

### Latency Budget

#### Single Speaker (Fast Path):
| Stage | Latency | Cumulative |
|-------|---------|------------|
| VAD | 2ms | 2ms |
| Overlap Detection | 5ms | 7ms |
| Whisper Encoder | 45ms | 52ms |
| CTC Head | 10ms | **62ms** |
| Decoder (async) | +150ms | **212ms** |

#### Multi-Speaker (Separation Path):
| Stage | Latency | Cumulative |
|-------|---------|------------|
| VAD | 2ms | 2ms |
| Overlap Detection | 5ms | 7ms |
| MossFormer2 | 30ms | 37ms |
| Whisper Encoder | 45ms | 82ms |
| CTC Head | 10ms | **92ms** |
| Decoder (async) | +150ms | **242ms** |

### Quality Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| CTC Streaming Latency | <100ms | 62ms | âœ… |
| CTC RTF | <0.2 | 0.092 | âœ… (12.5Ã— RT) |
| CTC Greedy WER | - | 43.45% | Expected (no LM) |
| Decoder WER | <20% | ~17% (training) | ğŸ”„ |
| Emotion Accuracy | >80% | ~85% | âœ… |
| Paralinguistics Accuracy | >75% | 96.96% | âœ… |
| Language ID Accuracy | >90% | 98.61% | âœ… |
| Phoneme PER | <15% | 19.5% | âš ï¸ |
| Hallucination Detection | >50% | 55.6% | âœ… |
| Source Separation SI-SDRi | >12dB | ~21dB | âœ… |

---

## File Structure

```
tools/whisper_mlx/
â”œâ”€â”€ model.py                    # Whisper encoder/decoder
â”œâ”€â”€ rich_ctc_head.py            # All CTC heads combined
â”œâ”€â”€ rich_decoder.py             # LoRA decoder with prosody
â”œâ”€â”€ kokoro_phoneme_head.py      # Phoneme head (hallucination)
â”œâ”€â”€ dual_stream.py              # StreamEvent, RichStreamConsumer
â”œâ”€â”€ confidence_calibration.py   # Temperature/Platt scaling
â”œâ”€â”€ prosody_beam_search.py      # Punctuation from prosody
â”œâ”€â”€ demo_rich_audio.py          # Terminal visualization
â”‚
â”œâ”€â”€ heads/
â”‚   â”œâ”€â”€ emotion.py
â”‚   â”œâ”€â”€ pitch.py
â”‚   â”œâ”€â”€ paralinguistics.py
â”‚   â”œâ”€â”€ language.py
â”‚   â””â”€â”€ speaker.py
â”‚
â”œâ”€â”€ upstream/
â”‚   â”œâ”€â”€ vad.py                  # Silero VAD wrapper
â”‚   â”œâ”€â”€ separator.py            # MossFormer2 wrapper (planned)
â”‚   â””â”€â”€ overlap_detector.py     # (planned)
â”‚
â”œâ”€â”€ train_*.py                  # Training scripts
â””â”€â”€ benchmark_*.py              # Benchmark scripts

tools/third_party/
â””â”€â”€ mossformer_ss_mlx/          # Source separation (Apache 2.0)

models/
â”œâ”€â”€ kokoro_phoneme_head/        # Production phoneme head
â”œâ”€â”€ sota/                       # Downloaded SOTA models for distillation
â””â”€â”€ checkpoints/                # Training checkpoints

data/
â”œâ”€â”€ LibriSpeech/                # ASR training
â”œâ”€â”€ emotion/                    # RAVDESS, CREMA-D, etc.
â”œâ”€â”€ paralinguistics/            # VocalSound, SEP-28k
â”œâ”€â”€ singing/                    # VocalSet, OpenCPOP
â”œâ”€â”€ augmentation/               # MUSAN, RIRS_NOISES
â”œâ”€â”€ separation/                 # LibriMix (generating)
â”œâ”€â”€ diarization/                # VoxConverse (downloading)
â””â”€â”€ multilingual/               # OpenSLR, CommonVoice
```

---

## Training Pipeline

### Current Training Status

| Model | Status | Checkpoint | Next Steps |
|-------|--------|------------|------------|
| RichCTC (all heads) | âœ… Production | checkpoints/rich_ctc/ | - |
| Kokoro Phoneme | âœ… Production | models/kokoro_phoneme_head/ | - |
| Paralinguistics | âœ… Production | checkpoints/paralinguistics_v3/ | - |
| Language Head | âœ… Production | checkpoints/language_head_v1/ | - |
| Emotion (distilled) | âœ… Done | checkpoints/emotion_distilled_v2/ | - |
| RichDecoder | ğŸ”„ 82.34% | checkpoints/rich_decoder/ | Complete training |
| Overlap Detector | âŒ Not started | - | Download data, train |

### Data Requirements

| Task | Current | Needed | Gap |
|------|---------|--------|-----|
| Source Separation | 0 | LibriMix (~100GB) | Download + generate |
| Overlap Detection | 0 | VoxConverse, LibriCSS | Download |
| Emotion | 87K | 200K+ | Pseudo-label |
| Paralinguistics | 31K | 50K+ | - |
| Phoneme | TIMIT | 100K+ | MFA alignment |

---

## Integration: Kokoro TTS Fusion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 STT â†” TTS INTEGRATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SHARED COMPONENTS:                                          â”‚
â”‚  â”œâ”€â”€ Phoneme representation (178 Misaki)                    â”‚
â”‚  â”œâ”€â”€ Speaker embeddings (256-dim)                           â”‚
â”‚  â””â”€â”€ Prosody features (pitch, emotion)                      â”‚
â”‚                                                              â”‚
â”‚  USE CASES:                                                  â”‚
â”‚  1. Voice Cloning: STT speaker_embed â†’ TTS voice selection  â”‚
â”‚  2. Pronunciation: STT phonemes â†’ TTS demo correct form     â”‚
â”‚  3. Emotion Transfer: STT emotion â†’ TTS expressiveness      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## References

- Whisper: https://github.com/openai/whisper
- MLX: https://ml-explore.github.io/mlx/
- MossFormer2: https://github.com/alibabasglab/MossFormer2
- MossFormer2 MLX: https://github.com/starkdmi/mossformer_ss_mlx
- Misaki G2P: https://github.com/hexgrad/misaki
- LoRA: https://arxiv.org/abs/2106.09685

---

## Document History

| Date | Change |
|------|--------|
| 2026-01-02 | Created comprehensive architecture doc |
| 2026-01-02 | Added MossFormer2 MLX (replaces Conv-TasNet) |
| 2026-01-02 | Consolidated from multiple design docs |
